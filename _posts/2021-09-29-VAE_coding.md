---
title: Math AI - VAE Coding 
date: 2021-09-29 23:10:08
categories:
- AI
tags: [ML, VAE, Autoencoder, Variational, EM]
typora-root-url: ../../allenlu2009.github.io
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

## Main Reference

* [@kingmaIntroductionVariational2019] : excellent reference

* [@kingmaAutoEncodingVariational2014]

* [@roccaUnderstandingVariational2021]

### VAE Recap

$$
\begin{aligned}\log p_{\boldsymbol{\theta}}(\mathbf{x}) &=\underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=\mathcal{L}_{\theta,\phi}{(\boldsymbol{x}})\,\text{, ELBO}}+\underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)}\end{aligned}
$$

$$
\begin{aligned}\underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=\mathcal{L}_{\theta,\phi}{(\boldsymbol{x}})\,\text{, ELBO}} &= \mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log p_{\theta}(\mathbf{x} | \mathbf{z})\right] - D_{K L}\left(q_{\phi}(\mathbf{z} | \mathbf{x}) \|\,p(\mathbf{z})\right) \\&= (-1) \times \text{VAE Loss Function}\end{aligned}
$$

Êúâ‰∫Ü loss function, ÁêÜË´ñ‰∏äÂ∞±ÂèØ‰ª• training.

* Gradient
* Some term are samples (1), some has analytical form (2) (see appendix A)

(1) Naive Monte Carlo gradient estimator

 $\nabla_{\phi} E_{q_{\phi}(\mathbf{z})}[f(\mathbf{z})] = E_{q_{\phi}(\mathbf{z})}\left[f(\mathbf{z}) \nabla_{q_{\phi}(\mathbf{z})} \log q_{\phi}(\mathbf{z})\right] \simeq \frac{1}{L} \sum_{l=1}^{L} f(\mathbf{z}) \nabla_{q_{\phi}\left(\mathbf{z}^{(l)}\right)} \log q_{\phi}\left(\mathbf{z}^{(l)}\right)$

where $\mathbf{z}^{(l)} \sim q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)$.

This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])

#### SGVB estimator and AEVB algorithm

ÈÄôÁØÄË®éË´ñÂØ¶ÈöõÁöÑ estimator of approximate posterior in the form of $q_\phi(\mathbf{z}\mid \mathbf{x})$. Ê≥®ÊÑè‰πüÂèØ‰ª•ÈÅ©Áî®Êñº $q_\phi(\mathbf{z})$.  

Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior $q_\phi(\mathbf{z}\mid \mathbf{x})$ we can reparametrize the random variable $\tilde{\mathbf{z}} \sim q_\phi(\mathbf{z}\mid \mathbf{x})$ using a differentiable transformation $g_{\phi}(\epsilon, x)$ of an (auxiliary) noise variable :

$$E_{q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}[f(\mathbf{z})]=E_{p(\epsilon)}\left[f\left(g_{\phi}\left(\boldsymbol{\epsilon}, \mathbf{x}^{(i)}\right)\right)\right] \simeq \frac{1}{L} \sum_{l=1}^{L} f\left(g_{\phi}\left(\boldsymbol{\epsilon}^{(l)}, \mathbf{x}^{(i)}\right)\right) \quad$ where $\quad \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})$$

We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator $\widetilde{\mathcal{L}}^{A}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right) \simeq \mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)$ :

$$
\widetilde{\mathcal{L}}^{A}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=\frac{1}{L} \sum_{l=1}^{L} \log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}, \mathbf{z}^{(i, l)}\right)-\log q_{\phi}\left(\mathbf{z}^{(i, l)} \mid \mathbf{x}^{(i)}\right)
$$

where $\quad \mathbf{z}^{(i, l)}=g_{\phi}\left(\boldsymbol{\epsilon}^{(i, l)}, \mathbf{x}^{(i)}\right) \quad$ and $\quad \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})$

##### Algorithm 1:  Minibatch version of Auto-Encoding Variational Bayes (AEVB) algorithm.  We set M=100 and L=1

$\theta, \phi$ : Initialize parameters

Repeat

* $X^M$ Random minibatch of M datapoints (drawn from full dataset)

* $\boldsymbol{\epsilon}$  Random samples from noise distribution $p(\boldsymbol{\epsilon})$

* $\mathbf{g}$  gradients of minibatch estimator

* $\theta, \phi$  Update parameters using gradients $\mathbf{g}$

??? SGVB estimator $\widetilde{\mathcal{L}}^{B}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right) \simeq \mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)$, corresponding to eq. (3), which typically has less variance than the generic estimator:

$$
\widetilde{\mathcal{L}}^{B}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=-D_{K L}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)+\frac{1}{L} \sum_{l=1}^{L}\left(\log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}^{(i, l)}\right)\right)
$$

where $\quad \mathbf{z}^{(i, l)}=g_{\phi}\left(\boldsymbol{\epsilon}^{(i, l)}, \mathbf{x}^{(i)}\right) \quad$ and $\quad \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})$

Given multiple datapoints from the dataset $X$ with N datapoints, we can

$$
\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{X}) \simeq \widetilde{\mathcal{L}}^{M}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{X}^{M}\right)=\frac{N}{M} \sum_{i=1}^{M} \widetilde{\mathcal{L}}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)
$$

#### Example: Variational Auto-Encoder, assuming Gaussian

$$
\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right) \simeq \frac{1}{2} \sum_{j=1}^{J}\left(1+\log \left(\left(\sigma_{j}^{(i)}\right)^{2}\right)-\left(\mu_{j}^{(i)}\right)^{2}-\left(\sigma_{j}^{(i)}\right)^{2}\right)+\frac{1}{L} \sum_{l=1}^{L} \log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}^{(i, l)}\right)
$$

where $\quad \mathbf{z}^{(i, l)}=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)} \odot \boldsymbol{\epsilon}^{(l)} \quad$ and $\quad \boldsymbol{\epsilon}^{(l)} \sim \mathcal{N}(0, \mathbf{I})$

### VAE Encoder-Decoder Structure

From [@roccaUnderstandingVariational2021],‰∏ÄÂÄãÊòØ encoder NN, Â¶Ç‰∏ãÂºè $(g^*, h^*)$

$$
\begin{aligned}
\left(g^{*}, h^{*}\right) &=\underset{(g, h) \in G \times H}{\arg \min } K L\left(q_{x}(z), p(z \mid x)\right) \\
&=\underset{(g, h) \in G \times H}{\arg \max }\left(\mathbb{E}_{z \sim q_{x}}\left(-\frac{\|x-f(z)\|^{2}}{2 c}\right)-D_{K L}\left(q_{x}(z), p(z)\right)\right)
\end{aligned}
$$

Âè¶‰∏ÄÂÄãÊòØ decoder NN, Â¶Ç‰∏ãÂºè $f^*$

$$
\begin{aligned}
f^{*} &=\underset{f \in F}{\arg \max } \mathbb{E}_{z \sim q_{x}^{*}}(\log p(x \mid z)) \\
&=\underset{f \in F}{\arg \max } \mathbb{E}_{z \sim q_{x}^{*}}\left(-\frac{\|x-f(z)\|^{2}}{2 c}\right)
\end{aligned}
$$

Gathering all the pieces together, we are looking for optimal $\mathrm{f}^{*}, \mathrm{~g}$* and $\mathrm{h}^{*}$ such that

$$
\left(f^{*}, g^{*}, h^{*}\right)=\underset{(f, g, h) \in F \times G \times H}{\arg \max }\left(\mathbb{E}_{z \sim q_{x}}\left(-\frac{\|x-f(z)\|^{2}}{2 c}\right)-D_{K L}\left(q_{x}(z), p(z)\right)\right)
$$

Á≠âÂÉπÊñº minimize VAE loss function

$$
\begin{aligned}
\text {VAE loss }&=C\|x-\hat{x}\|^{2}+D_{KL}\left(N\left(\mu_{x}, \sigma_{x}\right), N(0, I)\right)\\
&=C\|x-f(z)\|^{2}+D_{KL}(N(g(x), h(x)), N(0, l))
\end{aligned}
$$

Á¨¨‰∏ÄÈ†ÖÊòØ reconstruction loss, Á¨¨‰∫åÈ†ÖÊòØ regularization loss.  Á¨¨‰∏ÄÈ†ÖÂæû sampling ÂæóÂà∞„ÄÇÁ¨¨‰∫åÈ†ÖÊúâ analytical form, Ë¶ã Appendix A.

In practice, g and h are not defined by two completely independent NN but share a part of their architecutre and theier weights so that

$\mathbf{g}(x) = \mathbf{g}_2(\mathbf{g}_1(x)) \quad  \mathbf{h}(x) = \mathbf{h}_2(\mathbf{h}_1(x)) \quad \mathbf{g}_1(x) = \mathbf{h}_1(x)$

<img src="/media/img-2021-10-02-20-25-39.png" style="zoom:67%;" />

<img src="/media/img-2021-10-02-21-06-37.png" style="zoom:67%;" />

#### ‰ª•‰∏ãÊòØ VAE PyTorch code example for MNIST

##### MNIST dataset

* MNIST image: size 28x28=784 pixels of grey value between 0 and 1.  0: ÁôΩÔºõ1ÔºöÈªë„ÄÇ0.1-0.9 ‰ª£Ë°®‰∏çÂêåÁöÑÁÅ∞ÈöéÔºåÂ¶Ç‰∏ãÂúñ„ÄÇ
* MNIST datset: 60K for training;  10K for testing.  Total 70K.

<img src="/media/img-2021-10-03-09-27-59.png" style="zoom:67%;" />

##### VAE Model

* VAE **encoder** first uses FC network (fc1: 784->400) + ReLU, Á≠âÂÉπ‰∏äÂúñÁöÑ $\mathbf{h}_1 = \mathbf{g}_1$
* ÂÜçÊé•‰∏äÂÖ©ÂÄã FCs (fc21=$\mathbf{g}_2$, fc22=$\mathbf{h}_2$, 400->20) Áî¢Áîü mean,mu, and log of variance, logvar of 20 dimensions.  **Ê≥®ÊÑèÈÄô‰∫åÂÄã FCs Ê≤íÊúâ‰∏≤Êé• ReLU, Âõ†Áà≤ mean and logvar ÂèØÊ≠£ÂèØË≤†„ÄÇ**
* Âü∫Êñº reparameterization trick:  $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} $ (20-dimension)
* VAE **decoder** ÂÖàÊòØ FC network (fc3, 20->400) + ReLU
* ÂÜç‰∏≤‰∏ÄÂÄã FC network (fc4, 400->784=28x28) + sigmoid ‰øùË≠â‰ªãÊñº 0~1 (to match mnist image grey level).  ‰πüÂ∞±ÊòØ $\mathbf{f}$ = fc3+ReLU+fc4+sigmoid
* Forward path ÂåÖÂê´ encode, reparameterize, decode.

```python
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(784, 400)
        self.fc21 = nn.Linear(400, 20)
        self.fc22 = nn.Linear(400, 20)
        self.fc3 = nn.Linear(20, 400)
        self.fc4 = nn.Linear(400, 784)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

model = VAE().to(device)
```

##### VAE Loss function and optimizer

* BCE ÊòØ binary cross-entropy, ‰ª£Ë°® reconstruction loss. Ê≥®ÊÑèÈõñÁÑ∂Á®±Áà≤ binary cross-entropy, label ÂèØ‰ª•ÊòØ 0-1 ÁöÑ value, Âõ†Áà≤ mnist ÁöÑ image ÊòØ grey level ËÄåÈùû binary value.  Áà≤‰ªÄÈ∫ΩÊòØ reduction = sum ËÄåÈùû mean?
* KLD ÊòØ KL divergence, ÊòØ regularization term.  Âú® Gaussian assumption Êúâ analytical form.

```python
# Reconstruction + KL divergence losses summed over all elements and batch
def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

    # see Appendix A from VAE paper:
    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
    # https://arxiv.org/abs/1312.6114
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    return BCE + KLD

optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

##### Êï¥Âêà training code

* Training dataset (60K) Áî± train_loader ËºâÂÖ•„ÄÇMini-batch size ÂèØÁî± command line ÊåáÂÆö, default = 128.
* model(data) ÂÆåÊàê forward, ÂÇ≥Âõû reconstructed image, mu, logvar Áî®Êñº loss computation with batch_size=128.  Â∞±ÊòØ**ÊØèÂºµ image ÁöÑ loss** Á∫çÁ©ç 128 Âºµ„ÄÇ
* Êé•ËëóÊØèÂÄã mini-batch Ë®àÁÆó backward and use Adam optimizer to update weights.  ‰∏çÈÅéÁà≤‰∫ÜÈÅøÂÖçÈõú‰∫ÇÔºåÂè™Êúâ log_interval (default=10) Êâç print ‰∏ÄÊ¨° log, default = 128x10 = 1280.
* ÊØèÂÄã epoch print average training loss (default 10 epoches).

```python
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.ToTensor()),
    batch_size=args.batch_size, shuffle=True, **kwargs)

def train(epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss.item() / len(data)))

    print('====> Epoch: {} Average loss: {:.4f}'.format(
          epoch, train_loss / len(train_loader.dataset)))
```

##### ÁµêÊûú

* ÊØè‰∏ÄÊ¨° log ÊòØ 128x10=1280, Â§ßÊñº 2% of 60K dataset per epoch.
* Epoch 1 average loss ÂæàÂ§ßÔºö164.  Âà∞‰∫Ü Epoch 10 average loss: 106.  Âü∫Êú¨Â∑≤Á∂ì saturated. ÈÄôÂÄã loss ÂåÖÂê´ BCE and KLD.  
  * Total loss: Epoch 1 ~ 164;  Epoch 10 ~ 106.
  * KLD loss:   Epoch 1 ~  14;   Epoch 10 ~ 25.
  * BCE loss:   Epoch 1 ~ 150;  Epoch 10 ~ 81.
* BCE loss Â∞±ÊòØ‰∏ÄËà¨ autoencoder loss Èö®Ëëó epoch Â¢ûÂä†ËÆäÂ∞èÔºå‰ΩÜ KLD loss ËÆäÂ§ßÔºåÂêåÊôÇ regularize BCE loss saturate.  

```
Train Epoch: 1 [0/60000 (0%)]           Loss: 550.513977
Train Epoch: 1 [1280/60000 (2%)]        Loss: 310.610535
.... omit
Train Epoch: 1 [57600/60000 (96%)]      Loss: 129.696487
Train Epoch: 1 [58880/60000 (98%)]      Loss: 132.375336
====> Epoch: 1 Average loss: 164.4209

... Epoch 2 to 9, TL;DP

Train Epoch: 10 [0/60000 (0%)]          Loss: 105.353363
Train Epoch: 10 [1280/60000 (2%)]       Loss: 103.786560
... omit
Train Epoch: 10 [57600/60000 (96%)]     Loss: 107.218582
Train Epoch: 10 [58880/60000 (98%)]     Loss: 105.427353
====> Epoch: 10 Average loss: 106.1371

```

‰∏ãÂúñÂ∑¶‰∏äÂíåÂ∑¶‰∏ãÂ∞çÊáâ epoch 1 ÁöÑ reconstructed images Âíå random generated images.
‰∏ãÂúñÂè≥‰∏äÂíåÂè≥‰∏ãÂ∞çÊáâ epoch 10 ÁöÑ reconstructed images Âíå random generated images. ÈÉΩÊòØ 20-dimension.

<img src="/media/img-2021-10-03-12-20-16.png" style="zoom:67%;" />

#### Appendix A - Solution of Gaussian Distribution of $D_{K L}(q_\phi(\mathbf{z})\|p_{\theta}(\mathbf{z}))$

$$
\begin{aligned}
\int q_{\boldsymbol{\theta}}(\mathbf{z}) \log p(\mathbf{z}) d \mathbf{z} &=\int \mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}, \boldsymbol{\sigma}^{2}\right) \log \mathcal{N}(\mathbf{z} ; \mathbf{0}, \mathbf{I}) d \mathbf{z} \\
&=-\frac{J}{2} \log (2 \pi)-\frac{1}{2} \sum_{j=1}^{J}\left(\mu_{j}^{2}+\sigma_{j}^{2}\right)
\end{aligned}
$$

And:

$$
\begin{aligned}
\int q_{\boldsymbol{\theta}}(\mathbf{z}) \log q_{\boldsymbol{\theta}}(\mathbf{z}) d \mathbf{z} &=\int \mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}, \boldsymbol{\sigma}^{2}\right) \log \mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}, \boldsymbol{\sigma}^{2}\right) d \mathbf{z} \\
&=-\frac{J}{2} \log (2 \pi)-\frac{1}{2} \sum_{j=1}^{J}\left(1+\log \sigma_{j}^{2}\right)
\end{aligned}
$$

Therefore:

$$
\begin{aligned}
-D_{K L}\left(\left(q_{\phi}(\mathbf{z}) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)\right.&=\int q_{\boldsymbol{\theta}}(\mathbf{z})\left(\log p_{\boldsymbol{\theta}}(\mathbf{z})-\log q_{\theta}(\mathbf{z})\right) d \mathbf{z} \\
&=\frac{1}{2} \sum_{j=1}^{J}\left(1+\log \left(\left(\sigma_{j}\right)^{2}\right)-\left(\mu_{j}\right)^{2}-\left(\sigma_{j}\right)^{2}\right)
\end{aligned}
$$
When using a recognition model $q_{\phi}(z|x)$ then $\mu$ and s.d. $\sigma$ are simply functions of $x$ and the variational parameters $\phi$, as exemplified in the text.
