#### Next

Q: Score function çš„å¹¾ä½•æˆ–æ˜¯ç‰©ç†æ„ç¾©æ˜¯ä»€éº½?  D-log-likelihood function. 

Q: çˆ²ä»€éº½ Fisher information = var = -1 * 2nd order derivative?   

Q: åˆ°åº• variance å¤§æ¯”è¼ƒå¥½ estimate? (yes), ä¼¼ä¹ counter intuitive!!  Variance å°æ¯”è¼ƒå¥½ estimate?



#### Fisher Information vs. Shannon  Information 

ç°¡å–®çš„æƒ³æ³•å°±æ˜¯åš n æ¬¡å¯¦é©—çœ‹ (joint) distribution.  ä¸éåˆæœƒç‰½æ‰¯ n è¦é¸å¤šå°‘ã€‚åœ¨ç†è«–æ¨å°æ˜¯æœ€å¥½åªä½¿ç”¨åŸå§‹çš„ distribution (n=1).

Fisher å¼•å…¥ score function (1st derivative of log-likelihood function)

æˆ‘å€‘å®šç¾©

å‡è¨­ $\sigma$ å·²çŸ¥ï¼Œscore function æ˜¯ 1 dimension linear function with negative slope as below:
$$
s(\mu;x_1, \ldots, x_n) =  l'\left(\mu, \sigma^2 ; x_1, \ldots, x_n\right)=\frac{1}{\sigma^2} \sum_{j=1}^n\left(x_j-\mu\right)
$$




Parameter estimation çš„è€ƒé‡:

å…ˆç”¨ observed çš„ (X1, X2, X3, â€¦, Xn) ç”¨ä¸€å€‹å…¬å¼ï¼Œä¼°è¨ˆå‡º Î¸est (e.g. Î¸est = A/n, or any other Î¸est = g(X1, X2, .., Xn) )

å†ä¾†ç”¨ Î¸est åŒæ™‚åšç„¡çª®å€‹å¹³è¡Œçš„æƒ³åƒå¯¦é©—çš„ parameterï¼Œæ‰¾å‡ºå°æ‡‰çš„ Î¸est'  "distribution" based on the above å…¬å¼ã€‚

(1) å¦‚ä½•ä¼°è¨ˆ Î¸: Î¸est ?  ç”¨ä¼°è¨ˆçš„ Î¸est  åšç„¡çª®å¹³è¡Œæƒ³åƒå¯¦é©—æ‰€ä¼°è¨ˆçš„ Î¸est' å¹³å‡å¾Œæ‡‰è©²è¦ç­‰æ–¼ (unbiased) æˆ–è¶¨è¿‘ Î¸est,  å°±æ˜¯ **constentency**.  (ref: Wifi[ statistics consistency](https://en.wikipedia.org/wiki/Consistency_(statistics)) or [Fisher consistency](https://en.wikipedia.org/wiki/Fisher_consistency))

(2) é€™å€‹ä¼°è¨ˆçš„ Î¸est æœ‰å¤šæº–ç¢º? æˆ–æ˜¯ Î¸est çš„ variance (or ä¿¡å¿ƒå€é–“) æœ‰å¤šå¤§? ç”¨ Fisher çš„è©±ä¾†èªªï¼Œæ˜¯å¦ **efficient.**  

(3) é€™å€‹ estimation Î¸est æ˜¯å¦æ˜¯ **optimal?**  å¦‚ä½•å®šç¾© optimal?  å¯ä»¥å¾ (1) and (2) ä¾†çœ‹ï¼Œ**optimal å°±æ˜¯å¿…é ˆ consistency è€Œä¸”æœ€ efficient (æœ€å° variance).**

é€™ 3 å€‹è§€å¿µæ˜¯ç’°ç’°ç›¸æ‰£ã€‚å¦‚æœ consistency ä¸æˆç«‹ã€‚åªè¦è®“ Î¸est = 0.  å°±æœƒæœ‰æœ€å°çš„ variance. ä¹Ÿæ˜¯æœ€ optimla estimation. ä½†é€™æ˜¯ç„¡ç”¨çš„ estimator.

å› æ­¤ parameter estimation æœ€åŸºæœ¬çš„è§€å¿µæ˜¯ consistency.  å°±æ˜¯ç”¨ estimated parameter ä¾†ä½œå¯¦é©—ï¼Œæ‡‰è©²è¦å¾—åˆ°ä¸€è‡´æ€§çš„çµæœã€‚å†åŠ ä¸Šæœ‰æœ€å¥½çš„ efficiency, å°±æ˜¯ optimal estimator.  

æƒ³åƒå¦‚æœæˆ‘å€‘ä¼°è¨ˆçš„ Î¸est æ¯”çœŸå¯¦çš„ Î¸o å° (e.g. Î¸est = A/2n instead of A/n = Î¸o/2)ã€‚é›–ç„¶æ©Ÿç‡å°ï¼Œé‚„æ˜¯æœ‰æ©Ÿæœƒä¸Ÿå‡º A æ¬¡ head.  

å¦‚æœç”¨é€™å€‹ Î¸est åŒæ™‚åšç„¡çª®å€‹å¹³è¡Œçš„ Bernoulli process with size n.  å‡ºç¾ head æ¬¡æ•¸çš„ distribution å°±æ˜¯ Bi-nomial distribution.  é€™å€‹ç„¡çª®å¹³è¡Œæƒ³åƒå¯¦é©—çš„ head å¹³å‡æ¬¡æ•¸æ˜¯ n*Î¸est = A/2 æ¬¡ã€‚å¾é€™äº›æƒ³åƒå¯¦é©—ä¼°å‡ºçš„ Î¸est' = (A/2)/2n = A/4n!

æ˜é¡¯ Î¸est <> Î¸est'  ä¹Ÿå°±æ˜¯ä¸ consistent!

 

### Fisher 1922 è«–è¨¼ (under some assumption) maximum likelihood estimator (MLE) å…·æœ‰ consistency, sufficiency çš„ optimal parameter estimator.  (Fisher èªç‚º MLE å¾—å‡ºçš„ statistics are always sufficient statistics. é€™å€‹èªªæ³•æ˜¯æœ‰å•é¡Œã€‚è«‹åƒè€ƒä¸‹ä¸€ç¯€çš„çµè«– "sufficiency â€¦")

### Rao (1963) è¨¼æ˜äº† MLE estimator is efficient (minimum variance) in the class of consistent and uniformly asymptotically normal (CUAN) estimator.

å¹¾å€‹å¸¸æœ‰çš„èª¤å€:

\* MLE ä¸¦éå”¯ä¸€çš„ consistent estimator.  MLE ä¹Ÿä¸¦é unbiased estimator.  ç„¡æ³•å¾ (1) æ¨å°å‡º MLE.  ä½†å¯ä»¥è¨¼æ˜ MLE æ˜¯ optimal estimator.

\* Fisher å®šç¾©çš„ efficiency æ˜¯åŸºæ–¼ minimum variance, ä¹Ÿå°±æ˜¯ least mean square error/loss.  é€™ä¸¦éå”¯ä¸€çš„ loss function.

ä¾‹å¦‚ä¹Ÿå¯ä»¥ç”¨ L1-norm loss function.  ä¸åŒçš„ loss function å°æ‡‰ä¸åŒçš„ optimal efficient parameter estimator.

 

### å¦‚ä½•è¨¼æ˜ MLE æ˜¯ consist and optimal parameter estimator? 

Consistency è«‹åƒè€ƒ [reference](http://www.econ.uiuc.edu/~wsosa/econ507/MLE.pdf). 

Optimality å°±ä¸æ˜¯é‚£éº¼å®¹æ˜“ã€‚

 

### Fisher å¤©æ‰ä¹‹è™•åœ¨1922 å¼•å…¥äº† sufficient statistic çš„è§€å¿µã€‚1925 åˆå¼•å…¥äº† Score and Efficiency è§€å¿µ

å…ˆèªªçµè«–: (è«‹åƒè€ƒ [Stigler](https://arxiv.org/pdf/0804.2996.pdf) çš„ The Epic Story of MLE)

Sufficiency implies optimality (efficiency, or minimum variance), at least when combined with consistency and asymptotic normality.

Fisher çµ¦äº†ä¸€å€‹ç°¡æ½”æœ‰åŠ›çš„è«–è¨¼é—œæ–¼ä»¥ä¸Šçš„çµè«–ã€‚è«‹åƒè€ƒ Stigler article.  

In general æˆ‘å€‘éœ€è¦å®Œæ•´çš„ samples (X1, X2, â€¦, Xn) æ‰èƒ½ç”¨æ–¼ parameter Î¸ estimation.

Fisher è€ƒæ…®æ˜¯å¦èƒ½ "ç²¾ç·´" å‡º S(X1, X2, â€¦, Xn) é‡å° Î¸ parameter, å°±å¯ä»¥ä¸Ÿæ‰åŸä¾†çš„ X1, X2, â€¦, Xn è€Œä¸å¤±å»ä»»ä½•çš„çµ±è¨ˆè³‡è¨Šã€‚åˆ‡è¨˜é€™å€‹ S(X1, X2, ..Xn), ç¨±ç‚º sufficient statistic å¿…é ˆæ˜¯é‡å°ç‰¹å®š Î¸ (Î¸ å¯ä»¥æ˜¯ vector, åŒ…å«å¤šå€‹åƒæ•¸); è€Œä¸æ˜¯ sufficient statistic å¯ä»¥ä»»æ„å–ä»£åŸå§‹çš„ samples æˆ– distribution.  å¦‚æœçœŸçš„å¦‚æ­¤ï¼Œä¹Ÿä¸ç”¨ big dataã€‚åªè¦å­˜å£“ç¸®ç‰ˆçš„ sufficient statistic å°±å¤ äº†ã€‚ä½†æˆ‘å€‘æ°¸é ä¸çŸ¥é“æ—¥å¾Œæœƒç”¨åŸå§‹è³‡æ–™ä¾† estimate ä»€éº¼æ±è¥¿ã€‚

Fisher çš„è«–è¨¼å¦‚ä¸‹:

Maximum likelihood estimation -->  Function of sufficient statistic --> Optimality

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage49.png)

å…¶ä¸­ç¬¬ä¸€æ­¥æ˜¯æœ‰å•é¡Œçš„ã€‚Maximum likelihood estimation ä¸¦ä¸ always å¾—å‡º sufficient statistic.  

Fisher ç•¶åˆè¨ˆç®—å¹¾å€‹å¸¸ç”¨çš„ distribution è€Œå¾—å‡º sufficient statistic å­˜åœ¨ä¸¦ä¸”å¯ç”± MLE æ¨å‡ºã€‚

å¯¦éš›ä¸Šå¾ˆå¤š distribution with parameter ä¸¦ä¸å­˜åœ¨ sufficient statistics.  ä½† MLE å­˜åœ¨ä¸”ç‚º optimal! 

ä¹‹å¾Œ Fisher ä¹Ÿä¸å†æå‡ºé€™å€‹è«–è¨¼ï¼Œè€Œç”¨å…¶ä»–çš„æ–¹æ³•å¾—åˆ° MLE --> Optimality çš„çµè«–ã€‚

 

### Sufficient Statistic

### é›–ç„¶ Fisher å¾Œä¾†æ²’æœ‰å†ç”¨ sufficient statistics è¨¼æ˜ MLE çš„ optimality.

Sufficient statistics ä»ç„¶æ˜¯æœ‰ç”¨çš„è§€å¿µã€‚å¸¸è¦‹æ–¼ä¸€äº›ç†è«–çš„ framework.  (e.g. exponential family).

æ•¸å­¸å®šç¾©å¾ˆç›´æ¥å¦‚ä¸‹ã€‚é‡é»æ˜¯å¯ä»¥æ¨å‡º Fisher-Neyman factorization theorem.

ä¾‹å­å’Œè¨¼æ˜å¯ä»¥åƒè€ƒ wiki.

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage50.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage51.png)

 

### Exponential Family

å†ä¾†çœ‹ Exponential Family, åŸºæœ¬ä¸Šå°±æ˜¯ sufficient statistic çš„æœ€ä½³ä»£è¨€ã€‚

Exponential Family åŒ…å«å¤§å¤šå¸¸è¦‹çš„ PDF/PMF å¦‚ä¸‹:

the **[normal](https://en.wikipedia.org/wiki/Normal_distribution), [exponential](https://en.wikipedia.org/wiki/Exponential_distribution),[gamma](https://en.wikipedia.org/wiki/Gamma_distribution), [chi-squared](https://en.wikipedia.org/wiki/Chi-squared_distribution), [beta](https://en.wikipedia.org/wiki/Beta_distribution), [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution), [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution), [categorical](https://en.wikipedia.org/wiki/Categorical_distribution), [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution), [Wishart](https://en.wikipedia.org/wiki/Wishart_distribution), [Inverse Wishart](https://en.wikipedia.org/wiki/Inverse_Wishart_distribution)** and many others. A number of common distributions are exponential families only when certain parameters are considered fixed and known, e.g**.** **[binomial](https://en.wikipedia.org/wiki/Binomial_distribution) (with fixed number of trials), [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) (with fixed number of trials), and [negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution)** **(with fixed number of failures).** Examples of common distributions that are **not exponential families are** **[Student's \*t\*](https://en.wikipedia.org/wiki/Student's_t_distribution), most [mixture distributions](https://en.wikipedia.org/wiki/Mixture_distribution)****,** and even the family of [uniform distributions](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))with unknown bounds. 

Scalar parameter exponential family å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹:

![f_{X}(x\mid \theta )=h(x)\exp \left(\eta (\theta )\cdot T(x)-A(\theta )\right)](https://wikimedia.org/api/rest_v1/media/math/render/svg/393d23d5e866e091c87546255856ea9313d8f126)

where *T*(*x*), *h*(*x*), *Î·*(Î¸), and *A*(Î¸) are known functions.

\* T(x) å°±æ˜¯ sufficient statistic.  

\* Î· ç¨±ç‚º natural parameter.  

\* A(Î·) ç¨±ç‚º log-partition function.  

 

**Exponential families æœ‰ä¸€äº›é‡è¦çš„ç‰¹æ€§:**

\* Exponential families çš„ sufficient statistics T(x) å¯ä»¥ summarize arbitrary amount of IID data.

\* Exponential families éƒ½æœ‰ conjugate priors, é€™å° Bayesian statistics å¾ˆæœ‰ç”¨ã€‚

\* Exponential families çš„ posterior pdf (prior * conditional pdf) é›–ç„¶æœ‰ close-form.  ä½†å¤§å¤šé exponential families. E.g. Student t-distribution. 

**Example of Normal distribution with known variance, estimate mean**



This is a single-parameter exponential family, as can be seen by setting



**Example of Normal distribution, estimate mean and variance** 



This is an exponential family which can be written in canonical form by defining



**å¯ä»¥çœ‹åˆ°ä¸åŒçš„ Î¸ å°æ‡‰ä¸åŒçš„ sufficient statistics.  å³ä½¿ PDF çš„å½¢å¼ä¸€æ¨£ã€‚**

 

### f(x; Î˜) å¯ä»¥è¦–ç‚º PDF/PMF function of x given Î˜.  æˆ–æ˜¯ L(Î¸; X) likelihood function of Î¸ given X.

### Fisher å²å®³ä¹‹è™•åœ¨ explore f(x;Î˜) or L(Î¸;X) æ›´å¤šçš„çµ±è¨ˆç‰¹æ€§

é¦–å…ˆå®šç¾© score V  (given observed **X**) as the gradient w.r.t. Î¸ of the log likelihood function 

V â‰£ V(Î¸; **X**) = âˆ‚ LL(Î¸; **X**) / âˆ‚Î¸ = âˆ‚ log L(Î¸; **X**) / âˆ‚Î¸ = âˆ‘ âˆ‚ log L(Î¸, Xi)/âˆ‚Î¸   

ä¸€èˆ¬è€Œè¨€, L(Î¸) ä¸ä¸€å®šæ˜¯ concave, e.g. normal distribution.  ä½† LL(Î¸) or log L(Î¸) or âˆ‘ log L(Î¸) é€šå¸¸æ˜¯ concave.

é¡¯ç„¶é€™æ˜¯ MLE çš„å»¶ä¼¸ã€‚MLE åŸºæœ¬ä¸Šæ˜¯è¨­å®š gradient wrt Î¸ ç‚º 0. i.e. V = 0 å¾—åˆ°æœ€ä½³çš„ Î¸mle given oberved **X**.



 

å†ä¾†å°±æ˜¯å° LL(Î¸; **X) or** V(Î¸; **X**) åœ¨ Î¸=Î¸est ä½œ Taylor series,

LL(Î¸; **X**) =  C - n A/2 ( Î¸ - Î¸mle )^2  (Log Likelihood function å¯ä»¥ç”¨ parabola è¿‘ä¼¼, é ‚é»å°±æ˜¯ Î¸mle) 

V(Î¸; **X**) = âˆ‚ LL(Î¸; **X**) / âˆ‚Î¸ = - n A ( Î¸ - Î¸mle )   (V å°±æ˜¯ Log Likelihood çš„ gradient or æ–œç‡)

E( V(Î¸; X) ) = 0

I(Î¸) = Var(V(Î¸; X)) = E( V(Î¸; X)^2 ) = - E( âˆ‚V/âˆ‚Î¸ ) = - E( âˆ‚^2 log f(x; Î¸)/âˆ‚Î¸âˆ‚Î¸ )  

 

 

ä»¥ä¸Šæœ‰é»è¤‡é›œ, æœ‰äº›åƒæ•¸æœ‰ n, æœ‰äº›åƒæ•¸æ˜¯ X or **X**ã€‚å¯ä»¥åƒè€ƒ[æœ¬æ–‡](http://www.econ.uiuc.edu/~wsosa/econ507/MLE.pdf)æ•´ç†å¾ˆæ¸…æ¥šã€‚ 

## ä»¥ä¸‹æ˜¯ Score and Information çš„è§£é‡‹å’Œæ¨å°

 Î¸ or Î¸o æ˜¯ K dimension parameters.  ä¸€èˆ¬ Î¸ æ˜¯è®Šæ•¸ï¼ŒÎ¸o æ˜¯ given a value. 

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage55.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage53.png) 

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage59.png)

 l(Î¸; Z) = log L(Î¸; Z)  ç¨±ç‚º log likelihood function 

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage54.png) 

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage56.png)

Score åˆ†ç‚º score and sample score (n observed samples).  åŸå‰‡ä¸Šéƒ½æ˜¯ function of Î¸ with a given Z or Zi.

ä½† Score s(Î¸; Z) çš„ Z å¯è¦–ç‚º random variable.  å› æ­¤ E[ s(Î¸;Z) ] æ˜¯æœ‰æ„ç¾©ã€‚ç•¶ç„¶ Z ä¹Ÿå¯ä»¥æ˜¯ observed value, é‚£å°±æ˜¯ä¸€å€‹è§€å¯Ÿå€¼çš„ score. å¯èƒ½æ„ç¾©ä¸å¤§ã€‚

ç›¸å½¢ä¹‹ä¸‹ Sample score ä¸­çš„ Zi æ¯”è¼ƒæ˜¯å¯¦éš› observed values.  æ¯”è¼ƒå°‘ç”¨ E(sample score).  ç•¶ç„¶ä¹Ÿå¯ä»¥å®šç¾©ç‚º Z1, Z2,â€¦ Zn çš„ joint pdf.  

In summary, å¦‚æœè¦ç®— Expectation score, å¤šåŠæ˜¯ç”¨ä¸€å€‹ random variable Z.  å¦‚æœæ˜¯ç®—å¯¦å€¼çš„ score, å¤šåŠæ˜¯ç”¨ sample score (deterministic).  

åŒæ¨£çš„é“ç†ä¹Ÿå¯ä»¥ apply to Hessian (RV) and Sample hessian (deterministic); æˆ–æ˜¯ (Fisher) Information and Sample information? (æœ‰ Sample information å—?)  åš´æ ¼ä¾†èªª Fisher information æ˜¯å– expectation value, æœ¬èº«å·²ç¶“æ˜¯ deterministic function of Î¸.  ä¸¦é random variable.  ç•¶ç„¶ä¹Ÿæ²’æœ‰æ‰€è¬‚çš„ sample information.  ä¸éæˆ‘å€‘å°±é¡æ¨ Sample information = -1 * Sample hessian, å› ç‚º expectation of a value = value. 

å¾å¤§æ•¸æ³•å‰‡ä¾†çœ‹:  Information â‰ˆ Sample information / n.  

log likelihood l(Î¸;Z) åŸºæœ¬ä¸Šå¤§å¤šæ˜¯ concave funciton (ç•¶ç„¶æœ‰ä¾‹å¤–ï¼Œç‰¹åˆ¥æ˜¯åœ¨ mixture pdf), æ‰€ä»¥åŸå‰‡ä¸Š Hessian and Sample hessian: H(Î¸; Z) â‰¦ 0

åä¹‹ information matrix J or Fisher information I(Î¸) æ˜¯ covaraince matrix (or variance for single variable), éƒ½æ˜¯ positive or postive semi-definite matrix.  å¾Œé¢è¨¼æ˜å› ç‚º E[Score] = 0, å› æ­¤ information matrix ä¹Ÿå°±æ˜¯ Score çš„ variance.  

æœ€ç¥å¥‡çš„æ˜¯ J (or I a KxK matrix) = Cov[ s(Î¸o; Z) ] = E[ s(Î¸o;Z) s(o;Z)' ] =  - E[ H(Î¸o; Z) ]  !!  è¨¼æ˜è¦‹ Appendix.

 

## Fisher Information çš„ç‰©ç†æ„ç¾©æ˜¯ä»€éº¼? 

é¦–å…ˆ Fisher information æ˜¯ Score (function of Z) random variable çš„ variance (or covaraince matrix).

Fisher information åªæ˜¯ function of Î¸.  Z, as a random variable åœ¨åš expectation è¨ˆç®—ä¸­æ¶ˆå¤±ã€‚

æ‰€ä»¥ given Î¸o and Z çš„ pdf.  å¯ä»¥å¾ˆå®¹æ˜“ç®—å‡º Fisher Information çš„ç†è«–å€¼ã€‚

å¯¦å‹™ä¸Šå‰‡éå¦‚æ­¤ã€‚å› ç‚º Î¸o æ˜¯æˆ‘å€‘è¦ estimate çš„åƒæ•¸!  

###  

### (1) æˆ‘å€‘åªæœ‰ given samples and Z çš„ pdf.  å¦‚ä½•æ¨ç®— Sample information?  

### (2) å¦å¤–ä¸€å€‹å•é¡Œæ˜¯ Fisher Information ç‚ºä»€éº¼é‡è¦? 

å…ˆèªªçµè«– (2):  Sample Information å°±æ˜¯ç”¨ Zi ä¼°è¨ˆ Î¸o æ‰€å¾—åˆ° Î¸est çš„ variance (or covaraince matrix) çš„å€’æ•¸, i.e. Cov[Î¸est - Î¸o]^(-1).  æ‰€ä»¥ Information æ„ˆå¤§ï¼ŒÎ¸est å°±æ„ˆæ¥è¿‘ Î¸o.  å¦‚ä½•å¢åŠ  Sample information, (1) å¢åŠ  samples n; (2) å¢åŠ  Var(Score^2) 

å›åˆ°ç¬¬ä¸€å€‹å•é¡Œï¼Œæ²’æœ‰ Î¸o, å¦‚ä½•æ¨ç®— Sample information. é›–ç„¶æ²’æœ‰ Î¸o, ä½†æœ‰ n å€‹ samples, Zi.

é¡¯ç„¶è§£æ³•æ˜¯å…ˆç”¨ Sample Zi ä¼°è¨ˆ Î¸o as Î¸est.  

Method 1:  ç”¨ Î¸est ä»£å…¥ Fisher Information å¯ä»¥å¾—åˆ° (unit) Fisher information.  Sample information = n * Fisher information  

Method 2:  ç”¨ Î¸est and Zi è¨ˆç®— Sample score, Sample hessian, and Sample Information.  ç•¶ç„¶æˆ‘å€‘å‡è¨­ Sample information = -1 * Sample hessian.  å¾å¦å¤–ä¸€å€‹è§’åº¦æƒ³ï¼Œå°±æ˜¯æŠŠ Z1, Z2, .. Zn æƒ³æˆä¸€å€‹ joint PDF or likelihood function.  åŒæ¨£å¯ä»¥ç®—å‡º Fisher information.  ä»£å…¥ Î¸est å°±æ˜¯é€™å€‹ sample (Z1, Z2, â€¦, Zn) çš„ Sample information.

ç›´è§€ä¾†èªª Method 1 å’Œ Method 2 åœ¨ n å¾ˆå¤§æ™‚ï¼Œæœƒè¶¨è¿‘ç›¸ç­‰(by å¤§æ•¸æ³•å‰‡)ã€‚ä½†åœ¨ sample ä¸å¤šæ™‚ (e.g. n = 2) å¦‚ä½•?

çµè«–ä¼¼ä¹ç›¸åŒã€‚åªè¦æ˜¯ Z1, Z2, .. Zn æ˜¯ IID.  

å¯ä»¥åƒè€ƒ[æœ¬æ–‡](http://www.econ.uiuc.edu/~wsosa/econ507/MLE.pdf) (p41. variance estimation) çµ¦å‡ºä¸‰å€‹æ–¹æ³•ã€‚Method 1 å°æ‡‰ (3); Method 2 å°æ‡‰ (1); é‚„æœ‰ (2) æ˜¯ç”¨ sample score å¹³æ–¹ (or empirical variance of score) ä¾†ç®—ã€‚

 

å¯¦éš›(ç‰©ç†)ä¸Šçš„ Information ç•¶ç„¶é‚„æ˜¯æœ‰ n å€‹ samples çš„ Sample information.  å¯ä»¥çœ‹åˆ° samples æ„ˆå¤šï¼ŒSample hessian å°±æ„ˆè² ã€‚ä¹Ÿå°±æ˜¯ Information å°±æ„ˆå¤§æ„ˆå¤šã€‚ (Sample) Information ~ n.

å¦‚æœæˆ‘å€‘ç•« log likelihood function of n-samples w.r.t Î¸ (e.g. of Normal distribution), æœƒçœ‹åˆ°ä¸€å€‹ concave function of Î¸.  

åœ¨ Sample score = 0 çš„ Î¸,  ä¹Ÿç¨±ç‚º Î¸mle.  å°æ‡‰çš„æ˜¯ log likelihood function çš„é ‚é»ã€‚Sample information å°æ‡‰çš„å°±æ˜¯é ‚é» (éœ€è¦æ˜¯é ‚é»å—? é‚„æ˜¯æ¯ä¸€é»éƒ½ ok?) çš„æ›²ç‡ã€‚é¡¯ç„¶ n æ„ˆå¤§ï¼Œsum of log likelihood function åœ¨é ‚é»çš„æ›²ç‡ä¹Ÿæ„ˆå¤§ã€‚æ›²ç‡çš„å€’æ•¸å°±æ˜¯ Î¸est çš„æº–ç¢ºåº¦ or varaince.  é€™æ˜¯åªæœ‰åœ¨ MLE é©ç”¨?  æˆ–æ˜¯é MLE estimator ä¹Ÿé©ç”¨? 

 

## Examples 

æˆ‘å€‘ç”¨ normal distribution with u = Î¸ (mean is the estimated parameter) and known Ïƒ=1 (variance) with two sample Z1, Z2 ç‚ºä¾‹ã€‚

å¾ˆç›´è§€ u â‰ˆ Î¸est = (Z1+Z2)/2

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage60.png)

Fisher information of a Normal distribution = 1 assuming ğœ = 1.

 

Method 1: Fisher information = 1 => Sample information = 2 ä¸ç®¡ Î¸est æ˜¯å¤šå°‘ã€‚

Method 2: å¦‚æœä»£å…¥ Z1 and Z2 into Sample hessian = hessian(Z1) + hessian(Z2) = -2 (ä¹Ÿæ˜¯å’Œ Z1, Z2 ç„¡é—œ).  å› æ­¤ Sample information = -1 * -2 = 2.  çµæœå’Œ method 1 ç›¸åŒã€‚

å…¶å¯¦åªè¦ Fisher information å’Œ Î¸ ç„¡é—œ (å¦‚ normal distribution, exponential distribution).  Method 1 and 2 çµæœæœƒä¸€æ¨£ã€‚

 

å†ä¾†çœ‹ Bernoulli distribution.  åƒè€ƒ http://www.ejwagenmakers.com/submitted/LyEtAlTutorial.pdf

 ![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage61.png)

Method 1:  Î¸est = 1/2(Z1+Z2)  =>  I(Î¸) = 1/[Î¸(1-Î¸)]

=>  Sample information = 2 * I(Î¸) = 2 / [0.5(Z1+Z2) * (1-0.5(Z1+Z2))]

Z1, Z2, Î¸est, Sample information (n=2) åˆ†åˆ¥å¦‚ä¸‹:

0, 0, 0, âˆ

0, 1, 0.5, 8

1, 0, 0.5, 8

1, 1, 1, âˆ

 

Method 2:  å› ç‚º Sample information = (-1) * Sample hessian = (-1) * [ sum of each hessian ] 

Z1, Z2, Î¸est, Sample information (n=2) åˆ†åˆ¥å¦‚ä¸‹:

0, 0, 0, âˆ

0, 1, 0.5, 8

1, 0, 0.5, 8

1, 1, 1, âˆ

ä¼¼ä¹åªè¦ Î¸est ç›¸åŒï¼Œmethod 1 and method 2 çš„çµæœä¸€æ¨£ã€‚é€™å¥½åƒ make sense.  å› ç‚ºåªè¦ I(Î¸) æ˜¯å¾ pdf/log likelihood ç®—å‡ºä¸” IID.  ?

ä»€éº¼æ™‚å€™æœƒ Fisher information*n å’Œ Sample information ä¸åŒ?  å°±æ˜¯åœ¨ Fisher information æ˜¯ç”¨çœŸçš„Î¸o å¾—å‡º (hidden variable), ä½†æ˜¯ sample information å»æ˜¯å¾ Z1, Z2, .., Zn å¾—å‡ºã€‚å› ç‚º Fisher information æ˜¯ weighted (by pdf) information. ä½†æ˜¯ Sample information æ˜¯ä¸€æ¬¡çš„ samples (Z1, â€¦, Zn) å¾—å‡ºçš„ Fisher information.  

åƒè€ƒæœ¬æ–‡çš„[ä¾‹å­](http://www.ejwagenmakers.com/submitted/LyEtAlTutorial.pdf)ã€‚ 

 

æ‰€ä»¥ consistent estimator å°±æ˜¯è®“ Î¸est å’ŒçœŸå¯¦çš„ Î¸o è¶¨è¿‘: consistency

ä»¥åŠ efficient estimtor å°±æ˜¯ Sample information under Î¸est è¶¨è¿‘ Fisher information (under Î¸o) * n:  efficiency

å¦å¤–é‚„è¦åŠ ä¸€å€‹ asymptotic normality æ¢ä»¶ã€‚

 

### Fisher åœ¨ 1925 å¾ consistency å’Œ efficiency è§’åº¦æå‡º maximum likelihood estimator (MLE) æ˜¯ consistent and efficient estimator.  è€Œä¸éœ€è¦ç”¨åˆ° sufficient statistic.  å› ç‚ºæœ‰äº› distribution çš„ sufficient statistic ä¸å­˜åœ¨ã€‚ä½†æ˜¯ MLE ä»å­˜åœ¨ã€‚ Rao åœ¨ 1963 è¨¼æ˜é€™é»ã€‚

 

### Asymptotic Normality of Maximum Likelihood Estimator

å›åˆ°å¯¦å‹™é¢ã€‚ä¸€èˆ¬æˆ‘å€‘åªæœ‰ Z1, Z2, â€¦, Zn observed samples ä»¥åŠ Z random variable çš„ PDF.

å…ˆå¾ Sample Score (deterministic function of Î¸) = 0 å¾—å‡º maximum likelihood estimation, Î¸n.

###  ![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage63.png)

å› ç‚º MLE æ˜¯ consistent estimator, åœ¨ n å¤ å¤§æ™‚ Î¸n â†’ Î¸o.  å¯ä»¥åš 1st order Taylor expansion.

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage64.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage66.png)

 

ä¸‹é¢é‡çµ„ä¸¦å¼•å…¥ n åšç‚º normalization constant.  åœ¨ Sample hessian éƒ¨ä»½è¢« normalized to unit Sample hessian.

å°æ‡‰çš„ n è¢«åˆ†ç‚º âˆšn * âˆšn.  ä¸€å€‹æ”¾åœ¨ Sample score çš„åˆ†æ¯ï¼Œå¦ä¸€å€‹æ”¾åœ¨ estimated error çš„åˆ†å­ã€‚

### ![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage67.png)

æ¥ä¸‹ä¾†å°±æ˜¯æœ€ç²¾é‡‡çš„éƒ¨ä»½:

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage69.png)

é€™æ‡‰è©²å®¹æ˜“çœ‹å‡ºã€‚è—‰è‘— ergodic ç‰¹æ€§ï¼Œtime average ç­‰æ–¼ expectation value.

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage70.png)

å†ä¾†å°±ä¸æ˜¯é‚£éº¼ç›´è¦ºï¼ŒNormalized sample score (by âˆšn) æœƒæ”¶æ­›åˆ° zero mean, variance ç‚º Fisher information çš„ normal distribution. 

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage71.png)

Zero mean å¾ˆå®¹æ˜“äº†è§£ã€‚å› ç‚º E[Score] = E[s(Î¸o;Z)] = 0

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage72.png)

åŒæ¨£ by ergodic property,  Sample score çš„å¹³å‡å€¼ (/n), æœƒæ”¶æ­›åˆ° E[s(Î¸o;Z)] = 0.  å³ä½¿ * âˆšn = 0

å¦å¤– Var[Score] = Var[s(Î¸o;Z)] = J.  æ‰€ä»¥ variance æœƒæ”¶åˆ° âˆšn çš„ boost.  è®Šæˆ n/n^2 = 1/n.  ä¹Ÿå°±æ˜¯ unit variance çš„å¹³å‡å€¼ã€‚

By Central Limit Theory (CLT), å¯ä»¥ show æœ€å¾Œæ”¶æ­›åˆ° N(0, J).

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage73.png)

å› æ­¤ estimated error, |Î¸n-Î¸o|, è§’åº¦æ˜¯åæ¯”æ–¼ âˆšn; å¾ Î¸n variance è§’åº¦å‰‡æ˜¯åæ¯”æ–¼ n

## |Î¸mle-Î¸o| æ”¶æ­›çš„é€Ÿåº¦å’Œ âˆšn æ­£æ¯”ã€‚æˆ–æ˜¯ Î¸mle çš„ variance å’Œ n æˆåæ¯”ï¼Œä¹Ÿå’Œ Fisher information åæ¯”ã€‚ 

 

## Rao-Cramer Bound or Inequality

## ![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage74.png)

æ³¨æ„ psd æ˜¯ positive semi-definite.  V(Î¸*) = Var(Î¸*)

å‰æ–‡å·²æåˆ° MLE æ˜¯ Var(Î¸mle) â†’  (nJ)^(-1).  å› æ­¤ MLE æ¯”å…¶ä»–çš„ unbiased estimator æ›´ efficient!

The proof is in appendix. 

 

## Appendix

ä¸Šè¿°ç”¨åˆ°çš„å¹¾å€‹ç­‰å¼çš„è¨¼æ˜ 

![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage56.png)

å¯ä»¥çœ‹åˆ°æ˜¯ç”¨ random variable Z çš„ Score and Hessian (é Sample score and Sample hessian).

åŒæ™‚ç”¨ Î¸o æš—ç¤º give a fixed Î¸.  æ³¨æ„ Î¸o ä¸éœ€è¦æ˜¯ Î¸mle, for any Î¸o ä»¥ä¸Šç­‰å¼éƒ½ç‚ºçœŸã€‚

 

**Proof of Score equality**

 

 ![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage57.png)

 ![NewImage](http://allenlu2007.files.wordpress.com/2016/07/mlenewimage58.png)

 

**Proof of E[Score] and Information equality**

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage5.png)

 

 

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage6.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage7.png) 

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage8.png)


![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage9.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage10.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage11.png)

![NewImage](http://allenlu2007.files.wordpress.com/2016/08/mlenewimage12.png)

 

**Proof of Rao-Cramer Inequality**

Use DK divergence >= 0 to prove. 

 

**Proof of MSL Consistency** 

 

### Relationship to KL divergence

 



, sufficient statistics, score, Fisher information, Rao-Cramer bound ç­‰éƒ½å’Œæ­¤æœ‰é—œã€‚

âˆ‚L(Î¸;X)/âˆ‚Î¸ = 0.  ç‚ºä½• pdf çš„ maximum point å¯ä»¥ç”¨ä¾†åš estimation.  ä¹çœ‹ make sense, ä½†å¦‚æœ pdf æœ‰å…©å€‹ max, æˆ–æ˜¯ uniform distribution?  çµæœå¦‚ä½•?

