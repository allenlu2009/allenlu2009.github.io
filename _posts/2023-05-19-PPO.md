---
title: Math Optimization - PPO 
date: 2023-05-11 23:10:08
categories:
- Math_AI
tags: [SDE, Stochastic equations, diffusion equations, reverse-time equations, Fokker-Planck equations]
typora-root-url: ../../allenlu2009.github.io

---


## Source



## Introduction

**在 optimization 最挑戰的問題之一是不可微分性。**

我們在 convex optimization 不可微分點使用 sub-differential 就看到不同的算法處理不可微分點。

例如 sub-gradient method, 或是 proximal operator.

不過 convex optimization 只是少數幾個點。在 reinforcement learning 遇到的情況是 discrete sequence decision 所造成的結果。Discrete sequence decision 是 discrete 而不可微分?

解法就是引入概率。這和 gradient descent (GD) 變成 stochastic gradient descent (SGD) 基本一樣？

因爲概率分佈是連續可微分。Discrete sequence decision (policy) 可以視爲一個 sample.





## PPO (Proximal Policy Optimization) for Reinforcement Learning

我們先 review policy.







### Policy Gradient

<img src="\media\image-20230511221410758.png" alt="image-20230511221410758" style="zoom:50%;" />

用玩 game 做例子。

<img src="\media\image-20230512200935232.png" alt="image-20230512200935232" style="zoom:50%;" />

<img src="\media\image-20230512201003504.png" alt="image-20230512201003504" style="zoom:50%;" />



## Appendix

