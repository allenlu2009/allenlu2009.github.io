---
title: LLM KV Cache Memory and BW
date: 2023-10-29 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
typora-root-url: ../../allenlu2009.github.io


---





## Source

* [大模型推理性能优化之KV Cache解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/630832593)
* The KV Cache: Memory Usage in Transformers  https://www.youtube.com/watch?v=80bIUggRJf4&ab_channel=EfficientNLP
* [[LLM\]KV cache详解 图示，显存，计算量分析，代码 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/646577898)



TBD

* [Efficient Memory Management for Large Language Model Serving with PagedAttention (arxiv.org)](https://arxiv.org/pdf/2309.06180.pdf)

* Flash Attention with attention bias:  https://zhuanlan.zhihu.com/p/567167376

* Flash attention 2: https://tridao.me/publications/flash2/flash2.pdf

* Flash Decoder: https://princeton-nlp.github.io/flash-decoding/

* 詳細的 GPT2/3 參數計算: https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters

* GPT3 原始 paper.

* GPT2 原始 paper.

* LLM1,  https://finbarr.ca/how-is-llama-cpp-possible/

* [FlashAttention图解（如何加速Attention） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/626079753)

* [NLP（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/638468472)

  

<img src="/media/image-20231029165342034.png" alt="image-20231029165342034" style="zoom: 67%;" />

## Takeaway

* KV cache 利用額外的記憶體 (DRAM or HBM) 減少 transformer 中 K, V 的計算量，同時減小 output token 的 latency.

* KV cache 的副作用是: 1. 額外的記憶體;  2. 額外的記憶體頻寬



* Attention block:  KV cache memory management on (on-die) SRAM and the 1st memory (off-die memory).

* KV cache split, KV flash, KV decode

* dynamic cache



## 0. 引言

做大模型性能优化的一定对KV Cache不陌生，那么我们对这个技术了解到什么程度呢？请尝试回答如下问题：

1. KV Cache节省了Self-Attention层中哪部分的计算？
2. KV Cache对MLP层的计算量有影响吗？NO
3. KV Cache对block间的数据传输量有影响吗？

本文打算剖析该技术并给出上面问题的答案。



## 1. KV Cache是啥？

大模型推理性能优化的一个常用技术是KV Cache，该技术可以在不影响任何计算精度的前提下，**通过空间换时间思想，提高推理性能。**网上有一些关于该技术的分析博客，但读过后仍然会很迷糊，甚至可能会被带偏，认为这个Cache过程和数据库读取或CPU Cache加速类似的荒谬结论。刚开始我也有类似误解，直到逐行查阅并运行源码，才清楚了解到其Cache了啥，以及如何节省计算的。

## 2. 背景

生成式generative模型的推理过程很有特点，我们给一个输入文本 (長度為 $s$)，模型会输出一个回答（长度为 $n$），其实该过程中执行了$n$ 次推理 (inference) 过程。即GPT类模型一次推理只输出一个token，输出token会与输入tokens 拼接在一起，然后作为下一次推理的输入，这样不断反复直到遇到终止符。

如上描述是我们通常认知的GPT推理过程。代码描述如下：

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer


model = GPT2LMHeadModel.from_pretrained("/WORK/Test/gpt", torchscript=True).eval()

# tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("/WORK/Test/gpt")
in_text = "Lionel Messi is a"
in_tokens = torch.tensor(tokenizer.encode(in_text))

# inference
token_eos = torch.tensor([198]) # line break symbol
out_token = None
i = 0
with torch.no_grad():
    while out_token != token_eos:
        logits, _ = model(in_tokens)
        out_token = torch.argmax(logits[-1, :], dim=0, keepdim=True)
        in_tokens = torch.cat((in_tokens, out_token), 0)
        text = tokenizer.decode(in_tokens)
        print(f'step {i} input: {text}', flush=True)
        i += 1

out_text = tokenizer.decode(in_tokens)
print(f' Input: {in_text}')
print(f'Output: {out_text}')
```

輸出

```
step 0 input: Lionel Messi is a player
step 1 input: Lionel Messi is a player who
step 2 input: Lionel Messi is a player who has
step 3 input: Lionel Messi is a player who has been
step 4 input: Lionel Messi is a player who has been a
step 5 input: Lionel Messi is a player who has been a key
step 6 input: Lionel Messi is a player who has been a key part
step 7 input: Lionel Messi is a player who has been a key part of
step 8 input: Lionel Messi is a player who has been a key part of the
step 9 input: Lionel Messi is a player who has been a key part of the team
step 10 input: Lionel Messi is a player who has been a key part of the team's
step 11 input: Lionel Messi is a player who has been a key part of the team's success
step 12 input: Lionel Messi is a player who has been a key part of the team's success.
step 13 input: Lionel Messi is a player who has been a key part of the team's success.

 Input: Lionel Messi is a
Output: Lionel Messi is a player who has been a key part of the team's success.
```

可以看出如上计算的问题吗？每次推理过程的输入tokens都变长了 ($n_{ctx}$)，导致推理FLOPs随之增大。有方法实现推理过程的FLOPs基本恒定不变或变小吗？（*埋个伏笔，注意是基本恒定*）。



## 3. 原理

b: batch

s: sequence length

h: model input dimension

在上面的推理过程中，每 step 内，输入一个 token序列，经过Embedding层将输入token序列变为一个三维张量[b, s, h]，经过一通计算，最后经logits层将计算结果映射至词表空间，输出张量维度为[b, s, vocab_size]。

**以上 GPT2 code 爲例： b = 1;  s 會每次加 1, 最大到 $n_{ctx}$ ;  h = 768;  vocab_size = 50257**

当前轮输出token与输入tokens拼接，并作为下一轮的输入tokens，反复多次。可以看出第 $i+1$ 轮输入数据只比第 $i$ 轮输入数据新增了一个token，其他全部相同！因此第 $i+1$轮推理时必然包含了第 $i$ 轮的部分计算。

從 attention block 的角度來看，就是下面的 $x_i : [b, s, h]$  到下次 $x_{i+1} : [b, s+1, h]$  每次 token 長度都會加一。計算量也會變大。

step $i$:

* 計算 Q, K, V:   input 和 output shape at i step:  $[b, s, h] \times [h, h] \to  [b, s, h]$  
* 計算 $Q K^T$  矩陣的 input 和 output $ [b, head_{num}, s, d_{head}] \times [b, head_{num}, s, d_{head}] \to [b, head_{num}, s, s]$

step $i+1$:

* 計算 Q, K, V:   input 和 output shape at i+1 step:  $[b, s+1, h] \times [h, h] \to  [b, s+1, h]$  
* 計算 $Q K^T$  矩陣的 input 和 output $ [b, head_{num}, s+1, d_{head}] \times [b, head_{num}, s+1, d_{head}] \to [b, head_{num}, s+1, s+1]$



<img src="/media/image-20231029190338478.png" alt="image-20231029190338478" style="zoom: 33%;" />

開始是 $s=1, 2, ...$, 直到最後 $s = n_{ctx}$ (maximum context length, GPT2 = 1024).   此時已到達 sequence lengthh 的上限 .  接下來每次進來的 token 都會 shift 掉一個最前面的 token.   也就是 $x_{i+1}$ 是 shifted $x_i$.

<img src="/media/image-20231029192355602.png" alt="image-20231029192355602" style="zoom: 33%;" />

最暴力的方法是每次都計算大的矩陣乘法。但是如果我們可以緩存前一次的 (key, value) 值。是否可以減少重算下一次的 (key, value)?

<img src="/media/image-20231029192501455.png" alt="image-20231029192501455" style="zoom: 33%;" />



**KV Cache的出发点就在这里，缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果，就是这么简单，不存在什么Cache miss问题。** 

SM stands for SoftMax.

<img src="/media/image-20231029202752642.png" alt="image-20231029202752642" style="zoom:33%;" />

例如在輸入新的 token “chill"，之前的 "cold" 對應的 K vector 和 attention score (V) 其實都不用重算。只需要計算新的 "chill" 對應的 vector 和 attention score (V)   

問題：如果是 bi-directional BERT 就需要重算, right?



<img src="/media/image-20231029203045625.png" alt="image-20231029203045625" style="zoom:33%;" />

<img src="/media/image-20231029204503470.png" alt="image-20231029204503470" style="zoom:33%;" />

問題：緩存的做法是否可以用在 FFN (feedforward block)?  好像不行?  因爲 FFN input vector shift 之後對應的 weights 就會完全不同？可是 attention 對應的 score 是 position independent?  **只有 Attention 有 context and KV cache gain!**



#### KV Cache Memory Usage:

KV parameter count: $2 b s h l$;   Memory size: $2 bshl  \text{ *precision}$

<img src="/media/image-20231029205351222.png" alt="image-20231029205351222" style="zoom: 50%;" />









## 实现细节

目前各大模型推理都实现了KV Cache，下面就看如何使用了。我们可以在上面代码基础上修改，主要改动：

- 在推理时新增了 past_key_values 参数，该参数就会以追加方式保存每一轮的K V值。kv cache变量内容为((k,v), (k,v), ..., (k,v))，即有 $n_{layers}$ 个 k,v 组成的一个元组，其中 k 和 v 的维度均为 $[b, n_{head}, s, d_{head}]$。这里可以顺带计算出每轮推理对应的 cache 数据量为 2∗b∗s∗ℎ∗$n_{layers}$ ，这里 s 值等于当前轮次值。以GPT3-175B为例，假设以 float16 来保存 KV cache，senquence长度为100，batchsize=1，则 KV cache占用显存为 2×100×12288×96×2 Byte= 472MB。
- 推理输出的token直接作为下一轮的输入，不再拼接，因为上文信息已经在 kvcache 中。

代码示例：

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer


model = GPT2LMHeadModel.from_pretrained("/WORK/Test/gpt", torchscript=True).eval()

# tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("/WORK/Test/gpt")
in_text = "Lionel Messi is a"
in_tokens = torch.tensor(tokenizer.encode(in_text))

# inference
token_eos = torch.tensor([198]) # line break symbol
out_token = None
kvcache = None
out_text = in_text
i = 0
with torch.no_grad():
    while out_token != token_eos:
        logits, kvcache = model(in_tokens, past_key_values=kvcache) # 增加了一个 past_key_values 的参数
        out_token = torch.argmax(logits[-1, :], dim=0, keepdim=True)
        in_tokens = out_token # 输出 token 直接作为下一轮的输入，不再拼接
        text = tokenizer.decode(in_tokens)
        print(f'step {i} input: {text}', flush=True)
        i += 1
        out_text += text

print(f' Input: {in_text}')
print(f'Output: {out_text}')
```



通过上面代码只能看到调用层面的变化，实现细节还需看各框架的底层实现，例如Hugging Face的transformers库代码实现就比较清爽，在[modeling_gpt2.py](https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py%23L319)中Attention部分相关代码如下：



```python
   query = self._split_heads(query, self.num_heads, self.head_dim)
        key = self._split_heads(key, self.num_heads, self.head_dim)
        value = self._split_heads(value, self.num_heads, self.head_dim)

        if layer_past is not None: # 当输出第一个token后，layer_past就是非None了
            past_key, past_value = layer_past # 取出之前计算好的 key, value
            key = torch.cat((past_key, key), dim=-2) # past_key 与当前 token 对应的 key 拼接
            value = torch.cat((past_value, value), dim=-2) # past_value 与当前 token 对应的 value 拼接

        if use_cache is True:
            present = (key, value)
        else:
            present = None
```



其实，KV Cache 配置开启后，推理过程可以分为2个阶段：

1. 预填充阶段 ($s = 1,2, .., n_{ctx}$)：发生在计算第一个输出token过程中，这时Cache是空的，计算时需要为每个 transformer layer 计算并保存key cache和value cache，在输出token时Cache完成填充；FLOPs同KV Cache关闭一致，存在大量 GEMM 操作，推理速度慢。 **正常推理，预存 key-value cache；compute-bound 计算**
2. 使用KV Cache阶段：发生在计算第二个输出token至最后一个token过程中，这时Cache是有值的，每轮推理只需读取Cache，同时将当前轮计算出的新的Key、Value追加写入至Cache；FLOPs降低，GEMM 变为 GEMV 操作，推理速度相对第一阶段变快，这时属于Memory-bound类型计算。**memory-bound 计算**

这里用图可能更有助理解，下图是一个Decoder Block，含有Self-Attention和MLP，标红部分为KV Cache影响到的内容，即KV Cache开启后，标红的序列长度 s 变为 1，当batch_size=1时，Self-Attention中的2个dense全都变为gemv操作，MLP中的dense也全都变为gemv操作。看懂这个图就可以答对上面的3个问题啦。

![ ](https://pic2.zhimg.com/80/v2-6f6b895d6d37154654ffcc13bd23bf9d_720w.webp)



如下链接也有这方面的定量分析，写的很棒，推荐大家看看。

[回旋托马斯x：分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)





### 总结

KV Cache是Transformer推理性能优化的一项重要工程化技术，各大推理框架都已实现并将其进行了封装（例如 transformers库 generate 函数已经将其封装，用户不需要手动传入past_key_values）并默认开启（config.json文件中use_cache=True）。



#### KV Cache for Inference (主要用於推理)

**KV Cache 的主要工作是減少 computation!  不是 DRAM BW reduction!  剛好相反，KV cache 會增加 DRAM bandwidth.**

**本质上是“空间换時间”。**

1. 0-cache  每次都要從 DRAM 讀 parameter 計算所有 output token:
   * DRAM BW = parameter size x output token/sec
   * Computation = 2 x parameter size TOPS

2. KV cache 假設 internal SRAM = parameter size + KV cache:  理論上 DRAM access 只需要一次?
   * DRAM BW = parameter size x output token/sec + KV cache size x 6 x output token/sec?
   * **DRAM BW / token = parameter size + KV cache size x 6 (讀幾次? 寫幾次?)**
   * **Computation = ??  TOPS (減少 s 倍)  見下文**




是否可能 “時間換空間"?  On-die 7GB 或是 3.5GB SRAM，不可能！





在推断阶段，transformer模型加速推断的一个常用策略就是使用 KV cache。一个典型的大模型生成式推断包含了两个阶段：

1. **预填充阶段**：输入一个prompt序列，为每个transformer层生成 key cache和value cache（KV cache）。
2. **解码阶段**：使用并更新KV cache，一个接一个地生成词，当前生成的词依赖于之前已经生成的词。



第 $i$个transformer层的权重矩阵为 $W_Q^i, W_K^i, W_V^i, W_O^i, W_1^i, W_2^i$。

其中 self-attention 的 4 個權重矩陣  $W_Q^i, W_K^i, W_V^i, W_O^i \in R^{h \times h}$。

并且MLP块的2个权重矩阵 $W_1^i \in R^{h \times 4h}, W_2^i \in R^{4h \times h}$。



**预填充阶段**

假设第 $i$个transformer层的输入为 $x^i$ ，self-attention块的key、value、query和output表示为 $x_K^i, x_V^i, x_Q^i, x_{out}^i$ 其中 $x_K^i, x_V^i, x_Q^i, x_{out}^i \in R^{b\times s\times h}$。

Key cache 和 value cache 的計算過程為

$x_K^i = x^i \cdot W_K^i$

$x_V^i = x^i \cdot W_V^i$

第 $i$ 個 transformer 層剩餘的計算過程為

<img src="/media/image-20231022220017738.png" alt="image-20231022220017738" style="zoom: 67%;" />

**解码阶段**

给定当前生成词在第 $i$ 个transformer层的向量表示为 $t^i \in R^{b \times 1 \times h}$.  推理計算分兩部分：更新 KV cache 和計算第 $i$ 個 transformer 層的輸出。

更新 key cache 和 value cache 的計算過程如下：

<img src="/media/image-20231022220801197.png" alt="image-20231022220801197" style="zoom: 67%;" />

## 计算量減少分析：

输入数据的形状为 [b,1,ℎ]，kv cache中含有kv_length个past word。我们**先分析self-attention块的计算**，

1. 计算 Q, K, V ：矩阵乘法的输入和输出形状为 [b,1,ℎ]×[ℎ,ℎ]→[b,1,ℎ] 。计算量为 3∗2bℎ2=6bℎ2 。

2. QK^T 矩阵乘法的输入和输出形状为[b, head_num, 1, per_head_hidden_size]×[b, head_num, per_head_hidden_size, kv_length+s]→[b,head_num,1,kv_length+1] 。计算量为 2bs (kv_length +1 )ℎ 。

1. 计算在V上的加权 score . V ，矩阵乘法的输入和输出形状为 [b, head_num, 1, kv_length+1]×[b,head_num,kv_length+1,per_head_hidden_size]→[b,head_num,1,per_head_hidden_size] 。计算量为 2bs(kv_length +1)ℎ 。

2. attention后的线性映射，矩阵乘法的输入和输出形状为 [b,1,ℎ]×[ℎ,ℎ]→[b,1,ℎ] 。计算量为 2bℎ^2 。

   

**接下来分析MLP块的计算，计算公式如下**：

1. 第一个线性层，矩阵乘法的输入和输出形状为 [b,1,ℎ]×[ℎ,4ℎ]→[b,1,4ℎ] 。计算量为 8bℎ2 。
2. 第二个线性层，矩阵乘法的输入和输出形状为 [b,1,4ℎ]×[4ℎ,ℎ]→[b,1,ℎ] 。计算量为 8bℎ2 。

将上述计算量相加，得到**每个transformer层的计算量大约为** 24 b h2 + 4bℎ+4b(kv_length )ℎ 。

不采用kv cache时为： 24 b sh ℎ2+4 b s2 ℎ

此外，另一个计算量的大头是logits的计算，将隐藏向量映射为词表大小。矩阵乘法的输入和输出形状为 [b,1,ℎ]×[ℎ,V]→[b,1,V] ，计算量为 2bℎV 。

不采用kv cache时为： 2bsℎV



**Attention 的計算量可以節省 s 倍！  Really??**



### KV cache 額外的显存占用分析

<img src="/media/image-20231029093628970.png" alt="image-20231029093628970" style="zoom:33%;" />

* 存储 kvlength 个K｜V value，形状为 [b, head_num, kv_seq_len, head_dim]，

* 显存占用为： 4blh(kv_length)

假设输入序列的长度为 $s$，输出序列的长度为  $n$，以float16来保存KV cache，那么**KV cache的峰值显存占用大小为** $b(s+n)h*l*2*2 = 4blh(s+n)$。**这里第一个2表示K/V cache，第二个2表示float16占2个bytes。**

* Training 的中間激活時 : $34 blsh + 11 b l s^2 a$,  KV cache 只存了 attention 中的 K and V 部分，有包含 score?
* Model 參數量是 $12 l h^2$ (和 b, s 無關！),  假設是 16-bit,  Model 内存是  $24 l h^2$
* 假設 inference $b=1$ (這不一定是對的，在 speculative decode, 大 model 的 $b > 1$):   KV cache : $4 blh (s+n)$.   KV cache / model parameter ~ $b (s+n) / 6 h$!   對於 long context,  $s + n$ 可能會大於 $h$!!  $s$ 就是 $n_{ctx}$,  $h$ 就是 $d_{model}$
* 以 Llama2-7B 爲例,  $h = 4096$,  但是 $n_{ctx} 最大也有 4096$!

##### 

#### Example Llama2 (4A16W)

以 Llama2-7B 爲例。

| 模型名     | 参数量 | 层数, l | 隐藏维度, h | 注意力头数 a | Context s |
| ---------- | ------ | ------- | ----------- | ------------ | --------- |
| Llama2-7B  | 7B     | 32      | 4096        | 32           | 4096      |
| Llama2-13B | 13B    | 40      | 5120        | 40           | 4096      |
| Llama2-33B | 33B    | 60      | 6656        | 52           | 4096      |
| Llama2-70B | 70B    | 80      | 8192        | 64           | 4096      |

Llama2 的模型参数量为7B，占用的显存大小为 **(INT8**) 7Bx2 = 7GB 。假設 activation 是 FP16.

假設 Llama2 的序列长度 $s$ 为 2048 。对比不同的批次大小 $b$ 占用的中间激活：

当 b=1 时，KV cache 占用显存为 $(4bsh)*l$ byte ≈1GB ，大约是模型参数显存的15%。

假設 Llama2 的序列长度 $s$ 为 4096 。对比不同的批次大小 $b$ 占用的中间激活：

当 b=1 时，KV cache 占用显存为 $(4bsh)*l$ byte ≈2.1GB ，大约是模型参数显存的31%。

如果 model 是 4-bit (4W16A)  7Bx0.5 = 3.5GB, 更糟糕:  KV cache 佔的比例 double.   



#### Example GPT3-175B (8A16W)

以GPT3-175B为例，我们来直观地对比下模型参数与中间激活的显存大小。GPT3的模型配置如下。我们假设采用混合精度训练，模型参数和中间激活都采用float16数据类型，每个元素占2个bytes。

| 模型名 | 参数量 | 层数, l | 隐藏维度, h | 注意力头数 a |
| ------ | ------ | ------- | ----------- | ------------ |
| GPT3   | 175B   | 96      | 12288       | 96           |

GPT3的模型参数量为175B，占用的显存大小为 1×175B = 175GB for inference。

GPT3的序列长度 $s$ 为 2048 。对比不同的批次大小 $b$ 占用的中间激活：

b=1 ，输入序列长度 s=2048,  中间激活占用显存为 $(4bsh)*l$ byte ≈9.7GB ，大约是模型参数显存的 5.6%。

 b=64 ，输入序列长度 s=512 ，输出序列长度 n=32 ，则KV cache占用显存为 $4blh(s+n) = 164 GB$，大约是模型参数显存的 1 倍。

<img src="/media/image-20231029092759285.png" alt="image-20231029092759285" style="zoom: 50%;" />





## GPU Memory Hierarchy



先比較一下常見的 edge device memory hierarchy.

|            | Compute Core                                | SRAM Size/BW  | 1st Mem Size/BW                  | 2nd Mem Size/BW           |
| ---------- | ------------------------------------------- | ------------- | -------------------------------- | ------------------------- |
| A100       | (FP16) 312 TOPS<br>Tensor                   | 20MB / 19TB/s | (HBM2?) 40GB / 1.5TB/s           | CPU DRAM > 1TB / 12.8GB/s |
| Smartphone | (INT8) 40 TOPS                              | 8MB / ??      | (LP5-8500, 64bit) 12GB /  50GB/s | Flash, 512TB / 1GB/s?     |
| RTX4070TI  | 7680 Shader<br>184 Tensor<br>(FP32) 40 TOPS |               | (G6X, 192bit)  12GB / 504GB/s    | NA                        |

<img src="/media/image-20231021222248937.png" alt="image-20231021222248937" style="zoom:50%;" />

<img src="/media/image-20230620201108355.png" alt="image-20230620201108355" style="zoom:80%;" />

