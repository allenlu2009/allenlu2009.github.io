---
title: Llama on CPU
date: 2023-11-26 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
typora-root-url: ../../allenlu2009.github.io


---





## Source

* Llama C version:  [GitHub - ggerganov/llama.cpp: Port of Facebook's LLaMA model in C/C++](https://github.com/ggerganov/llama.cpp)

* [用CPU在Windows上部署原版llama.cpp - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/652963043)

  



## Takeaway



### 下載 Llama2 7B/13B/70B 模型

首先到 Meta 的 Github 做 git clone.   https://github.com/facebookresearch/llama

要下載模型必須要 request.    接著執行:  ./download.sh  貼上 email 中的 URL link. (注意：超過 24 小時會失效，需要重新申請)



Llama 有四個 models: 7B/13B/33B/65B.  不過 Llama2 取消了 33B 模型 (改成 code llama)，65B 模型改成 70B models.

所以 Llama2 有三個 models: 7B/13B/70B.   但是每個 parameter size 都有兩個models.

* 一個是 Pretrained base model:  沒有 fine-tune, 只會做文字接龍；
* 另一個是 Fine-tuned chat model:  經過 fine-tune, 可以作爲 chat-bot.   Karpathy 在 introduction 做了很好的説明。
  * 另外 Meta 還提供一個 code-llama model (34B),  這應該是原來 Llama 33B 經過 fine-tuned 成爲 34B 的 code model. 





#### 檔案格式

Meta 提供的格式是 .pth.   可以直接在 Meta 網站申請之後下載。

每個 pth 檔案大小約 13-17GB.  llama-7B 只要一個 pth (13GB).  llama-13B 需要兩個 pth (13GBx2=26GB).  llama-70B 有 8 個 pth file (17GBx8=136GB)

```bash
llama
├── CODE_OF_CONDUCT.md
├── CONTRIBUTING.md
├── LICENSE
├── MODEL_CARD.md
├── README.md
├── Responsible-Use-Guide.pdf
├── UPDATES.md
├── USE_POLICY.md
├── download.sh
├── example_chat_completion.py
├── example_text_completion.py
├── llama
│   ├── __init__.py
│   ├── generation.py
│   ├── model.py
│   └── tokenizer.py
├── llama-2-13b
│   ├── consolidated.00.pth to conslidated.01.pth
│   └── params.json
├── llama-2-13b-chat
│   ├── consolidated.00.pth to conslidated.01.pth
│   └── params.json
├── llama-2-70b
│   ├── consolidated.00.pth to conslidated.07.pth
│   └── params.json
├── llama-2-7b
│   ├── consolidated.00.pth
│   └── params.json
├── llama-2-7b-chat
│   ├── consolidated.00.pth
│   └── params.json
├── requirements.txt
├── setup.py
├── tokenizer.model
└── tokenizer_checklist.chk
```







## Llama on x86



### 編譯 Llama

Windows:  make









### Tokenizer

1. character level encoder:  codebook 65
2. BPE (byte-pair encoder)
   * GPT:   codebook 50541?
   * Tiktoken (OpenAI)

基本是 trade-off of the codebook vs. the token length!

[Hii, hello world]:  character tokenizer: 12 tokens;  BPE:  3 tokens



### 如何使用 llama.cpp in command line



```bash
 ./main -m ../../../../llama_model/llama-2-7b.Q2_K.gguf --color -f ./prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.3
```









## Appendix

