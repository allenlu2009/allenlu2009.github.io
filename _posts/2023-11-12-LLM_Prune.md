---
title: LLM Prune
date: 2023-11-12 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
typora-root-url: ../../allenlu2009.github.io


---





## Source

* Sheared Llama  https://arxiv.org/pdf/2310.06694.pdf

* SparseGPT


## Takeaway

* Attention parameters (1/3) and FFN (2/3)
  * FFN 一般可以 prune
  * Attention 的 multi-heads 可以 prune 




<img src="/media/image-20231112095709919.png" alt="image-20231112095709919" style="zoom: 50%;" />



### Attention is what you need, Memory is the Bottleneck

Attention 已經是必備的 core network.   相較於 CNN,  attention 最大的問題是 memory bandwidth.

主要在計算 K, Q 的 correlation, 以及 softmax.  以下是 GPT1/2/3 的參數。

下圖應該畫錯了！ GPT 應該是 decoder only (右邊)。所以對應的方塊圖是沒有 encoder (左邊)，只有 decoder (右邊)。所以打叉的地方相反。BERT 才是 encoder only (左邊)。不過兩者的架構非常類似。不過 decoder only 架構 output 會 shift right 再接回 input, 稱爲 auto-regression.

<img src="/media/image-20230723204336707.png" alt="image-20230723204336707" style="zoom:80%;" />



## Why Long Context

最早的大語言模型 (ChatGPT2/3, Llama) 的 context length 只有 768/1K/2K tokens.   在應用爲什麽大語言模型需要 long context (8K or longer)?   簡單說有兩點 

1. 處理長文本輸入。例如一篇長的文章做 summary.
2. 對話的記憶長度。例如長的對話有上下文的 context.  Question and Answering

因此實務上，long context (4K/8K or longer) 對於應用非常有用。 





## How to Make Long Context



### Naïve Way

最簡單的方法就是設定長的 input token length,  就是以下的 $n_{ctx}$ 例如從 1K/2K 改成 4K/8K/16K/32K.  幾個問題：

1. 整體的 **parameter number** 並沒有隨著 $n_{ctx}$ 而增加！只有在 查字典和 position encoder 增加一些 parameter 。 -> good things 如果我們知道如何 fine-tune 原來的 model 就可以從 1K/2K to 4K-32K!!!!!  不過要修改 position encoder!!!
2. 但是 internal matrix computation 隨著 $n_{ctx}$ 呈現綫性增加。
3. cache size (of activations) 隨著 $n_{ctx}$ 呈現綫性增加。



**另外的問題是需要從新訓練 LLM 使用更長的 context.**   例如從目前 Llama2-7B 只有 2K context, 如果要更長 context 就需要從新用更長的 text training.  Big effort to train from scratch!!!



### Ideal Goal

是否有方法

1. 只要 fine-tune 原來的 1K/2K 的 LLM model parameter 就可以改成 4K-32K, 不過要修改 position encoder.
2. 最好内部的計算和 activation 的 cache size 不需要增加？ too good to be true!!!!





#### Address Goal 1 (Fine-tune instead of pre-train)



##### 1. Fine-tune use 32K 



##### 2. RoPE (Rotation PE) + flash attention :  simpler than fine-tune

https://www.youtube.com/watch?v=UPYf3jxcMVY&ab_channel=1littlecoder  

[LLaMA2上下文长度暴涨至100万tokens，只需调整1个超参数 (baidu.com)](https://mbd.baidu.com/newspage/data/landingsuper?rs=3210073527&ruk=xed99He2cfyczAP3Jws7PQ&urlext={"cuid"%3A"_a2K8_uSBijAu-uOYiSKtguqHaY1i2tq_8Hsugi6v8KX0qqSB"}&isBdboxFrom=1&pageType=1&sid_for_share&context={"nid"%3A"news_10156585640535514928","sourceFrom"%3A"bjh"})



目前的Transformer位置编码方法，有绝对位置编码（将位置信息融入到输入）、相对位置编码（将位置信息写入attention分数计算）和旋转位置编码几种。其中，最火热的要属旋转位置编码，也就是**RoPE**了。

RoPE通过绝对位置编码的形式，实现了相对位置编码的效果，但与相对位置编码相比，又能更好地提升大模型的外推潜力。

如何进一步激发采用RoPE位置编码的大模型的外推能力，也成为了最近不少研究的新方向。

这些研究，又主要分为**限制注意力**和**调整旋转角**两大流派。

**限制注意力**的代表研究包括ALiBi、xPos、BCA等。最近MIT提出的StreamingLLM，可以让大模型实现无限的输入长度（但并不增加上下文窗口长度），就属于这一方向的研究类型。

**调整旋转角**的工作则更多，典型代表如线性内插、Giraffe、Code LLaMA、LLaMA2 Long等都属于这一类型的研究。

以Meta最近爆火的LLaMA2 Long研究为例，它就提出了一个名叫RoPE ABF的方法，通过修改一个超参数，成功将大模型的上下文长度延长到**3.2万tokens**。

这个超参数，正是Code LLaMA和LLaMA2 Long等研究找出的**“开关”**——

**旋转角底数**（base）。

只需要微调它，就可以确保提升大模型的外推表现。

但无论是Code LLaMA还是LLaMA2 Long，都只是在特定的base和续训长度上进行微调，使得其外推能力增强。

是否能找到一种规律，确保**所有**用了RoPE位置编码的大模型，都能稳定提升外推表现？

来自复旦大学和上海AI研究院的研究人员，针对这一问题进行了实验。

他们先是分析了影响RoPE外推能力的几种参数，提出了一种名叫**临界维度**（Critical Dimension）的概念，随后基于这一概念，总结出了一套**RoPE外推的缩放法则**（Scaling Laws of RoPE-based Extrapolation）。

只需要应用这个**规律**，就能确保任意基于RoPE位置编码大模型都能改善外推能力。

先来看看临界维度是什么。

对此论文认为，旋转角底数更小，能让更多的维度感知到位置信息，旋转角底数更大，则能表示出更长的位置信息。

基于这一规律，可以根据不同预训练和续训文本长度，来直接计算出大模型的外推表现，换言之就是预测大模型的支持的上下文长度。

反之利用这一法则，也能快速推导出如何最好地调整旋转角底数，从而提升大模型外推表现。

作者针对这一系列任务进行了测试，发现实验上目前输入10万、50万甚至100万tokens长度，都可以保证，无需额外注意力限制即可实现外推。

与此同时，包括Code LLaMA和LLaMA2 Long在内的大模型外推能力增强工作都证明了这一规律是确实合理有效的。

这样一来，只需要根据这个规律“调个参”，就能轻松扩展基于RoPE的大模型上下文窗口长度、增强外推能力了。





#### Address Goal 2 (Reduce computation and activation)



##### 1. Cache size optimization

就是使用 KV cache + flash decoder?  to break the 32K into 2K + 2K .... chunks?



##### 2. MGQ (Multiple Group Query)



##### <img src="/media/flash_dcoder.webp" alt="flash_dcoder" style="zoom:67%;" />

應該是減少 heads, 或是多個 heads 共享同一個 weights?



#### Flash Decoder

[FlashAttenion-V3: Flash Decoding详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/661478232)







## Appendix

