---
title: Speculative Decode
date: 2023-12-04 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
description: LLM Output Token Rate
typora-root-url: ../../allenlu2009.github.io


---





## Source

* Speculative Decoding with Big Little Decoder!! 
  * https://arxiv.org/abs/2302.07863
  
* https://zhuanlan.zhihu.com/p/684217993:  good çŸ¥ä¹ paper

* [LLMæ¨ç†åŠ é€Ÿæ–°èŒƒå¼ï¼æ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰æœ€æ–°ç»¼è¿°-CSDNåšå®¢](https://blog.csdn.net/Kaiyuan_sjtu/article/details/136084290)

* [2302.01318 (arxiv.org)](https://arxiv.org/pdf/2302.01318#page=10)   nice explanation of the speculative sampling math!

* [Speculative Sampling â€” Intuitively and Exhaustively Explained (substack.com)](https://iaee.substack.com/p/speculative-sampling-intuitively-and-exhaustively-explained-2daca347dbb9)    with code and example!!

* https://www.jinghong-chen.net/an-mathematical-intuition-of-speculative-sampling/ intuition of the accept with resampling!

* Cloud-edge hybrid SpD! [[2302.07863\] Speculative Decoding with Big Little Decoder (arxiv.org)](https://arxiv.org/abs/2302.07863)

  

## å‰è¨€

Speculative Decoding å¸Œæœ›è§£æ±ºçš„æ˜¯ç¾æœ‰çš„ Autoregressive æ¨¡å‹æ¨ç†éæ…¢çš„å•é¡Œã€‚å…¶æ€è·¯å¾ˆç°¡å–®ï¼šåœ¨ä¸€æ¬¡å‰å‘å‚³æ’­ä¸­ï¼ŒåŒæ™‚é©—è­‰å¤šå€‹ draft tokenã€‚**æ‰€ä»¥æ­¤æŠ€è¡“çš„æ ¸å¿ƒä¾¿åœ¨æ–¼å¦‚ä½•å„˜å¯èƒ½åˆå¿«åˆæº–åœ°ç”Ÿæˆ draft tokenï¼Œä»¥åŠå¦‚ä½•æ›´é«˜æ•ˆåœ°é©—è­‰ (verification)ã€‚**

ç›®å‰çš„å¤§èªè¨€æ¨¡å‹ (LLM) éƒ½æ˜¯åŸºæ–¼ auto-regression

<img src="/media/image-20240525213927416.png" alt="image-20240525213927416" style="zoom:80%;" />

**é‡é»æ˜¯ sampling of conditional probability.**   ä¸€èˆ¬æ˜¯ç”¨ **greedy algorithm (æº«åº¦ T = 0)**,  å°±æ˜¯å–æ©Ÿç‡æœ€é«˜çš„ $x_{n+1} \sim \arg \max p(x \mid x_1, ..., x_n)$â€‹, å†é‡è¤‡åŒæ¨£æ­¥é©Ÿã€‚

å¦‚æœ output è¦æœ‰å¤šä¸€é»è®ŠåŒ–æˆ–å‰µæ„ï¼Œå¯ä»¥æŠŠ T ä¸Šèª¿åˆ° 0.8 æˆ– 1.  æ­¤æ™‚æœƒæ ¹æ“šæ©Ÿç‡åˆ†ä½ˆéš¨æ©Ÿå–æ¨£ã€‚ 

**é€™æ˜¯ä¸€å€‹ sequential process.   è¦åŠ é€Ÿï¼Œå°±æ˜¯è¦æ‰“ç ´ sequence dependency,  ä¸¦åˆ©ç”¨ parallel verificationã€‚ç”¨æ•¸å­¸è¡¨é”ï¼š**

1. **ç”Ÿæˆä¸€å€‹ draft token çš„æ™‚é–“, $t_1$, é å°æ–¼ç”Ÿæˆ (ä¹Ÿå°±æ˜¯é©—è­‰) ä¸€å€‹çœŸæ­£ token çš„æ™‚é–“, $T_1$,  i.e. $t_1 \ll T_1$**

2. **åŒæ™‚é©—è­‰ n å€‹ tokens çš„æ™‚é–“æ¥è¿‘é©—è­‰ä¸€å€‹ token çš„æ™‚é–“, i.e. $T_n \approx T_1$**



**Speculative Decoding ä¸­æœ€é‡è¦çš„æŠ€è¡“æœ‰å…©å€‹ï¼š**

â€‹	**A. Speculative Sampling** 

â€‹	**B. Tree Attention**

å› çˆ²é€™æ˜¯å¾ŒçºŒæ‰€æœ‰æ–‡ç« éƒ½æœƒä½¿ç”¨çš„å…©å€‹æŠ€è¡“ã€‚



### Speculative Sampling

å…ˆå¾©ç¿’ä¹‹å‰çš„ Math_Sampling æ–‡ç« ï¼Œå¦‚ä¸‹ã€‚ $q(x)$ æ˜¯ target distribution,  $p(x)$ æ˜¯ draft, proxy, proposal distribution.

<img src="/media/image-20240610214958320.png" alt="image-20240610214958320" style="zoom: 67%;" />

å°æ¯”ä¸€ä¸‹ speculative sampling çš„å¯«æ³•æ˜¯å¦ä¸€è‡´ï¼ŸYES!

<img src="/media/image-20240612133835587.png" alt="image-20240612133835587" style="zoom:80%;" />

æ•¸å­¸è­‰æ˜è¦‹ Appendixã€‚åŸºæœ¬åŸç†å°±æ˜¯å¤šé€€å°‘è£œã€‚å‰æ–‡ ï¼ˆMath Samplingï¼‰æœ‰æ¯”è¼ƒè©³ç´°çš„èªªæ˜ï¼



### Speculative Decode vs. Speculative Sampling

Speculative sampling æ˜¯ç”¨ä¸€å€‹ draft distribution é€¼è¿‘ target distribution çš„å–æ¨£æ–¹æ³•ã€‚

Speculative decode ä¸€èˆ¬æ˜¯æŒ‡ (1) å¦‚ä½•æ‰¾å‡º draft distribution ä¸”åˆå¿«åˆæº–åœ°ç”Ÿæˆ draft tokens;  (2) å¦‚ä½•é«˜æ•ˆé©—è­‰ (verify) draft tokens.

Draft ç”¢ç”Ÿçš„æ–¹å¼åŒ…å«: (1) ç¨ç«‹åˆ†é›¢ drafting, åŒ…å«å¤§å°æ¨¡å‹ã€‚ (2) è‡ªæˆ‘ draftingï¼ŒåŒ…å« Medusa, SPEED, Lookahead.

Verification (é©—è­‰) çš„æ–¹å¼åŒ…å«: (1) Greedy decoding (T=0 å– maximum probability) ä»¥åŠèª¿æ•´ T çš„ samplingï¼›(2)  Nucleus sampling æˆ–æ˜¯ Top-k sampling å°±æ˜¯å–å‰ k å¤§çš„æ©Ÿç‡ sampling;  (3) Token tree verification,  é€™æ˜¯ç”¨æ–¼ tree attention çš„æ–¹æ³•ã€‚ 



<img src="/media/image-20240612201618218.png" alt="image-20240612201618218" style="zoom:80%;" />



**é€™æ˜¯ä¸€å€‹ sequential process.   è¦åŠ é€Ÿï¼Œå°±æ˜¯è¦æ‰“ç ´ sequence dependency,  ä¸¦åˆ©ç”¨ parallel verificationã€‚ç”¨æ•¸å­¸è¡¨é”ï¼š**

1. **ç”Ÿæˆä¸€å€‹ draft token çš„æ™‚é–“, $t_1$, é å°æ–¼ç”Ÿæˆ (ä¹Ÿå°±æ˜¯é©—è­‰) ä¸€å€‹çœŸæ­£ token çš„æ™‚é–“, $T_1$,  i.e. $t_1 \ll T_1$**

2. **åŒæ™‚é©—è­‰ n å€‹ tokens çš„æ™‚é–“æ¥è¿‘é©—è­‰ä¸€å€‹ token çš„æ™‚é–“, i.e. $T_n \approx T_1$**



**Speculative Decoding ä¸­æœ€é‡è¦çš„æŠ€è¡“æœ‰å…©å€‹ï¼š**

â€‹	**A. Speculative Sampling** 

â€‹	**B. Tree Attention**



SpD æœ‰å¹¾ç¨®æ–¹æ³•

* (5.1/6.2) Draft model (å°æ¨¡å‹) + Target æ¨¡å‹:  æŠ€è¡“æ˜¯ A.  åˆ©ç”¨å‡è¨­æ˜¯ 1 and 2.

* (5.2/6.3) Target æ¨¡å‹ + å¤šé ­ Medusa:  æŠ€è¡“æ˜¯ A and B.  åˆ©ç”¨å‡è¨­æ˜¯ 1 and **2**.  äº‚æ§æ‰“é³¥

* (5.2/6.1) Target æ¨¡å‹ + Lookahead:  æŠ€è¡“æ˜¯ A and B.  åˆ©ç”¨å‡è¨­æ˜¯ 2.  äº‚æ§æ‰“é³¥

* Target æ¨¡å‹ + early exit:  æŠ€è¡“æ˜¯ A.  åˆ©ç”¨å‡è¨­æ˜¯ 2.  äº‚æ§æ‰“é³¥




### åŠ é€Ÿæ¯”è¼ƒ

|          | Speculative Decode                      | Medusa            | Lookahead Decode   |
| -------- | --------------------------------------- | ----------------- | ------------------ |
| Model    | Small Draft + Large Native models       | Large + Multihead | Large + Lookahread |
| Overhead | 10%                                     | 7%                | ?%                 |
| Speed up | Depends on small model<br>å¤§ model æœ€å¥½ | 2x                | å° model æœ€å¥½ï¼Ÿ    |
| Input    | Sequential from draft model             |                   | Jacobe?            |
| Output   | Parallel verify                         |                   | Parallel verify    |



## Speculative Decoding èˆ‡ Speculative Sampling

**Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation**

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2203.16487)] [[paper reading](https://zhuanlan.zhihu.com/p/684204483)]

ç¬¬ä¸€ç¯‡æå‡º Speculative Decoding é€™å€‹è©çš„æ–‡ç« ï¼Œç¢ºç«‹äº†ä½¿ç”¨ *draft-then-verify* é€™ä¸€æ–¹æ³•åŠ é€Ÿ Auto-Regressive ç”Ÿæˆçš„ç¯„å¼ã€‚

Speculative Decoding å¸Œæœ›è§£æ±ºçš„æ˜¯ç¾æœ‰çš„ Autoregressive æ¨¡å‹æ¨ç†éæ…¢çš„å•é¡Œã€‚å…¶æ€è·¯å¾ˆç°¡å–®ï¼šåœ¨ä¸€æ¬¡å‰å‘å‚³æ’­ä¸­ï¼ŒåŒæ™‚é©—è­‰å¤šå€‹ draft tokenã€‚åœ¨ç¬¬ä¸€å€‹ draft token èˆ‡åŸå§‹æ¨¡å‹è¼¸å‡ºä¸ç›¸ç¬¦çš„ä½ç½®æˆªæ–·ï¼Œä¸¦ä¸Ÿæ£„åœ¨æ­¤ä¹‹å¾Œçš„æ‰€æœ‰ draft tokenã€‚

ä½œè€…åœ¨ draft é€™ä¸€æ­¥ä½¿ç”¨çš„æ˜¯ä¸€å€‹å¦è¡Œè¨“ç·´çš„æ¨¡å‹ã€‚é€™è£ä½œè€…ç¢ºç«‹äº† Drafter çš„å…©å€‹åŸå‰‡ï¼šCapability Principleï¼ˆå„˜å¯èƒ½æº–ï¼‰å’Œ Latency Principleï¼ˆå„˜å¯èƒ½å¿«ï¼‰ã€‚ä½œè€…é€™è£æ¡ç”¨çš„è¾¦æ³•æ˜¯å¢åŠ  Encoder å±¤æ•¸ï¼Œæ¸›å°‘ Decoder å±¤æ•¸ï¼Œå¾è€Œåœ¨ä¸å¤ªå½±éŸ¿æ€§èƒ½çš„åŒæ™‚é™ä½éŸ¿æ‡‰æ™‚é–“ã€‚ï¼ˆä½œè€…ä¸»è¦é—œå¿ƒçš„æ˜¯æ©Ÿå™¨ç¿»è­¯é€™å€‹ä»»å‹™ç”¨çš„æ˜¯ encoder + decoderï¼Œè€Œä¸æ˜¯ decoder onlyï¼Œè€Œä¸”æ²’ç”¨å¾ˆå¤§çš„èƒ½ç¨±å¾—ä¸Š LLM çš„æ¨¡å‹ï¼‰

**åœ¨ verify éšæ®µï¼Œä½œè€…æ”¾å¯¬äº†å¿…é ˆå’ŒåŸæ¨¡å‹è¼¸å‡ºå®Œå…¨ä¸€æ¨£çš„é™åˆ¶**ï¼Œåªè¦åœ¨ top-ğ›½ candidates ä¹‹å…§ä¸”å’Œ top-1 çš„ä¼¼ç„¶çš„ gap ä¸è¶…éé–¾å€¼ ğœ ä¾¿æ¥å—ã€‚é€™è£ä¸€æ–¹é¢æ˜¯çˆ²äº†æ¥å—ç‡å’ŒåŠ é€Ÿæ¯”è€ƒæ…®ï¼Œå¦ä¸€æ–¹é¢æ˜¯æœ¬æ–‡è¨­å®šä¸‹çš„ Drafter ä¸¦ä¸ä¸€å®šæ¯”åŸæ¨¡å‹å·®ï¼Œæ‰€ä»¥é©ç•¶æ¥å— Drafter çš„ç•°è¦‹ä¸¦ä¸æœƒçŠ§ç‰²æ€§èƒ½ã€‚

å¯¦é©—éƒ½æ˜¯åœ¨æ©Ÿå™¨ç¿»è­¯çš„æ•¸æ“šé›†ä¸Šåšçš„ã€‚ä½œè€…ä½¿ç”¨çš„åŸæ¨¡å‹æ˜¯ 6 å±¤ Encoder åŠ  6 å±¤ Decoder çš„ Transformerï¼ŒDrafter æ˜¯ 12 å±¤ Encoder åŠ  2 å±¤ Decoderã€‚çµæœæ˜¯é”åˆ°äº† 5 å€çš„åŠ é€Ÿæ¯”ã€‚

é€™ç¯‡æ–‡ç« çš„å—é—œæ³¨ç¨‹åº¦æ˜¯å’Œå¾ŒçºŒå¹¾ç¯‡æ–‡ç« ä¸æˆæ­£æ¯”çš„ã€‚æˆ‘å€‹äººæƒ³åˆ°äº†ä»¥ä¸‹å¹¾é»åŸå› ï¼š

1. å¾æ™‚é–“ä¸Šä¾†çœ‹ï¼ŒGoogle çš„é‚£ç¯‡å’Œå¾ŒçºŒ DeepMind çš„é‚£ç¯‡æˆ‘èªçˆ²æ˜¯æœ‰æ•…æ„ä¸å¼•ç”¨çš„å«Œç–‘çš„ã€‚
2. æœ¬æ–‡è§£æ±ºçš„å•é¡Œä¾·é™æ€§å¤ªå¼·äº†ï¼Œåªåšäº†æ©Ÿå™¨ç¿»è­¯çš„ç›¸é—œå¯¦é©—ã€‚Google é‚£ç¯‡åšäº†å››å€‹ç”Ÿæˆé¡ä»»å‹™ï¼šæ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬ç¸½çµã€1m1bæ–‡æœ¬ç”Ÿæˆã€å°è©±ã€‚
3. æœ¬æ–‡æ²’æœ‰ä½¿ç”¨ç‰¹åˆ¥å¤§çš„æ¨¡å‹ï¼Œå¯èƒ½æœƒä½¿å¾—é—œæ³¨åº¦æ²’æœ‰é‚£éº¼é«˜ã€‚Google é‚£ç¯‡æœ€å¤§ç”¨åˆ°äº† 137B çš„ LaMDAã€‚
4. Google é‚£ç¯‡çš„ Figure 1 å¾ˆæŠ“äººçœ¼çƒï¼Œä¸€å¼µåœ–å°±æŠŠæ•´å€‹ idea è¬›æ˜ç™½äº†ã€‚é€™å€‹ç¢ºå¯¦æ˜¯å€‹å¾ˆå¤§çš„å„ªå‹¢ã€‚
5. å¾æŠ€è¡“è§’åº¦ï¼ŒGoogle çš„æ–¹æ³•ä¿è­‰äº†æœ€å¾Œçš„ç”Ÿæˆçµæœå’ŒåŸæ¨¡å‹çš„è¼¸å‡ºå®Œå…¨ä¸€è‡´ï¼Œä¸¦æä¾›äº†ç†è«–è­‰æ˜ã€‚å°æ–¼ LLM çš„åŠ é€Ÿä¾†èªªï¼Œé€™ç¢ºå¯¦æ¯”æœ¬æ–‡çš„ verify åœ¨ç”Ÿæˆè³ªé‡ä¸Šæ›´æœ‰ä¿éšœã€‚

ä½†ä¸ç®¡æ€éº¼èªªï¼ŒGoogle é‚£ç¯‡å¼•ç”¨é‡æ˜¯æœ¬æ–‡çš„åå€ä»ç„¶æ˜¯ä¸€ä»¶å¾ˆå¥‡æ€ªçš„äº‹æƒ…ã€‚å¦‚æœ Google é‚£ç¯‡æ­£ç¢ºå¼•ç”¨äº†æœ¬æ–‡ï¼Œé‚£æˆ–è¨±å°±æœƒä¸å¤ªä¸€æ¨£äº†ã€‚

**(Google) Fast Inference from Transformers via Speculative Decoding**

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2211.17192)] [[paper reading](https://zhuanlan.zhihu.com/p/684630970)]

é€™ç¯‡è¢«å¾ˆå¤šäººéŒ¯èªçˆ²æ˜¯ Speculative Decoding çš„é–‹å‘ä¹‹ä½œã€‚

ä¸è¨è«–å­¸è¡“é“å¾·çš„å•é¡Œï¼Œä½†å°±è«–æ–‡æœ¬èº«ï¼Œé€™ç¯‡æ–‡ç« ç¢ºå¯¦å¯«å¾—ç›¸ç•¶æ£’ã€‚

é¦–å…ˆå°±æ˜¯é€™å€‹ Figure 1ï¼Œéå¸¸ç°¡æ½”ç›´è§€ã€‚æœ¬æ–‡ç”¨ target modelï¼ˆç›®æ¨™æ¨¡å‹ï¼‰æŒ‡ä»£å¾…åŠ é€Ÿçš„å¤§æ¨¡å‹ï¼Œç”¨ approximation modelï¼ˆè¿‘ä¼¼æ¨¡å‹ï¼‰æŒ‡ä»£ç”¨ä¾†å¹«åŠ©åŠ é€Ÿå¤§æ¨¡å‹çš„å°æ¨¡å‹ã€‚

![img](https://pic1.zhimg.com/80/v2-8e643ed74247813e38b79f9b864914bc_720w.webp)

ç¶ è‰²tokenï¼šè¿‘ä¼¼æ¨¡å‹æå‡ºä¸”ç›®æ¨™æ¨¡å‹æ¥å—çš„å»ºè­°ï¼›ç´…è‰²tokenï¼šè¿‘ä¼¼æ¨¡å‹æå‡ºä½†ç›®æ¨™æ¨¡å‹æ‹’çµ•çš„å»ºè­°ï¼›è—è‰²tokenï¼šç›®æ¨™æ¨¡å‹å°æ–¼ç´…è‰²tokençš„è¨‚æ­£

**ä¹‹å¾Œæ˜¯æœ¬æ–‡æœ€é‡è¦çš„æŠ€è¡“ï¼šSpeculative Samplingã€‚é€™å€‹æ–¹æ³•çš„å¯ä»¥å…¼å®¹ä¸åŒçš„æ¡æ¨£ç­–ç•¥ã€‚**å…·é«”æ­¥é©Ÿå¦‚ä¸‹ï¼š

1. é¦–å…ˆä»‹ç´¹ä¸€ä¸‹ notationï¼šæˆ‘å€‘æœ‰å‰ç¶´ $x_{<t}$ï¼Œæˆ‘å€‘å¸Œæœ›ç”Ÿæˆ $x_t$ï¼Œç›®æ¨™æ¨¡å‹è¼¸å‡ºåˆ†ä½ˆçˆ² $q(x_t \vert x_{<t})$ï¼Œç°¡è¨˜çˆ² $q(x)$ï¼Œdraft æ¨¡å‹çš„è¼¸å‡ºåˆ†ä½ˆçˆ²  $p(x_t \vert x_{<t})$ï¼Œç°¡è¨˜çˆ² $p(x)$ã€‚**å†æ¬¡å¼·èª¿ $p(x), q(x)$ éƒ½æ˜¯æ¢ä»¶æ©Ÿç‡ã€‚**
2. å– $x \sim p(x)$ï¼Œå¦‚æœ $p(x) \le q(x)$ å‰‡ä¿ç•™ $x$ï¼Œå¦‚æœ $p(x) > q(x)$ å‰‡ä»¥ $1 - \frac{q(x)}{p(x)}$ çš„æ©Ÿç‡ä¸Ÿæ£„ $x$ã€‚
3. å°æ–¼ä¸Ÿæ£„çš„ $x$ï¼Œæˆ‘å€‘ä»¥ $q'(x) = norm(\max(0, q(x)-p(x)))$â€‹ çš„æ©Ÿç‡é‡æ–°æ¡æ¨£ã€‚

<img src="/media/image-20240525220052804.png" alt="image-20240525220052804" style="zoom:67%;" />

å¯ä»¥è­‰æ˜ç¶“éé€™æ¨£çš„æ¡æ¨£æ­¥é©Ÿï¼Œ$x \sim q(x)$ï¼Œä»¥ä¸‹æ˜¯ç°¡ç•¥çš„è­‰æ˜æ€è·¯ï¼š

å°æ–¼çµ¦å®šçš„è¼¸å‡º $x'$ï¼Œå­˜åœ¨å…©ç¨®å¯èƒ½ï¼š draft æ¨¡å‹çš„è¼¸å‡ºè¢«æ¥å—äº†ï¼Œé€™å€‹æ©Ÿç‡çˆ² $p(x') \min\left(1, \frac{q(x')}{p(x')}\right)$ï¼› draft æ¨¡å‹çš„è¼¸å‡ºè¢«æ‹’çµ•äº†ï¼Œé‡æ–°æ¡æ¨£å¾—åˆ°äº† $x'$ï¼Œé€™å€‹æ©Ÿç‡çˆ² $ \left(1 - \Sigma_x p(x) \min\left(1, \frac{q(x)}{p(x)}\right)\right) q'(x')$ã€‚å¯ä»¥è¨ˆç®—å¾—äºŒè€…ç›¸åŠ çˆ² $q(x')$ã€‚

ç‰¹åˆ¥æŒ‡å‡ºï¼Œå¦‚æœç›®æ¨™æ¨¡å‹ç”¨çš„æ˜¯ argmax ä¹‹é¡çš„ sampling æ–¹æ³•ï¼Œé‚£éº¼å¯ä»¥æŠŠ $q(x)$ è¦–çˆ² one-hot åˆ†ä½ˆï¼Œé‚£éº¼é€™å°±å’Œæœ€æ¨¸ç´ çš„ Speculative Sampling ä¸€è‡´äº†ã€‚é€™å…¶å¯¦å°±æ˜¯ Greedy decoding.

ä¹‹å¾Œä½œè€…ç”¨è¨ˆç®—æ©Ÿå¤šç´šæµæ°´çš„é¡ä¼¼æ€æƒ³è­‰æ˜äº†ä¸€äº›å’Œè¿‘ä¼¼æ¨¡å‹è¼¸å‡ºæ¥å—ç‡ç›¸é—œçš„çµè«–ï¼Œä¹Ÿè¨è«–äº†è¿‘ä¼¼æ¨¡å‹æ‡‰ç•¶è¼¸å‡ºå¤šé•·çš„åºåˆ—ã€‚

å¯¦é©—éƒ¨åˆ†ï¼Œä½œè€…ç”¨ 11B çš„ T5-XXL çˆ²ç›®æ¨™æ¨¡å‹ï¼Œåšäº†è‹±-å¾·ç¿»è­¯å’Œæ–‡æœ¬ç¸½çµå…©å€‹ä»»å‹™ï¼›ç”¨ 97M çš„ GPT-Like æ¨¡å‹çˆ²ç›®æ¨™æ¨¡å‹ï¼Œåšäº† 1m1b çš„æ–‡æœ¬ç”Ÿæˆä»»å‹™ï¼›ç”¨ 137B çš„ LaMDA çˆ²ç›®æ¨™æ¨¡å‹ï¼Œåšäº†å°è©±ä»»å‹™ã€‚



### Accelerating Large Language Model Decoding with Speculative Sampling

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.01318)]

é€™ç¯‡å’Œ Fast Inference from Transformers via Speculative Decoding çš„è²¢ç»ä¸€æ¨¡ä¸€æ¨£ï¼Œæå‡ºçš„ Speculative Sampling åœ¨ç´°ç¯€ä¸Šä¹Ÿæ˜¯ä¸€æ¨£çš„ã€‚å¯¦é©—ä¸Šæœƒæœ‰å€åˆ¥ï¼Œç”¨çš„æ˜¯ 70B çš„ Chinchilla åšç›®æ¨™æ¨¡å‹ï¼Œ4B çš„æ¨¡å‹åš Draft Modelï¼Œä¸»è¦åšäº†æ–‡æœ¬ç¸½çµå’Œä»£ç¢¼ç”Ÿæˆçš„ä»»å‹™ã€‚

## Tree Verification

### SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.09781)] [[paper reading](https://zhuanlan.zhihu.com/p/684923217)]

æœ¬æ–‡æŠŠå°çš„æ¨¡å‹å« SSMï¼ˆSmall Speculative Modelï¼‰ï¼Œå¤§çš„æ¨¡å‹å« LLMã€‚

æœ¬æ–‡æœ‰å…©å€‹è²¢ç»é»ï¼š1. ä½¿ç”¨äº†å¤šå€‹ SSMï¼Œä¸¦ä½¿ç”¨äº†é¡ä¼¼é›†æˆå­¸ç¿’çš„æ–¹æ³•ä½¿å¤šå€‹ SSM çš„è¼¸å‡ºå„˜å¯èƒ½è¦†è“‹ LLM çš„è¼¸å‡ºï¼›2. ä½¿ç”¨äº†ä¸€ç¨®åŸºæ–¼æ¨¹çš„ Speculative Inferenceï¼Œä½¿å¾—åœ¨ä¸€æ¬¡ inference ä¸­å¯ä»¥å®Œæˆå°å¤šå€‹çŒœæ¸¬çš„è¼¸å‡º sequence çš„é©—è­‰

![img](https://pic1.zhimg.com/80/v2-70dc139c553d986d78611398003bf254_720w.webp)

SSM çš„è¨“ç·´æ˜¯ç”¨äº†é¡ä¼¼æ–¼ boost-tuning çš„æ–¹æ³•ï¼šæ¯æ¬¡åªè¨“ç·´ä¸€å€‹ SSMï¼Œç•¶ SSM è¨“ç·´å®Œæˆå¾Œï¼Œå°‡è¨“ç·´é›†ä¸­é€™å€‹SSM çš„è¼¸å‡ºèˆ‡ LLM è¼¸å‡ºä¸€è‡´çš„é‚£äº›è¨“ç·´æ•¸æ“šåˆªå»ï¼Œä¸¦ç”¨å‰©ä¸‹çš„è¨“ç·´é›†ç¹¼çºŒè¨“ç·´ä¸‹ä¸€å€‹ SSMã€‚é€™æ¨£ï¼Œå¤šå€‹ SSM çš„è¼¸å‡ºå¯ä»¥å„˜å¯èƒ½åœ°è¦†è“‹åˆ° LLM å¯èƒ½çš„è¼¸å‡ºã€‚

åœ¨ Speculative Inference éšæ®µï¼Œä½œè€…å…ˆçˆ²æ¯å€‹ SSM ç”Ÿæˆäº†ä¸€æ£µè¼¸å‡ºæ¨¹ï¼Œå³åœ¨æ¯å€‹ token å–è‹¥å¹²ç¨®å¯èƒ½æ€§æ§‹æˆä¸€æ£µæ¨¹ï¼Œä¹‹å¾Œå°‡é€™äº›æ¨¹åˆä½µæˆä¸€æ£µæ›´å¤§çš„æ¨¹ã€‚

![img](https://pic2.zhimg.com/80/v2-f284b773b68ab197f04c21a53b913141_720w.webp)

ä¹‹å¾Œä¾¿æ˜¯å°‡ç”Ÿæˆçš„æ¨¹é€²è¡Œé©—è­‰ã€‚é€™è£ä½œè€…é€šéæ”¹è®Š mask çŸ©é™£ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥ä¸€æ¬¡é©—è­‰å¤šå€‹ sequenceã€‚å¦‚ä¸‹åœ–æ‰€ç¤ºï¼Œå°æ–¼é€™æ¨£ä¸€æ£µæ¨¹ï¼Œå¦‚æœæ¡ç”¨å¸¸è¦çš„ mask æ–¹å¼ï¼Œt6 æ˜¯å¯ä»¥çœ‹åˆ° t5 çš„ï¼Œä½†åœ¨åœ–ç¤ºçš„ mask çŸ©é™£ä¸‹ï¼Œæ¯å€‹ token åªå¯ä»¥çœ‹åˆ°è‡ªå·±çš„ prefixï¼Œå¾è€Œä½¿å¾— LLM å¯ä»¥ä¸€æ¬¡å®Œæˆå°æ–¼å¤šå€‹ sequence çš„ä¸äº’ç›¸å¹²æ“¾çš„é©—è­‰ã€‚

![img](https://pic1.zhimg.com/80/v2-9a7a519c0ca62a0eed2097f536523404_720w.webp)

ä¹‹å¾Œä½œè€…åƒè€ƒè°·æ­Œé‚£ç¯‡çš„ Speculative Samplingï¼Œæå‡ºäº† Multi-Step Speculative Samplingã€‚ä½œè€…è­‰æ˜äº† Multi-Step Speculative Sampling çš„æ¡æ¨£èˆ‡ç›´æ¥å¾ LLM æ¡æ¨£ç­‰åƒ¹ï¼Œä¸” Multi-Step Speculative Sampling çš„æ¡æ¨£é€šéç‡æ›´é«˜ï¼ˆå‰è€…åœ¨å‰æ–‡ä¹Ÿæœ‰é¡ä¼¼è­‰æ˜ï¼Œä½†å¾Œè€…ä¼¼ä¹æ˜¯æœ¬æ–‡æœ€å…ˆè­‰æ˜çš„ï¼‰ã€‚

![img](https://pic2.zhimg.com/80/v2-c17bd5596bcdd14ec76dd0efca332565_720w.webp)

ç¸½é«”æµç¨‹å¤§è‡´å¦‚ä¸‹åœ–æ‰€ç¤ºï¼š

![img](https://pic2.zhimg.com/80/v2-180a9c8edf628a89741fdf6160917ba5_720w.webp)

å¯¦é©—éƒ¨åˆ†ï¼Œæœ¬æ–‡ä¸»è¦é©—è­‰çš„æ˜¯å°è©±ä»»å‹™ï¼Œä½¿ç”¨äº† LLaMA-7B, OPT-13B, OPT-30B å’Œ LLaMA-65B ä½œçˆ² LLMï¼Œ LLaMA-68M å’Œ OPT-125M ä½œçˆ² SSMã€‚

### Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.12374)] [[paper reading](https://zhuanlan.zhihu.com/p/692323616)]

æœ¬æ–‡çš„æœ€å¤§çš„å‰µæ–°é»æ˜¯ Tree Verification æ™‚æ¨¹çš„æ§‹å»ºã€‚æœ¬æ–‡çš„æœ‰è¶£ä¹‹è™•åœ¨æ–¼ï¼Œå…ˆå®šå¥½æ¨¹çš„çµæ§‹ï¼Œç„¶å¾Œå¾€è£é¢å¡« draft tokenã€‚

æ¨¹çš„å…·é«”æ§‹å»ºæ–¹æ³•åŸºæ–¼ positional acceptance assumptionï¼šæ¥å—æŸå€‹å·²æ¥å— token çš„é æ¸¬æ©Ÿç‡ç¬¬ ğ‘˜ å¤§çš„å¾Œç¹¼ token çš„æ©Ÿç‡åªå–æ±ºæ–¼ ğ‘˜ ï¼Œè¨­çˆ² ğ‘ğ‘˜ ã€‚æ¯å€‹å­ç¯€é»çš„å¾—åˆ†çˆ²å¾æ ¹ç¯€é»åˆ°æ­¤ç¯€é»çš„æ‰€æœ‰ ğ‘ğ‘˜ ç›¸ä¹˜ã€‚æœ€å¾Œçš„ç›®æ¨™æ˜¯åœ¨çµ¦å®šç¯€é»æ•¸é‡çš„æƒ…æ³ä¸‹ä½¿æ•´æ£µæ¨¹æ‰€æœ‰ç¯€é»å¾—åˆ†ç›¸åŠ æœ€å¤§ã€‚

é€™å€‹å•é¡Œçš„è§£å¯ä»¥ç”¨æ›´å°çš„å­å•é¡Œçš„è§£ä¾†è¡¨ç¤ºï¼Œå› æ­¤æ­¤å•é¡Œå¯ä»¥é€šéå‹•æ…‹è¦åŠƒæ±‚è§£ã€‚æ±‚å¾—çš„æ¨¹çµæ§‹æœƒæ»¿è¶³é æ¸¬æ©Ÿç‡è¼ƒå¤§çš„å­ç¯€é»æœƒæœ‰æ›´å¤šçš„å­å­«ã€‚æ‰€ä»¥æœ¬æ–‡ä½¿ç”¨çš„æ¨¹çµæ§‹å¤§è‡´å¦‚ä¸‹åœ–ï¼ˆå¾æœ¬æ–‡çš„åšå®¢è£æ‰¾åˆ°çš„åœ–ï¼‰ï¼š

![img](https://pic1.zhimg.com/80/v2-00a991fe08db030802f20f08cf106890_720w.webp)

ä¹‹å¾Œä¾¿å¾€é€™æ¨£çš„æ¨¹çµæ§‹è£å¡«ç©º draft model çš„è¼¸å‡ºã€‚Sequoia æœƒé€²è¡Œç„¡æ”¾å›çš„æ¡æ¨£ã€‚åœ¨å¡«å……åŒä¸€å­æ¨¹çš„åŒå±¤å­ç¯€é»æ™‚ï¼Œæœƒå°‡å·²ç¶“æ¡æ¨£éçš„ç¯€é»æ©Ÿç‡æ­¸é›¶ã€‚

ä½œè€…å° Sequoiaé€²è¡Œä¸€äº›ç†è«–åˆ†æã€‚ä½œè€…å®šç¾©äº†å…©å€‹å±¬æ€§ï¼šoptimal transport å±¬æ€§å’Œ cover å±¬æ€§ï¼š

1. æ‰€è¬‚ optimal transportï¼Œæ˜¯æŒ‡æ ¹æ“š SpecTr é€™ç¯‡æ–‡ç« æŒ‡å‡ºï¼Œé æ¸¬ token åªæœ‰ä¸€å€‹çš„æ™‚å€™ï¼Œæ¥å—ç‡åœ¨æœ€å„ªå‚³è¼¸çš„æƒ…æ³ä¸‹çˆ² 1âˆ’â€–ğ‘ƒâˆ’ğ‘„â€–12 ï¼ˆ ğ‘ƒ å’Œ ğ‘„ åˆ†åˆ¥çˆ²å…©å€‹æ¨¡å‹çš„è¼¸å‡ºæ©Ÿç‡ï¼‰ã€‚
2. cover å±¬æ€§æŒ‡çš„æ˜¯ draft model çš„è¼¸å‡ºä¸çˆ²é›¶çš„ token å¯ä»¥è¦†è“‹ target model çš„æ‰€æœ‰è¼¸å‡ºå¯èƒ½æ€§ã€‚

ä½œè€…æŒ‡å‡ºï¼Œä¸€èˆ¬çš„ Tree Verification æ–¹æ³•åªæ»¿è¶³ optimal transport å±¬æ€§ï¼Œè€Œæ¨¸ç´ çš„ top-k sampling åªæ»¿è¶³ cover å±¬æ€§ã€‚è€Œ Sequoia åŒæ™‚æ»¿è¶³å…©å€‹å±¬æ€§ï¼Œå› è€Œå¯ä»¥å¦‚ä¸‹åœ–æ‰€ç¤ºåœ¨ä¸åŒçš„æº«åº¦ä¸‹éƒ½è¡¨ç¾è‰¯å¥½ã€‚

- ç•¶æº«åº¦è¼ƒå°çš„æ™‚å€™ï¼Œè¼¸å‡ºè¼ƒçˆ² sharpï¼Œæ­¤æ™‚ target model çš„è¼¸å‡ºå€™é¸é›†ä¸€èˆ¬æ˜¯æœƒå°æ–¼ draft model çš„ï¼ŒSequoia æ¡ç”¨ç„¡æ”¾å›æ¡æ¨£å°±æœƒå°è‡´ç¸½æœƒé¸åˆ° target model çš„è¼¸å‡ºï¼Œè€Œä¹‹å‰çš„æœ‰æ”¾å›æ¡æ¨£æ–¹æ³•å°±å­˜åœ¨ä¸€ç›´é¸ä¸åˆ° target model çš„è¼¸å‡ºçš„å¯èƒ½æ€§ã€‚
- è€Œæº«åº¦è¼ƒå¤§çš„æ™‚å€™ï¼Œè¼¸å‡ºè¼ƒçˆ²å¹³æ»‘ï¼Œæ­¤æ™‚ top-k sampling è¡¨ç¾æœƒåš´é‡ä¸‹æ»‘ï¼Œè€Œæ¡ç”¨äº† Speculative Sampling çš„ Tree Verification æ–¹æ³•ï¼ˆåŒ…æ‹¬ Sequoiaï¼‰æœƒè¡¨ç¾è‰¯å¥½ã€‚

![img](https://pic1.zhimg.com/80/v2-9a6b408472cd8e66efd8a906cfc864c8_720w.webp)

Sequoia é‚„æœƒçˆ²ä¸åŒçš„ç¡¬ä»¶é¸å–ä¸åŒçš„æ¨¹ç¯€é»æ•¸å’Œæ·±åº¦é™åˆ¶ï¼Œå¾è€Œä½¿å¾—æœ¬ç®—æ³•å¯ä»¥å¾ˆå¥½åœ°é©æ‡‰ä¸åŒçš„ç¡¬ä»¶ã€‚

é—œæ–¼å¯¦é©—éƒ¨åˆ†ï¼Œå€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨ä¸€èˆ¬çš„è¨­ç½®ä¹‹å¤–ï¼Œæœ¬æ–‡é‚„è·‘äº† offload inference çš„è¨­ç½®ã€‚ç•¶æ²’æœ‰äº†é¡¯å­˜å’Œå¸¶å¯¬çš„é™åˆ¶ï¼ŒTree Verification é¡¯ç¤ºå‡ºäº†ææ€–çš„åŠ é€Ÿæ¯”ï¼šSpecInfer å¯ä»¥è·‘åˆ° 5x å·¦å³çš„åŠ é€Ÿæ¯”ï¼Œæœ¬æ–‡å¯ä»¥è·‘åˆ° 8x å·¦å³çš„åŠ é€Ÿæ¯”ï¼Œæœ€é«˜å¯ä»¥åˆ° 9.96x çš„åŠ é€Ÿæ¯”ã€‚

## åŸæ¨¡å‹+æ–°é æ¸¬é ­ ä½œçˆ² Draft Model

### Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2401.10774)] [[paper reading](https://zhuanlan.zhihu.com/p/684964189)]

Medusa å’Œ SpecInfer ä¸€æ¨£æ¡ç”¨äº† Tree-based Attentionï¼Œä½†é€™è£æ²’æœ‰ä½¿ç”¨å°æ¨¡å‹ä½œçˆ² Draft Modelï¼Œè€Œæ˜¯åœ¨åŸæ¨¡å‹çš„æœ€å¾Œä¸€å±¤åŠ äº†è‹¥å¹²å€‹ Medusa Headï¼Œç¬¬ ğ‘– å€‹ Medusa Head è² è²¬é æ¸¬ç•¶å‰é æ¸¬ token ä¹‹å¾Œçš„ç¬¬ ğ‘– å€‹tokenã€‚æ¯å€‹ head å– top-k çš„é æ¸¬ï¼Œå°‡é€™äº›é æ¸¬çš„ token å–ç¬›å¡çˆ¾ç©ï¼Œå³å¯å¾—åˆ°è‹¥å¹²å€™é¸ sequenceã€‚

![img](https://pic4.zhimg.com/80/v2-2ab5eef0359d683af296ed9fc19fccc7_720w.webp)

é€™äº› sequence æ§‹æˆäº†ä¸€æ£µæ¨¹ã€‚é€šé tree mask çš„æ–¹æ³•ï¼Œåœ¨ä¸‹æ¬¡ inference çš„éç¨‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥ä¸€æ¬¡é©—è­‰å¤šå€‹ sequenceã€‚çˆ²äº†å€åˆ†ä¸åŒçš„ prefixï¼Œæœ¬æ–‡è¨­ç½®äº†ä¸€äº›å†—é¤˜ï¼Œä¾‹å¦‚ Head 2 çš„ä¸‰å€‹é æ¸¬ token å‡å‡ºç¾äº†å…©æ¬¡ï¼Œé€™æ˜¯çˆ²äº†åˆ†åˆ¥å°æ‡‰ It å’Œ I é€™å…©å€‹ä¸åŒçš„ prefixã€‚æ¯å€‹ token åœ¨ tree mask çš„ä½œç”¨ä¸‹åªå¯ä»¥çœ‹è¦‹è‡ªå·±çš„ prefixã€‚

![img](https://pic1.zhimg.com/80/v2-3fcb60c969099c63ea2771abfea67380_720w.webp)

å°æ–¼é€™äº› Medusa Head çš„è¨“ç·´ï¼Œä½œè€…æä¾›äº†å…©ç¨®ç­–ç•¥ã€‚ç¬¬ä¸€ç¨®æ˜¯å‡çµåŸæ¨¡å‹åƒæ•¸åªè¨“ç·´ Medusa Headã€‚å› çˆ²é å¾Œçš„ Head æœƒæ›´åŠ çš„ä¸ç¢ºå®šï¼Œçˆ²äº†å¹³è¡¡å„å€‹ Head ä¸Š loss çš„å¤§å°ï¼Œä½œè€…è¨­ç½®äº†ä¸€å€‹æŒ‡æ•¸è¡°æ¸›çš„æ¬Šé‡ã€‚ç¬¬äºŒç¨®æ˜¯åŸæ¨¡å‹å’Œ Medusa Head ä¸€èµ·è¨“ç·´ã€‚ä½œè€…å°‡åŸæ¨¡å‹çš„ loss èˆ‡è¨“ç·´ Medusa Head çš„ loss ç›¸åŠ ä¹‹å¾Œï¼Œçˆ² Medusa Head è¨­ç½®äº†æ›´å¤§çš„å­¸ç¿’ç‡ï¼Œä½µçˆ²é€™äº› Head åšäº† warm upã€‚

ä¹‹å¾Œä½œè€…é‚„æä¾›äº†ä¸‰å€‹é€²ä¸€æ­¥æå‡æ€§èƒ½çš„å·¥å…·ã€‚

1. **Typical Acceptance** ä¹‹å‰æœ‰å·¥ä½œæŒ‡å‡ºï¼Œåœ¨åŠ å¤§æº«åº¦çš„æ™‚å€™ï¼Œ Speculative Decoding çš„æ•ˆæœæœƒè®Šå·®ã€‚ä½œè€…èªçˆ²ï¼ŒåŠ å¤§å•é¡Œçš„ä½œç”¨å°±æ˜¯å¢åŠ è¼¸å‡ºçš„å¤šæ¨£æ€§ï¼Œå› æ­¤æ­¤æ™‚ä¸å¿…ä¸€å®šè¦èˆ‡åŸæ¨¡å‹çš„è¼¸å‡ºå°é½Šã€‚ä½œè€…å°‡è¶…éä¸€å®šæ©Ÿç‡é–¾å€¼çš„ token åŠå…¶ prefix ä¿ç•™ã€‚ç•¶å‰æ­¥é©Ÿçš„æœ€çµ‚é æ¸¬ç”±æ‰€æœ‰å€™é¸ä¸­æœ€é•·çš„å¯æ¥å— prefix ä¾†ç¢ºå®šã€‚
2. **Self-Distillation** é€™å€‹ä¸å¤šèªªï¼Œå°é½Šäº†åŸæ¨¡å‹çš„é æ¸¬æ©Ÿç‡å’Œ Medusa Head çš„é æ¸¬æ©Ÿç‡ã€‚
3. **Searching for the Optimized Tree Construction** æ¨¹çš„è¦æ¨¡æœƒå¾ˆå¤§ã€‚ä½œè€…çˆ²äº†æ¸›å°æ¨¹çš„å°ºå¯¸ï¼Œç”¨ä¸€å€‹æ ¡æº–æ•¸æ“šé›†è¨˜éŒ„äº†æ¯å€‹ç¯€é»é æ¸¬æ­£ç¢ºçš„æ©Ÿç‡ï¼Œä¹‹å¾Œç”¨è²ªå¿ƒæ³•ä¿ç•™äº†é æ¸¬æœ€æº–ç¢ºçš„ç¯€é»ã€‚

å¯¦é©—éƒ¨åˆ†ï¼Œä½œè€…è·‘äº† Vicuna-7Bã€13Bã€33B å’Œ Zephyr-7Bã€‚

### Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.05109)] [[paper reading](https://zhuanlan.zhihu.com/p/691883733)]

![img](https://pic3.zhimg.com/80/v2-b1245da529664b7651972fa85dfb4946_720w.webp)

åœ¨ Medusa çš„åŸºç¤ä¸Šåšçš„æ”¹é€²ï¼Œå¢åŠ äº† draft head é æ¸¬ä¹‹é–“çš„é—œè¯æ€§ã€‚æœ€åŸºç¤çš„ Hydra å·²ç¶“å¯ä»¥åœ¨ Medusa çš„åŸºç¤ä¸Šæœ‰ 1.1x çš„åŠ é€Ÿã€‚

ä½œè€…å¯èƒ½æ˜¯è¦ºå¾—é€™å€‹è²¢ç»ä¸å¤ªå¤ ï¼Ÿæ–¼æ˜¯åˆæå‡ºäº† Hydra++ï¼ŒåŠ äº†å¹¾å€‹æ–° trickï¼Œæœ€çµ‚èƒ½é”åˆ°ç›¸å°æ–¼ Medusa æœ‰ 1.31x çš„åŠ é€Ÿæ¯”ï¼š

1. çˆ²è¼¸å…¥åºåˆ—åŠ å™ªè²
2. ä½¿ç”¨ base model çš„è¼¸å‡ºé æ¸¬æ©Ÿç‡ä½œçˆ²çŸ¥è­˜è’¸é¤¾çš„æ•™å¸«æ¨¡å‹è¼¸å‡ºä¾†è¨“ç·´ draft head
3. å¢åŠ ä¸€å€‹ç¨ç«‹çš„ decoder layerï¼Œæ¯å€‹ Hydra head é™¤äº†ä¸Šä¸€å€‹ token æœ¬èº«ï¼Œé‚„æ·»åŠ äº†ä¸Šä¸€å€‹ token åœ¨é€™å€‹ decoder layer çš„ representation ä½œçˆ²è¼¸å…¥ï¼ˆåˆç†æ‡·ç–‘é€™å°±æ˜¯å€Ÿé‘‘äº† EAGLEï¼Œé›–ç„¶ä½œè€…åœ¨æ–‡ç« æœ€å¾Œè²æ˜è‡ªå·±å’Œ EAGLE æ˜¯åŒæ™‚çš„æ–‡ç« ï¼‰



## Jacobi Decoding åŠå…¶è¡ç”Ÿæ–¹æ³•

### Accelerating Transformer Inference for Translation via Parallel Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.10427)] [[paper reading](https://zhuanlan.zhihu.com/p/686121542)]

æœ¬æ–‡æå‡ºäº† Jacobi Decodingï¼Œæ˜¯ Lookahead Decoding å’Œ CLLM çš„å‰é©…å·¥ä½œ

æœ¬æ–‡çš„æ€è·¯æ˜¯æŠŠ Autoregressive çš„éç¨‹çœ‹ä½œæ˜¯è¯ç«‹ä»¥ä¸‹æ–¹ç¨‹æ±‚æ–¹ç¨‹çµ„çš„è§£çš„å•é¡Œï¼š

{ğ‘¦1=argâ¡maxğ‘ğœƒ(ğ‘¦1|ğ‘¥)ğ‘¦2=argâ¡maxğ‘ğœƒ(ğ‘¦2|ğ‘¦1,ğ‘¥)â‹®ğ‘¦ğ‘š=argâ¡maxğ‘ğœƒ(ğ‘¦ğ‘š|ğ‘¦1:ğ‘šâˆ’1,ğ‘¥)

é‚£éº¼æ™®é€šçš„ Autoregressive è§£ç¢¼éç¨‹å°±ç›¸ç•¶æ–¼æ¯æ¬¡éƒ½å°‡ä¸Šä¸€å¼è§£å‡ºä¹‹å¾Œå¸¶å…¥ä¸‹ä¸€å¼ã€‚è€Œä½œè€…æƒ³åˆ°äº†ç›´æ¥ä½¿ç”¨è‡ªè¡Œè¿­ä»£çš„æ–¹æ³•å°‹æ‰¾æ–¹ç¨‹çµ„çš„è§£ã€‚å› çˆ²æ˜¯ **Greedy Decoding**ï¼Œæ‰€ä»¥æ¯æ¬¡è¿­ä»£è‡³å°‘èƒ½ç²å¾—ä¸€å€‹ç©©å®šçš„ token ï¼Œå› è€Œè¿­ä»£çš„æ¬¡æ•¸ ğ‘˜â‰¤ğ‘š ã€‚

![img](https://pic1.zhimg.com/80/v2-0f5e1efb3ce2c992e16c0ae572d0c9e4_720w.webp)

æŠ€è¡“ç´°ç¯€ä¸Šï¼Œæ—¢å¯ä»¥æ•´é«”è¿­ä»£ï¼Œä¹Ÿå¯ä»¥åˆ†å¡Šè¿­ä»£ã€‚è€ƒæ…®åˆ°æ©Ÿå™¨ç¿»è­¯çš„ä»»å‹™ç•¶ä¸­æœ‰ <EOS> ç¬¦è™Ÿï¼Œæœ¬æ–‡é‚„æå‡ºäº†æ··åˆå¼è§£ç¢¼æ³•ï¼Œé‡åˆ° <EOS> ä¹‹å¾Œå°±æ¢å¾©åˆ° Autoregressive è§£ç¢¼ã€‚

**æœ¬æ–‡çš„ä¸€å¤§ç¼ºé™·æ˜¯åªé©ç”¨æ–¼ Greedy Decodingã€‚**

### Break the Sequential Dependency of LLM Inference Using Lookahead Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.02057)] [[paper reading](https://zhuanlan.zhihu.com/p/686437857)]

æœ¬æ–‡æ˜¯ Accelerating Transformer Inference for Translation via Parallel Decodingï¼ˆå³ Jacobi Decodingï¼‰ çš„ä¸€ç¨®æ¨å»£ã€‚

æœ¬æ–‡å°‡ Jacobi Decoding è¦–çˆ²æœ¬æ–‡åœ¨ 2-gram æƒ…æ³ä¸‹çš„ç‰¹ä¾‹ã€‚Jacobi Decoding å°‡æ¯æ¬¡è¿­ä»£ä¸Šä¸€æ¬¡çš„è¼¸å‡ºæ•´é«”ä½œçˆ²ä¸‹ä¸€æ¬¡çš„è¼¸å…¥ï¼Œå…¶å¯¦å°±æ˜¯æŠŠæ¯ä¸€å€‹ token ä¸Šçš„è¼¸å…¥è¼¸å‡ºè¦–ä½œä¸€å€‹ 2-gram ä½œçˆ² Draft Modelã€‚ä½œè€…æƒ³åˆ°ï¼Œå¦‚æœå¯ä»¥è¨˜éŒ„ä¸‹æ›´å¤šçš„æ­·å²ä¿¡æ¯ï¼Œå°±å¯ä»¥è£½é€ ä¸€å€‹ N-gram ä½œçˆ² Draft Modelï¼Œé€™æ¨£å°±å¯ä»¥æé«˜ Speculative Decoding çš„æº–ç¢ºç‡ã€‚

æœ¬æ–‡æå‡ºçš„ Lookahead Decoding åœ¨ä¸€æ¬¡å‰å‘å‚³æ’­éç¨‹ä¸­å®Œæˆäº†å…©ä»¶äº‹ï¼šç”Ÿæˆ N-gram æ­·å²ä¿¡æ¯ï¼ˆLookahead Branchï¼‰å’Œ é¸å–åˆé©çš„ N-gram æ­·å²ä¿¡æ¯é€²è¡Œ verificationï¼ˆVerification Branchï¼‰ã€‚åœ–ä¸­ï¼Œè— 0 æŒ‡çš„æ˜¯ prompt èˆ‡ä¹‹å‰å·²ç¢ºå®šè¼¸å‡ºçš„æœ€å¾Œä¸€ä½ã€‚é€™è£å– window size ğ‘Š=5 ï¼ŒN-gram size ğ‘=4 ï¼Œverification æ•¸é‡ ğº=2 ã€‚

![img](https://pic2.zhimg.com/80/v2-d61085c6cc93a36bb82d3867e61eea99_720w.webp)

Lookahead Branch è£ï¼ŒåŒç¨®é¡è‰²çš„è¡¨ç¤ºæ˜¯åŒä¸€æ¬¡å‰å‘å‚³æ’­è£ä¸€èµ·ç”Ÿæˆçš„ tokenã€‚åœ¨ä¸Šåœ–ä¸­ï¼Œç¶  1~5 æ˜¯ æ©™ 0~4ï¼ˆåœ–ä¸­æ²’æœ‰ æ©™ 0 æ˜¯å› çˆ² æ©™ 0 è¢«æ¨¡å‹çš„æ­£ç¢ºè¼¸å‡º è— 0 å–ä»£ï¼‰åœ¨ä¸Šä¸Šæ¬¡å‰å‘å‚³æ’­çš„è¼¸å‡ºï¼Œç´… 2~6 æ˜¯ ç¶  1~5 åœ¨ä¸Šæ¬¡å‰å‘å‚³æ’­çš„è¼¸å‡ºã€‚å‡è¨­æœ¬æ¬¡å¾—åˆ°çš„è¼¸å‡ºæ˜¯ ç° 3~7ï¼Œé‚£éº¼å°‡ [è— 0ï¼Œç¶  1ï¼Œç´… 2ï¼Œç° 3]ï¼Œ[æ©™ 1ï¼Œç¶  2ï¼Œç´… 3ï¼Œç° 4]ï¼Œ[æ©™ 2ï¼Œç¶  3ï¼Œç´… 4ï¼Œç° 5]ï¼Œ[æ©™ 3ï¼Œç¶  4ï¼Œç´… 5ï¼Œç° 6]ï¼Œ[æ©™ 4ï¼Œç¶  5ï¼Œç´… 6ï¼Œç° 7] åŠ å…¥åˆ° N-gram Pool ä¸­ã€‚åœ¨ä¸‹ä¸€æ¬¡å‰å‘å‚³æ’­çš„è¼¸å…¥è£ï¼Œåœ¨ç•¶å‰ è— 0 çš„è¼¸å‡ºï¼ˆå§‘ä¸”ç¨±ä¹‹çˆ² è— 1ï¼‰ä¹‹å¾Œï¼ŒLookahead Branch è£å°±æ‡‰è©²æ˜¯ ç¶  2~5ï¼Œç´… 2~6ï¼Œç° 3~7ã€‚

é—œæ–¼ Lookahead Branch è£é€™äº›åºåˆ—çš„åˆå§‹ç”Ÿæˆï¼Œæœ€å¥½ç¿»çœ‹ä¸€ä¸‹æºä»£ç¢¼ã€‚ä»¥ä¸‹åœ–çˆ²ä¾‹ï¼Œä½œè€…å…ˆåœ¨çµ¦å®š prompt ä¹‹å¾Œéš¨æ©Ÿç”Ÿæˆäº† ğ‘Š+ğ»âˆ’3 ï¼ˆ5 + 4 - 3 = 6ï¼‰å€‹ tokenï¼ˆå³ æ©™ 1~6ï¼‰ï¼Œå°‡ prompt å’Œé€™äº› æ©™ 1~6 ä¸€ä½µä½œçˆ²è¼¸å…¥ã€‚åœ¨ç¬¬ä¸€æ¬¡å‰å‘å‚³æ’­å¾Œï¼Œå°‡ æ©™ 1~6 çš„è¼¸å‡ºï¼ˆå³ ç¶  2~7ï¼‰åŠ åœ¨ç¾æœ‰çš„è¼¸å…¥ä¹‹å¾Œï¼Œä¸¦ç”¨ prompt æœ€å¾Œä¸€ä½çš„è¼¸å‡ºï¼ˆè— 1ï¼‰æ›¿ä»£ ç¬¬ä¸€æ¬¡æ·»åŠ çš„è¼¸å…¥çš„ç¬¬ä¸€ä½ï¼ˆæ©™ 1ï¼‰ã€‚æ¥ä¸‹ä¾†é€²è¡Œç¬¬äºŒæ¬¡å‰å‘å‚³æ’­ï¼Œå°‡ ç¶  2~7 çš„è¼¸å‡º ç´… 3~8 ç¹¼çºŒæ·»åŠ åˆ°è¼¸å…¥è£ï¼Œä¸¦ç”¨ è— 2 æ›¿ä»£ æ©™ 2 ä½œçˆ²ç¬¬ä¸‰æ¬¡çš„è¼¸å…¥ã€‚

åœ¨å¾—åˆ°ç¬¬ä¸‰æ¬¡å‰å‘å‚³æ’­çš„è¼¸å‡ºå¾Œï¼Œæˆ‘å€‘ä¾¿å®Œæˆäº† Lookahead Branch çš„æ­å»ºã€‚ä¹‹æ‰€ä»¥é‚„ç•«å‡ºäº†ç¬¬å››æ¬¡å‰å‘å‚³æ’­ï¼Œæ˜¯çˆ²äº†æ–¹ä¾¿è®€è€…è§€å¯Ÿ Lookahead Branch çš„æ­å»ºåˆ°æ­£å¸¸é‹è¡Œä¸­é–“çš„ç´°å¾®å·®åˆ¥ã€‚

![img](https://pic1.zhimg.com/80/v2-5ec74456ec56ade499152fc6df2d9db0_720w.webp)

Lookahead Branch éœ€è¦ ğ‘âˆ’2 æ¬¡å‰å‘å‚³æ’­æ‰èƒ½å®Œå…¨æ­å»ºå¥½ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œ N-gram Pool çˆ²ç©ºï¼Œæ­¤æ™‚æ˜¯æ²’æœ‰ Verification Branch çš„ã€‚

Verification Branch è£æ‰€é¸å–çš„æ¨£æœ¬å¾ˆç°¡å–®ï¼Œæ˜¯ç›´æ¥åœ¨ N-gram Pool è£é¸å–ç¬¬ä¸€ä½æ˜¯ è—è‰² token æœ€å¾Œä¸€ä½çš„ N-gramã€‚é€™å…¶ä¸­é©—è­‰ä¹‹å¾Œè¢«æ¥å—çš„å³å¯ä½œçˆ²æœ¬æ¬¡çš„è¼¸å‡ºï¼Œè‹¥å…¨éƒ¨æ²’æœ‰æ¥å—åœ¨è¼¸å‡º è—è‰² token æœ€å¾Œä¸€ä½çš„è¼¸å‡ºã€‚

æœ¬æ–‡é‚„æåˆ°äº†ä¸€å€‹å°çš„æŠ€è¡“ç´°ç¯€ï¼šåœ¨åˆ†ä½ˆå¼è¨“ç·´çš„éç¨‹ä¸­ï¼Œå¯ä»¥å°‡ Verification Branch æ”¾åœ¨ Lookahead Branch çš„é•·åº¦è¼ƒå°çš„æ©Ÿå™¨ä¸Šï¼Œé”åˆ°è² è¼‰å‡è¡¡çš„æ•ˆæœã€‚

å¯¦é©—éƒ¨åˆ†ï¼Œä½œè€…ä½¿ç”¨äº† 7Bã€13Bã€34B å’Œ 70B çš„ LLaMA-2 å’Œ CodeLLaMAã€‚

### Ouroboros: Speculative Decoding with Large Model Enhanced Drafting

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.13720)] [[paper reading](https://zhuanlan.zhihu.com/p/694571261)]

æœ¬æ–‡å¯ä»¥ç®—æ˜¯ Lookahead Decoding çš„ draft-target åˆ†é›¢ç‰ˆèˆ‡æ€§èƒ½åŠ å¼·ç‰ˆã€‚æœ¬æ–‡çš„æ ¸å¿ƒé‚„æ˜¯ candidate poolã€‚

![img](https://pic1.zhimg.com/80/v2-07688d0a6d6fc07ceec8fe3c0db73450_720w.webp)

çµ¦å®šè¼¸å…¥å‰ç¶´ ABCDï¼Œå‡è¨­ target model æœƒç”Ÿæˆ EFGHI**J**KLMNï¼Œè€Œ draft model è¼¸å‡ºEFGHI**W**KLMNã€‚

1. åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘å€‘é¦–å…ˆä½¿ç”¨æœ€å¾Œä¸€å€‹ tokenï¼ˆåœ¨æœ¬ä¾‹ä¸­çˆ²Dï¼‰åœ¨ candidate pool ä¸­æª¢ç´¢å¯èƒ½ç·ŠæŒ¨ç€ D çš„ä¸€äº›å€™é¸è¼¸å‡ºã€‚
2. ä½¿ç”¨ draft model ä¾†é©—è­‰é€™äº›å€™é¸è¼¸å‡ºï¼Œä¸¦ä¸”åœ¨æ ¡æ­£ä¹‹å¾Œï¼Œç”Ÿæˆåºåˆ—EFGã€‚
3. ä»¥ä¸Šéç¨‹åŸ·è¡Œå¤šæ¬¡ï¼Œå…¶ä¸­ EFG ç”Ÿæˆ HI**W**K ï¼ŒEFGHI**W**K ç”Ÿæˆ LMNã€‚
4. ä¹‹å¾Œï¼ŒåŸºæ–¼ draft model ç”Ÿæˆçš„åºåˆ—è¢«çµ„åˆæˆ draftï¼šEFGHI**W**KLMNï¼Œä¸¦ä¸”ä»¥ candidate pool ä¸­ä»¥æœ€å¾Œä¸€å€‹ token N é–‹å§‹çš„çµ„åˆä½œçˆ²è‰ç¨¿å¾Œç¶´ã€‚
5. target model åŒæ™‚é©—è­‰å®ƒå€‘ã€‚
   1. target model ç™¼ç¾ I çš„ä¸‹ä¸€å€‹æ¨™è¨˜æ‡‰è©²æ˜¯ **J** è€Œä¸æ˜¯ **W**ï¼Œé€™æ¨£ EFGHI**J** å°±å¯ä»¥ç”¨ä½œç”Ÿæˆã€‚
   2. **W** ä¹‹å¾Œçš„é‚£äº› draft tokenï¼Œå³ KLMNï¼Œä¸èƒ½åœ¨ç•¶å‰è¿­ä»£ä¸­ç”¨ä½œç”Ÿæˆï¼Œå› çˆ²å®ƒå€‘åŸºæ–¼éŒ¯èª¤çš„ä¸Šä¸‹æ–‡ **W**ã€‚ç„¶è€Œï¼Œç”±æ–¼å®ƒå€‘èˆ‡ target model çš„è¼¸å‡ºé«˜åº¦åŒ¹é…ï¼Œæˆ‘å€‘å¯ä»¥ç”Ÿæˆé«˜è³ªé‡çš„å€™é¸ KLMN å’Œ LMNOï¼Œé€™å¯ä»¥çµ¦EFGHI**J**ä¹‹å¾Œçš„ candidate pool å¸¶ä¾†å•“ç™¼ã€‚
   3. ä½è³ªé‡å€™é¸å¾Œç¶´ NOXQ å’Œ NRSY ç”±ç›®æ¨™æ¨¡å‹å›ºå®šï¼Œåˆ†åˆ¥æ›´æ”¹çˆ² NOPQ å’Œ NOPTã€‚é€™å…©å€‹å¾Œç¶´éƒ½ç²å¾—äº†è‡³å°‘ä¸€å€‹æ ¡æ­£çš„ tokenï¼Œé€™æœ‰åŠ©æ–¼åœ¨æœªä¾†çš„è¿­ä»£ä¸­åŠ å¿«ç”Ÿæˆé€Ÿåº¦ã€‚

ä»¥ä¸Šå°±æ˜¯ä¸€å€‹å®Œæ•´çš„ Ouroboros åŸ·è¡Œéç¨‹ã€‚

ä½œè€…åœ¨å¾ŒçºŒé‚„å¢åŠ äº†ä¸€å€‹ warm startï¼Œå°±æ˜¯åœ¨å•“å‹•éšæ®µç›´æ¥ç”¨ä¹‹å‰ decoding éç¨‹ä¸­ç”Ÿæˆçš„ candidate poolï¼Œé€™æ¨£å¯ä»¥ç·©è§£ä½¿ç”¨ candidate pool çš„æ–¹æ³•åœ¨èµ·å§‹éšæ®µ candidate pool çˆ²ç©ºçš„çª˜å¢ƒã€‚

å°‡æœ¬æ–‡å’Œ Lookahead Decoding ç›¸æ¯”è¼ƒï¼Œæˆ‘ç¸½çµäº†å¹¾è™•æœ‰è¶£çš„æ”¹é€²ï¼š

- é¦–å…ˆæ˜¯ draft model èˆ‡ target model çš„åˆ†é›¢ã€‚ä½¿ç”¨æ›´å°çš„ draft model ä»¥ç”Ÿæˆæ›´é•·çš„ draft token åºåˆ—ã€‚é€™æ‰“ç ´äº† Lookahead Decoding å–®æ¬¡é©—è­‰æœ€å¤šéš»èƒ½é©—è­‰é•·åº¦çˆ² N-gram ä¸­çš„ N çš„ä¾·é™æ€§ã€‚
- èˆ‡ Lookahead Decoding ç›¸æ¯”ï¼ŒN-gram çš„ç”Ÿæˆç¶“æ¿Ÿå¯¦æƒ çš„å¤šã€‚ Lookahead Decoding çš„ lookahead branch éå¸¸çš„å¥¢ä¾ˆï¼Œæ•´å€‹ branch ä½”æ“šäº†å¤§é‡çš„è¼¸å…¥ç¯‡å¹…ï¼Œå»åªæœ‰æœ€å¾Œå¹¾å€‹è¼¸å‡ºçš„ token æœ‰ç”¨ã€‚åŒæ™‚ï¼Œ Lookahead Decoding çš„ verification branch ä¹Ÿæ²’æœ‰æŠŠè¢«å¦æ‰çš„è¼¸å‡ºå»¢ç‰©åˆ©ç”¨èµ·ä¾†ã€‚æœ¬æ–‡çš„æ–¹æ³•å°±å¾ˆå¥½åœ°è§£æ±ºäº†ä¸Šè¿°å…©å€‹å•é¡Œã€‚
- Candidate Refinement è¨‚æ­£äº†åŸæœ‰ candidate pool ä¸­æŸäº›å€™é¸å¾Œç¶´ï¼Œé€™ä¸€æ–¹é¢æœƒç¸®æ¸› candidate pool çš„å¤§å°ï¼ˆå› çˆ²ä¸ç„¶çš„è©±æ˜¯ç›´æ¥åŠ é€²å»ï¼‰ï¼Œå¦ä¸€æ–¹é¢æœƒå¹«åŠ©å‰”é™¤ candidate pool ä¸­æŸäº›èˆ‡çœ¼ä¸‹çš„ç”Ÿæˆé—œè¯åº¦ä¸å¤§çš„å€™é¸å¾Œç¶´ã€‚
- warm start å…¶å¯¦æœ‰ä¸€é»é»è³´çš®ï¼Œä½†åˆå¾ˆåˆç†ã€‚æœ‰é»å¥½å¥‡ Lookahead Decoding åŠ ä¸Š warm start æœƒæ˜¯ä»€éº¼çµæœã€‚

### CLLMs: Consistency Large Language Models

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.00835)] [[paper reading](https://zhuanlan.zhihu.com/p/699273579)]

æœ¬æ–‡åŸºæ–¼çš„æ˜¯ Jacobi Decodingï¼Œä¹Ÿå°±æ˜¯ Accelerating Transformer Inference for Translation via Parallel Decoding é€™ç¯‡æ–‡ç« ä¸­æå‡ºçš„æ–¹æ³•ã€‚å¤§è‡´æ€è·¯æ˜¯ LLM è¼¸å‡ºï¼ˆåœ¨ Greedy Decoding ä¸‹ï¼‰æ˜¯ä¸€å€‹ä¸å‹•é»ï¼Œé€šé LLM ä¸æ–·çš„è‡ªæˆ‘è¿­ä»£èƒ½ç”¨æ›´å°‘çš„æ¬¡æ•¸æ‰¾åˆ°é€™å€‹ä¸å‹•é»ã€‚ä½œè€…æŠŠé€™æ¨£çš„ä¸€å€‹è¿­ä»£éç¨‹çœ‹ä½œäº†ä¸€å€‹è»Œè·¡ï¼š

![img](https://pic1.zhimg.com/80/v2-9da9bf84d74f33a4de02557ca37b9718_720w.webp)

æ–¼æ˜¯ä½œè€…å¾ Consistency Model å¾—åˆ°äº†å•“ç™¼ï¼ŒåŠªåŠ›ä½¿ LLM çš„ Jacobi trajectory æ›´çŸ­ã€‚ä½œè€…åœ¨æ­£å¸¸è‡ªè¿´æ­¸æ¨¡å‹çš„è¨“ç·´æå¤±ä¹‹å¤–å¼•å…¥äº† Consistency Lossã€‚Consistency Loss åˆ†çˆ² Global Consistency Loss å’Œ Local Consistency Lossã€‚å…¶ä¸­ï¼ŒGlobal Consistency Loss è©¦åœ–ä½¿åˆå§‹ç‹€æ…‹çš„è¼¸å‡ºèˆ‡æœ€çµ‚å¾—åˆ°çš„ä¸å‹•é»æ¥è¿‘ï¼Œè€Œ Local Consistency Loss è©¦åœ–ä½¿ Jacobi trajectory ä¸Šå…©å€‹ç›¸é„°ç‹€æ…‹çš„è¼¸å‡ºæ›´æ¥è¿‘ã€‚å…·é«”å…¬å¼åƒè¦‹æœ¬æ–‡çš„ [blog](https://link.zhihu.com/?target=https%3A//hao-ai-lab.github.io/blogs/cllm/%23consistency-and-ar-loss) å°æ‡‰ç« ç¯€ã€‚

é€™æ¨£è¨“ç·´å¾—åˆ°çš„æ¨¡å‹æœƒèˆ‡åŸæ¨¡å‹æœ‰æ‰€ä¸åŒï¼Œå› è€Œèˆ‡ç„¡æ³•å…¶å®ƒ Speculative Decoding æ–¹æ³•ç›¸æ¯”ï¼Œåªèƒ½å’ŒæŠŠåŸæ¨¡å‹ä¸€èµ· finetune çš„ Medusa2 ç›¸æ¯”ã€‚

## ç‰¹å¾µå±¤ Speculative Decoding

### EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2401.15077)] [[paper reading](https://zhuanlan.zhihu.com/p/687404563)]

æœ¬æ–‡å‰µæ–°æ€§åœ°å°‡ Speculative Decoding å‰ç§»è‡³äº†ç‰¹å¾µå±¤ï¼ˆå³å€’æ•¸ç¬¬äºŒå±¤ï¼‰ã€‚é€™è£ä½œè€…æå‡ºäº†å…©å€‹å‹•æ©Ÿï¼š

1. ç‰¹å¾µå±¤çš„è¼¸å‡ºç›¸è¼ƒæ–¼ token å±¤ï¼Œæ›´æœ‰è¦å¾‹æ€§ã€‚ï¼ˆé€™é»æˆ‘æ„Ÿè¦ºæ²’èªªæ˜ç™½ï¼Œä½œè€…çµ¦äº†é€™å€‹çµè«–ä¹‹å¾Œå°±èªªæ‰€ä»¥é€™æ¨£æ•ˆæœæœƒæ›´å¥½äº‘äº‘ï¼‰
2. ä¿ç•™ç‰¹å¾µå±¤å¯ä»¥æ›´å¥½çš„å…‹æœæ¡æ¨£éç¨‹ä¸­çš„ä¸ç¢ºå®šæ€§ã€‚å¦‚ä¸‹åœ–ï¼Œåœ¨è¼¸å‡º I ä¹‹å¾Œï¼ŒæœƒæŒ‰æ©Ÿç‡æ¡æ¨£è¼¸å‡º am æˆ–æ˜¯ alwaysã€‚åœ¨é€²ä¸€æ­¥å°‹æ‰¾ always çš„å¾ŒçºŒè¼¸å‡ºæ™‚ï¼Œå¦‚æœèƒ½ä¿ç•™ I çš„ç‰¹å¾µå±¤è¼¸å‡ºï¼Œå°±èƒ½ä¿ç•™ä½æ¡æ¨£éç¨‹ä¸­ä¸Ÿæ‰çš„é—œæ–¼ am çš„ä¿¡æ¯ã€‚ï¼ˆé€™ä¸€é»æˆ‘è¦ºå¾—æ˜¯æ¯”ä¸Šä¸€é»æ›´åŠ  make sense çš„ï¼‰

![img](https://pic1.zhimg.com/80/v2-193c1cc6a49b54bab6cb308fc5e32d40_720w.webp)

ä¸‹åœ–å°æ¯”äº† EAGLE å’Œå…¶å®ƒå¹¾ç¨® Speculative Decoding æ–¹æ³•ã€‚é€™è£ï¼Œ ğ‘¡ğ‘– è¡¨ç¤ºç¬¬ ğ‘– æ¬¡è¼¸å…¥çš„ tokenï¼Œ ğ‘“ğ‘– è¡¨ç¤º ğ‘¡ğ‘– ç¶“é LLM å¾Œåœ¨å€’æ•¸ç¬¬äºŒå±¤çš„è¼¸å‡ºï¼ˆå³ LM Head ä¹‹å‰çš„è¼¸å‡ºï¼‰ã€‚å¯ä»¥çœ‹åˆ°ï¼ŒEAGLE å‰µæ–°æ€§åœ°é¸æ“‡å° ğ‘“ åš Autoregressive Decodingã€‚

![img](https://pic4.zhimg.com/80/v2-3a1b5b3942a80c3b265f6c3a08dfb603_720w.webp)

æŠ€è¡“ç´°ç¯€å¤§è‡´å¦‚ä¸‹åœ–æ‰€ç¤ºã€‚ä½œè€…å°‡ embedding å’Œ ç‰¹å¾µ ğ‘“ æ‹¼æ¥åœ¨ä¸€èµ·ä½œçˆ² Draft Model çš„è¼¸å…¥ã€‚é€™æ¨£å¯ä»¥ä¿ç•™æœ€çµ‚è¼¸å‡º token ä¸­éºå¤±çš„å…¶å®ƒä¿¡æ¯ã€‚

é€™è£éœ€è¦è¨“ç·´çš„éƒ¨åˆ†æ˜¯è‡ªè¿´æ­¸é ­ï¼Œç”±ä¸€å€‹å…¨é€£æ¥å±¤å’Œä¸€å€‹ Decoder å±¤çµ„æˆã€‚å…¨é€£æ¥å±¤çš„ä½œç”¨æ˜¯å°‡æ‹¼æ¥å¾Œçš„å‘é‡é™ç¶­è‡³ç‰¹å¾µç¶­åº¦ã€‚ä¹‹å¾Œç”± Decoder å±¤è² è²¬é æ¸¬ä¸‹ä¸€å€‹ç‰¹å¾µã€‚é€™è£ä½œè€…åŒæ¨£æ¡ç”¨äº† Tree Attention ä»¥é”åˆ°ä¸€æ¬¡é©—è­‰å¤šå€‹ sequence çš„ç›®çš„ã€‚

è£œå……ä¸€é»ï¼Œé€™è£ç¬¬ä¸€æ¬¡å‰å‘å‚³æ’­ç„¡æ³•åŠ é€Ÿï¼Œå› çˆ²éœ€è¦é€šéä¸€æ¬¡å‰å‘å‚³æ’­æ‰èƒ½å¾—åˆ°å¾ŒçºŒ EAGLE æ‰€éœ€è¦çš„ç‰¹å¾µã€‚é€™è£ä¹Ÿå°±èƒ½çœ‹å‡ºä¸Šä¸€å¼µåœ–è£ä½œè€…ç•«è‡ªå·±çš„ EAGLE çš„æ™‚å€™çˆ²ä½•è¦å¾ ğ‘¡2 ç•«èµ·ã€‚

![img](https://pic2.zhimg.com/80/v2-cd8e80c2db1d2391bb0af68675de2749_720w.webp)

ä¹‹å¾Œä¾¿æ˜¯é€™å€‹è‡ªè¿´æ­¸é ­çš„è¨“ç·´ã€‚ä½œè€…ç”¨äº†å…©å€‹ lossã€‚ä¸€å€‹æ˜¯ç‰¹å¾µ ğ‘“ æœ¬èº«çš„é‡å»º lossï¼Œå¦ä¸€å€‹æ˜¯è‡ªè¿´æ­¸é ­è¼¸å‡ºçš„ç‰¹å¾µèˆ‡åŸæ¨¡å‹ç‰¹å¾µåœ¨ç¶“é LM Head ä¹‹å¾Œçš„åˆ†é¡ lossã€‚

ç†è«–ä¸Šä¾†èªªè‡ªè¿´æ­¸é ­éœ€è¦ä½¿ç”¨åŸæ¨¡å‹è‡ªè¿´æ­¸ç”Ÿæˆçš„æ•¸æ“šè¨“ç·´ã€‚ä¸éä½œè€…é€šéæ¶ˆèå¯¦é©—è­‰æ˜äº† EAGLE å°æ–¼è¨“ç·´æ•¸æ“šä¸æ•æ„Ÿã€‚å› è€Œä½œè€…ä½¿ç”¨äº†å›ºå®šçš„æ•¸æ“šé›† ShareGPTï¼Œå¾è€Œé™ä½äº†è¨“ç·´è² æ“”ã€‚ï¼ˆé€™è£ä½œè€…æ˜¯ç›´æ¥æŠŠ ShareGPT è¼¸å…¥äº†ï¼‰

åŒæ™‚ï¼Œä½œè€…æŒ‡å‡ºï¼ŒEAGLE åœ¨è¨“ç·´éç¨‹ä¸­è‡ªè¿´æ­¸åœ°ç”Ÿæˆç‰¹å¾µï¼Œé€™æœŸé–“ç‰¹å¾µçš„ä¸æº–ç¢ºæœƒå°è‡´éŒ¯èª¤ç´¯ç©ã€‚å› æ­¤ä½œè€…åœ¨è¨“ç·´æ•¸æ“šä¸ŠåŠ äº†ä¸€å€‹å‡å‹»åˆ†ä½ˆçš„éš¨æ©Ÿå™ªè²ä½œçˆ²æ•¸æ“šå¢å¼·ã€‚

æ•ˆæœæ¥µå¼·ï¼ŒSOTAã€‚

ä½œè€…é‚„æäº†å€‹å¾ˆæœ‰æ„æ€çš„é»ï¼Œå°±æ˜¯ MoE æ¨¡å‹å¤©ç”Ÿæœƒå’Œ Speculative Decoding å…«å­—ä¸åˆã€‚å› çˆ²åœ¨ Vanilla Inference éšæ®µï¼Œæ¯å€‹ token åªæœƒéœ€è¦å…©å€‹ experts çš„æ¬Šé‡ã€‚ä½† Speculative Decoding çš„ verification éšæ®µéœ€è¦åŒæ™‚é©—è­‰å¤šå€‹ tokenï¼Œé€™å°±æœƒå‰Šå¼± MoE çš„å„ªå‹¢ï¼Œå¾è€Œå°è‡´åŠ é€Ÿæ¯”çš„ä¸‹é™ã€‚åœ¨ Mixtral 8x7B æµè¡Œçš„èƒŒæ™¯ä¸‹ï¼Œé€™ç¢ºå¯¦æˆäº†ä¸€å€‹äºŸéœ€è§£æ±ºçš„æœ‰è¶£å•é¡Œã€‚

## Hierarchical Speculative Decoding

### Cascade Speculative Drafting for Even Faster LLM Inference

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2312.11462)] [[paper reading](https://zhuanlan.zhihu.com/p/685053191)]

æœ¬æ–‡æå‡ºäº† Vertical Cascade å’Œ Horizontal Cascadeã€‚Vertical Cascade ç”¨ Speculative Decoding ä¾†åŠ é€Ÿ Speculative Decodingã€‚Horizontal Cascade æŒ‡çš„æ˜¯åœ¨æ¥å—ç‡è¼ƒé«˜çš„å‰å¹¾å€‹ token ç”¨è¼ƒå¤§çš„ Draft Modelï¼Œåœ¨æ¥å—ç‡è¼ƒå°çš„é å¾Œçš„ token ç”¨è¼ƒå°çš„æ¨¡å‹ä¾†â€œç³Šå¼„â€ï¼ˆé€™å€‹è©æ˜¯æˆ‘è‡ªå·±æƒ³åˆ°çš„ï¼Œæˆ‘çœ‹åˆ°æ–‡ç« çš„ç¬¬ä¸€æ„Ÿè¦ºå°±æ˜¯é€™å€‹è©ï¼Œå°±æ˜¯é‚£ç¨®â€œåæ­£ä¹ŸçŒœä¸æº–ï¼Œéš¨ä¾¿çŒœå¹¾å€‹å¾—äº†â€çš„æ„Ÿè¦ºï¼‰ã€‚

![img](https://pic2.zhimg.com/80/v2-ea17477fef43d3257c250bf202ab9531_720w.webp)

å¯¦é©—éƒ¨åˆ†ç”¨äº† FLAN-T5-XXL (FLAN-T5-small, FLAN-T5-base) å’Œ LLaMA2 7B (LLaMA2 160M)ã€‚

å¯¦é©—éƒ¨åˆ†æœ‰é€™éº¼ä¸€æ®µï¼Œæ„Ÿè¦ºæœ‰äº›å¥‡æ€ªï¼šSince we do not observe any significant difference between sampling with temperature 1 and greedy decoding in previous speculative decoding experiments (Leviathan et al., 2023), and to ensure our experiments are fully reproducible, we perform sampling at temperature0, i.e., using greedy decoding by default.

æˆ‘æ„Ÿè¦ºé€™å¯èƒ½é‚„æŒºé‡è¦çš„ï¼Œé€™éº¼åšå¯¦é©—çš„è©±é‚£ Leviathan çš„é€™ç¯‡æœ€å¤§çš„è²¢ç»é» Speculative Sampling å°±æ²’æœ‰å­˜åœ¨çš„æ„ç¾©äº†ã€‚

### TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2404.11912)] [paper reading]

æœ¬æ–‡æƒ³è§£æ±ºçš„æ˜¯ long-context generation èƒŒæ™¯ä¸‹çš„åŠ é€Ÿå•é¡Œã€‚åœ¨ long-context generation è¨­å®šä¸‹ï¼Œé™¤äº†æ¨¡å‹æœ¬èº«çš„æ¬Šé‡ï¼Œæ¨¡å‹æ¨ç†çš„ KV cache ä¹Ÿæœƒä½”æ“šå¤§é‡çš„é¡¯å­˜ï¼Œä¸¦åœ¨ä¸Šä¸‹æ–‡è¶³å¤ é•·çš„æ™‚å€™åŒæ¨£æˆçˆ²åˆ¶ç´„æ¨ç†é€Ÿåº¦çš„é—œéµå› ç´ ã€‚

å› æ­¤ï¼Œåœ¨ draft model æ¸›å°æ¨¡å‹æ¬Šé‡å¤§å°å°æ–¼æ¨ç†é€Ÿåº¦çš„åˆ¶ç´„ä¹‹å¤–ï¼Œä½œè€…å¼•å…¥äº†ä¸€å€‹åªä½¿ç”¨éƒ¨åˆ† KV cache çš„ target model ä¾†æ¸›å°å…¨é‡çš„ KV cache å°æ–¼æ¨ç†é€Ÿåº¦çš„åˆ¶ç´„ï¼Œå¾è€Œæ§‹æˆäº†ä¸€ç¨®åˆ†å±¤çš„ Speculative Decodingã€‚

![img](https://pic1.zhimg.com/80/v2-01f0ea2b384c2847e36cb48fa0325430_720w.webp)

æœ¬æ–‡é€™å€‹æ–¹æ³•ä¸¦ä¸æ˜¯å…¨æ–°çš„ã€‚äº‹å¯¦ä¸Šï¼Œã€ŠCascade Speculative Drafting for Even Faster LLM Inferenceã€‹é€™ç¯‡åœ¨ä¹‹å‰å°±æå‡ºéé¡ä¼¼çš„æ–¹æ³•ï¼Œä½†å…©ç¯‡æ–‡ç« çµ¦äººçš„è§€æ„Ÿæ˜¯æˆªç„¶ä¸åŒã€‚Cascade é€™ç¯‡ä¸¦æ²’æœ‰ long-context generation é€™å€‹è¨­å®šï¼Œå› æ­¤æ¨¡å‹çš„åˆ†å±¤è¨­è¨ˆå°±ä¸¦ä¸æ˜¯å¾ˆè‡ªç„¶ã€‚ä½†åœ¨æœ¬æ–‡ä¸­ï¼Œå…©æ¬¡ Speculative Decoding è§£æ±ºçš„å•é¡Œæ˜¯ä¸åŒçš„ï¼Œé€™è£çš„åˆ†å±¤è¨­è¨ˆå°±éå¸¸çš„åˆç†ã€‚ï¼ˆæœ¬æ–‡æ²’å¼•ç”¨ Cascadeï¼Œæ„Ÿè¦ºâ€¦â€¦å…¶å¯¦ä¹Ÿå¯ä»¥ä¸å¼•ï¼Œä½†é‚„æ˜¯å¼•ç”¨ä¸€ä¸‹æ¯”è¼ƒå¥½ï¼‰

æ‰€ä»¥é€™è£çš„é‡é»ä¸¦ä¸æ˜¯åŒ…è£çš„è—è¡“ï¼Œè€Œæ˜¯å°‹æ‰¾ä¸€å€‹å¥½å•é¡Œçš„è—è¡“ã€‚ç•¶ä½ æƒ³è³£å‡ºä¸€ç“¶æ´—é«®æ°´åˆæ°å¥½ç¢°è¦‹ä¸€ä½åƒ§äººçš„æ™‚å€™ï¼Œé‡é»ä¸¦ä¸æ˜¯å¦‚ä½•å…œå”®ï¼Œè€Œæ˜¯æœæ–·æ›ä¸€å€‹æ¨éŠ·å°è±¡ã€‚

## Draft Model èˆ‡ Target Model çš„å°é½Š

### DistillSpec: Improving Speculative Decoding via Knowledge Distillation

[[paper](https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DrsY6J3ZaTF)] [[paper reading](https://zhuanlan.zhihu.com/p/679429488)]

éå¸¸ç›´è§€çš„æƒ³æ³•ï¼šçŸ¥è­˜è’¸é¤¾ï¼ˆKDï¼‰ç”¨æ–¼ Speculative Decoding å¯ä»¥æé«˜ acceptance rateï¼Œå¾è€Œæé«˜åŠ é€Ÿæ¯”

æ—¢ç„¶ç”¨åˆ°äº†èªè¨€æ¨¡å‹çš„ KDï¼Œé‚£éº¼æˆ‘å€‘å¿…é ˆè¦å•å…©å€‹å•é¡Œï¼šç”¨ä»€éº¼è’¸é¤¾æ–¹æ³•ï¼Ÿç”¨ä»€éº¼æ•¸æ“šï¼Ÿ

å°æ–¼ç¬¬ä¸€å€‹å•é¡Œï¼Œä½œè€…ç”¨å¯¦é©—èªªæ˜äº†æœ€å„ªçš„è’¸é¤¾æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šå–æ±ºæ–¼ä»»å‹™å’Œ Decoding Strategyã€‚

![img](https://pic3.zhimg.com/80/v2-0ea8f0fce631e3161ee528a4a0782a46_720w.webp)

å°æ–¼ç¬¬äºŒå€‹å•é¡Œï¼Œä½œè€…è­‰æ˜äº†ï¼šå¦‚æœåœ¨ draft model ç”Ÿæˆçš„ ğ‘¦ ä¸Šï¼Œtarget model å’Œ draft model çš„é æ¸¬æ©Ÿç‡åˆ†ä½ˆå·®è·è¶Šå°ï¼Œé‚£éº¼ acceptance rate çš„ä¸‹ç•Œè¶Šé«˜ï¼Œé€™çˆ²ä½¿ç”¨ draft model ç”Ÿæˆçš„ ğ‘¦ é€²è¡Œ KD æä¾›äº†ç†è«–ä¿è­‰ã€‚ä¹‹å¾Œä½œè€…ç”¨å¯¦é©—è­‰æ˜ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ•¸æ“šä¸Šè’¸é¤¾çš„æ•ˆæœè¦å„ªæ–¼å›ºå®šçš„æ•¸æ“šé›†ï¼Œä¸”ä½¿ç”¨ target model ç”Ÿæˆçš„ ğ‘¦ å’Œä½¿ç”¨ draft model ç”Ÿæˆçš„ ğ‘¦ è’¸é¤¾æ•ˆæœå·®ä¸å¤šï¼ˆä¸Šåœ–ç¬¬ä¸€è¡Œæ˜¯å›ºå®šçš„æ•¸æ“šé›†ï¼Œé¡¯è‘—åŠ£æ–¼å¾Œä¸‰è¡Œæ¨¡å‹ç”Ÿæˆçš„æ•¸æ“šï¼‰ã€‚ç”±æ–¼ draft model æ›´å°ï¼Œç”Ÿæˆæ•¸æ“šçš„æˆæœ¬æ›´ä½ï¼Œä½œè€…å»ºè­°é¸ç”¨ draft model ç”Ÿæˆçš„ ğ‘¦ é€²è¡Œè’¸é¤¾ã€‚

![img](https://pic4.zhimg.com/80/v2-610b295436ea33b0b0f421587dd87567_720w.webp)

### Online Speculative Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.07177)] [[paper reading](https://zhuanlan.zhihu.com/p/685037821)]

æœ¬æ–‡æŠŠåœ¨ç·šçŸ¥è­˜è’¸é¤¾èå…¥äº† Speculative Decodingã€‚

åœ¨ Speculative Decoding éšæ®µï¼Œæœƒè¨˜éŒ„ä¸‹æ‰€æœ‰ Draft Model çš„éŒ¯èª¤çŒœæ¸¬ï¼Œä¸¦å°‡å°æ‡‰çš„æ­£ç¢ºçŒœæ¸¬æ”¾å…¥ Replay Bufferã€‚æ¯éš”ä¸€æ®µæ™‚é–“ï¼Œç”¨ Replay Buffer å…§çš„æ•¸æ“šè¨“ç·´ Draft Modelï¼Œä½¿å¾— Draft Model åœ¨ç”¨æˆ¶ç•¶å‰çš„è¼¸å…¥åˆ†ä½ˆä¸Šèˆ‡åŸæ¨¡å‹æ›´å¥½çš„å°é½Šã€‚ä¹‹å¾Œæ¸…ç©º Replay Buffer ä¸¦ç¹¼çºŒæ­£å¸¸çš„ Speculative Decodingã€‚

è’¸é¤¾éç¨‹ä¸­ä¹Ÿç”¨åˆ°äº†ä¸€äº›ä¹‹å‰ç ”ç©¶ LLM è’¸é¤¾çš„æ–‡ç« æåˆ°çš„ tricksï¼Œé€™è£ä¸è´…è¿°äº†ã€‚

å¯¦é©—éƒ¨åˆ†ï¼Œç”¨äº† Vicuna-7Bï¼ˆLLaMA-160Mï¼‰å’Œ Flan-T5-XL 3Bï¼ˆT5-small 80Mï¼‰ã€‚

## å…¶å®ƒçš„ Draft Model å‰µæ–°

### SpecTr: Fast Speculative Decoding via Optimal Transport

å¯«çš„ç°¡çŸ­ä¸€äº›ã€‚ä¸€æ˜¯å› çˆ²é€™å€‹æ–¹æ³•æ˜é¡¯æ¯”ä¸é Tree Verificationï¼Œå·²ç¶“æ²’ä»€éº¼äººç”¨äº†ã€‚äºŒæ˜¯å› çˆ² optimal transport æˆ‘ä¸¦ä¸æ˜¯å¾ˆæ‡‚ï¼Œçœ‹äº†æ–‡ç« ç¾å­¸çš„ï¼Œå­¸äº†ä¹Ÿæ²’å¤ªçœ‹æ‡‚ç†è«–éƒ¨åˆ†åœ¨å¹¹å•¥ã€‚

æœ¬æ–‡æƒ³è§£æ±ºçš„å•é¡Œæ˜¯å¤šå€‹ draft sequence å¦‚ä½•é¸æ“‡çš„å•é¡Œï¼Œå¤§è‡´å¦‚ä¸‹åœ–ï¼ˆæŒ‰ä¸‹åœ–çš„ caption æè¿°å°±æ˜¯æ•¸å€‹æ•¸ï¼‰ï¼š

![img](https://pic2.zhimg.com/80/v2-ecb0b3ba4ae06d02f3f3d36b23ca6849_720w.webp)

æœ¬æ–‡ç”¨äº†å¤§é‡ optimal transport çš„ç†è«–åˆ†æï¼Œè­‰æ˜äº† Speculative Sampling åœ¨åªçŒœæ¸¬ä¸‹ä¸€å€‹ token çš„æ™‚å€™æ˜¯æœ€å„ªçš„ï¼Œä¸¦æŒ‡å‡ºæœ€å„ªçš„ draft é¸æ“‡å¯ä»¥ä½¿ç”¨ç·šæ€§è¦åŠƒå¾—åˆ°ã€‚

### REST: Retrieval-Based Speculative Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2311.08252)] [[paper reading](https://zhuanlan.zhihu.com/p/685234708)]

æœ¬æ–‡æ²’æœ‰ Draft Modelï¼Œè€Œæ˜¯ä½¿ç”¨ç¾æˆçš„æ•¸æ“šåº«æ›¿ä»£ Draft Model çš„è¼¸å‡ºã€‚å…·é«”åˆ†çˆ²ä¸‰æ­¥ï¼š

1. åœ¨æ–‡æª”ä¸­å°‹æ‰¾æœ€é•·åŒ¹é…å¾Œç¶´ã€‚
2. å°‡æª¢ç´¢åˆ°çš„é€™äº›æ¢ç›®æ§‹å»ºçˆ²å­—å…¸æ¨¹ã€‚ç”¨æ¢ç›®å‡ºç¾é »ç‡è¨­ç½®çˆ²å­—å…¸æ¨¹ä¸­çµé»çš„æ¬Šé‡ã€‚
3. ä½¿ç”¨é¡ä¼¼æ–¼ SpecInfer å’Œ Medusa çš„æ–¹æ³•æ§‹å»º Tree-based Attentionï¼Œçˆ¾å¾Œé€²è¡Œ verifyã€‚

![img](https://pic2.zhimg.com/80/v2-75066fb51ed8db60b1ec1a483c36b18d_720w.webp)

å¯¦é©—éƒ¨åˆ†ï¼ŒHumanEval æ•¸æ“šé›†ä¸Šç”¨çš„æ˜¯ CodeLlama 7Bã€13Bï¼Œç”¨ä¾†æª¢ç´¢çš„æ•¸æ“šé›†æ˜¯ TheStackï¼›MT-Bench æ•¸æ“šé›†ä¸Šç”¨çš„æ˜¯ Vicuna 7Bã€13Bï¼Œç”¨ä¾†æª¢ç´¢çš„æ•¸æ“šé›†æ˜¯ UltraChatã€‚

### Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2307.05908)] [[paper reading](https://zhuanlan.zhihu.com/p/685400320)]

æœ¬æ–‡çš„æƒ³æ³•æ˜¯æŠŠ Early-Exiting èˆ‡ Speculative Decoding ç›¸çµåˆï¼Œä½¿ç”¨æ¨¡å‹ä¸­é–“å±¤çš„è¼¸å‡ºä½œçˆ²é æ¸¬ã€‚é€™å€‹çµæ§‹å°±å¾ˆæœ‰è¨ˆç®—æ©Ÿå¤šç´šæµæ°´çš„å‘³é“ã€‚

![img](https://pic1.zhimg.com/80/v2-28de34267a1fcca94a1d32aad798a398_720w.webp)

ä½œè€…ä¸¦æ²’æœ‰å®Œæ•´åœ°å¯¦ç¾æ•´å€‹ç®—æ³•ï¼Œç•¢ç«ŸæŒ‰é€™å€‹çµæ§‹ï¼Œå³ä½¿æ˜¯é¸å– top-3ï¼Œä¹Ÿéœ€è¦å››å€é¡¯å­˜ï¼Œé€™æ˜é¡¯ä¸å¤ªå¯¦ç”¨ã€‚ç•¶ç„¶ï¼Œå¦‚æœæŠŠå¾ŒçºŒ SpecInfer å’Œ Medusa è£çš„ tree-based attention ç”¨é€²ä¾†çš„çš„è©±ï¼Œé‚£å°±æ˜¯æ°¸é æ˜¯å…©å€é¡¯å­˜ã€‚ä½†å³ä½¿æ˜¯é€™æ¨£ä¹Ÿä¸¦ä¸å¾ˆçœé¡¯å­˜ï¼Œè€Œä¸”åŠ é€Ÿæ¯”å¯èƒ½ä¸¦ä¸æœƒå¾ˆçªå‡ºã€‚

### Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2309.08168)] [[paper reading](https://zhuanlan.zhihu.com/p/685958090)]

æœ¬æ–‡å°‡ skip éƒ¨åˆ†å±¤åŸæ¨¡å‹çš„åŸæ¨¡å‹ä½œçˆ² Draft Modelã€‚å¦‚ä¸‹åœ–æ‰€ç¤ºï¼Œåœ¨ Drafting éšæ®µæœƒè·³éä¸€äº›å±¤ï¼Œè€Œåœ¨ Verification éšæ®µæœƒé€šéæ‰€æœ‰å±¤ã€‚é‚£éº¼ä¸‹ä¸€æ­¥è¦è§£æ±ºçš„å•é¡Œå°±æ˜¯ç©¶ç«Ÿéœ€è¦è·³éå“ªäº›å±¤ã€‚

![img](https://pic3.zhimg.com/80/v2-a25d7aed312201d6a101d4760539b7b2_720w.webp)

æœ¬æ–‡é‡‡ç”¨äº†ä¸€ç¨®è²è‘‰æ–¯å„ªåŒ–çš„æ–¹æ³•ï¼Œä½¿ç”¨ Gaussian Process ä¾†å„ªåŒ–å±¤æ©ç¢¼ï¼Œåœ¨å„ªåŒ–å®Œæˆå¾Œå›ºå®šé€™ä¸€æ©ç¢¼ï¼Œä¸¦ç›´æ¥ç”¨åœ¨äº†å¾ŒçºŒçš„ Self-Speculative Decoding ä¸­ã€‚

![img](https://pic3.zhimg.com/80/v2-786fbfbc6d6fb044aab59ac6ffacf026_720w.webp)

ç”±æ–¼åœ¨æœ¬æ–‡çš„è¨­å®šä¸‹æ¨¡å‹ç„¡æ³•åŒæ™‚æ‰®æ¼” Draft Model å’ŒåŸæ¨¡å‹çš„è§’è‰²ï¼Œä½œè€…å°ˆé–€è¨è«–äº†ä½•æ™‚åœæ­¢ Draft è½‰è€Œé€²è¡Œ Verification çš„å•é¡Œã€‚æœ¬æ–‡çš„è§£æ±ºæ–¹æ¡ˆæ˜¯è¨­ç½®äº†ä¸€å€‹æ ¹æ“š Accept Rate å‹•æ…‹è®ŠåŒ–çš„è‡ªé©æ‡‰é–¾å€¼ï¼Œç•¶ä¸‹ä¸€å€‹ token çš„é æ¸¬æ©Ÿç‡å°æ–¼é–¾å€¼çš„æ™‚å€™åœæ­¢ Draftingã€‚

å¯¦é©—éƒ¨åˆ†ç”¨äº† LLaMA-2-13B, LLaMA-2-13B-Chat, CodeLLaMA-13B å’Œ LLaMA-2-70Bã€‚è²¼ä¸€å¼µé—œæ–¼ skip å±¤æ•¸çš„åœ–ã€‚å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ skip ä¸€åŠçš„å±¤çš„æ™‚å€™æœ‰ä¸€å€‹åŠ é€Ÿæ¯”çš„é«˜å³¯ï¼Œskip çš„å±¤æ•¸è¶…éä¸€åŠä¹‹å¾Œï¼ŒDrafting çµ¦å‡ºçš„çŒœæ¸¬æº–ç¢ºç‡å°±æœƒå¤§å¹…ä¸‹é™ï¼Œå°è‡´é›–ç„¶ Drafting é€Ÿåº¦è®Šå¿«ä½†æ•´é«”åŠ é€Ÿæ¯”æ€¥åŠ‡ä¸‹é™ï¼Œç”šè‡³ä½æ–¼1ã€‚

![img](https://pic3.zhimg.com/80/v2-f6f89037f261d7c7b919c285564a484a_720w.webp)

### Speculative Decoding with Big Little Decoder

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.07863)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]

æœ¬æ–‡æŠŠæ˜¯å¦éœ€è¦å¤§æ¨¡å‹é€²è¡Œç¢ºèªçš„æ¬ŠåŠ›äº¤çµ¦äº†å°æ¨¡å‹ï¼Œç¨±ä¹‹çˆ² Fallbackã€‚Fallback ä¹‹å¾Œï¼Œå°æ–¼å…©æ¬¡ Fallback ä¹‹é–“å°æ¨¡å‹ç”Ÿæˆçš„ tokenï¼Œå¼•å…¥ Rollback ç¢ºä¿å…¶æ€§èƒ½ã€‚

![img](https://pic2.zhimg.com/80/v2-02517bea1c30a5d511bc677e11cec905_720w.webp)

å…·é«”ä¾†èªªï¼Œä¸€æ—¦å°æ¨¡å‹åœ¨ç•¶å‰ token è¼¸å‡ºæ©Ÿç‡çš„æœ€å¤§å€¼ä½æ–¼è¨­å®šçš„é–¾å€¼ ğ›¼ğ¹ğµï¼Œå°±é€²è¡Œ Fallbackï¼Œé–‹å§‹å¼•å…¥å¤§æ¨¡å‹é€²è¡Œ verifyã€‚åœ¨ verify éç¨‹ä¸­ï¼Œè¨ˆç®—æ¯å€‹ token ä¸Šå¤§å°æ¨¡å‹è¼¸å‡ºæ©Ÿç‡ä¹‹é–“çš„è·é›¢ ğ‘‘ï¼Œä¸€æ—¦ ğ‘‘ å¤§æ–¼è¨­å®šçš„é–¾å€¼ ğ›¼ğ‘…ğµï¼Œå°±å°‡æ­¤ token æ”¹çˆ²å¤§æ¨¡å‹çš„è¼¸å‡ºï¼Œä¸¦è®“å°æ¨¡å‹åœ¨å¾ŒçºŒå¾é€™å€‹ token é–‹å§‹ç”Ÿæˆã€‚

æ­¤æ–¹æ³•ç„¡æ³•ç¢ºä¿è¼¸å‡ºèˆ‡åŸæ¨¡å‹å®Œå…¨ä¸€è‡´ã€‚å¯¦é©—çš„æ¨¡å‹æ¯”è¼ƒå°ï¼Œæœªè¶…é 1Bã€‚

### PaSS: Parallel Speculative Sampling

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2311.13581)] [[paper reading](https://zhuanlan.zhihu.com/p/686654676)]

æœ¬æ–‡åœ¨ç¾æœ‰çš„è¼¸å…¥ä¹‹å¾ŒåŠ ä¸Šäº†ä¸€äº› lookahead tokensï¼Œç„¶å¾Œç”¨é€™äº› tokens çš„è¼¸å‡ºç•¶ä½œçŒœæ¸¬ï¼Œä¸¦ç”¨ Speculative Decoding é€²è¡Œé©—è­‰ã€‚é€™äº› lookahead tokens çš„ embedding æ˜¯å¯å­¸ç¿’çš„ã€‚

![img](https://pic2.zhimg.com/80/v2-f00b1aadfb17c1e968f45f50316c6fa9_720w.webp)

å¯¦é©—ç”¨çš„æ˜¯ 7B çš„ LLaMAã€‚

### Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2402.11809)] [[paper reading](https://zhuanlan.zhihu.com/p/694142504)]

æœ¬æ–‡å…©å¼µåœ–ç•«å¾—é‚„æ˜¯å¾ˆæ˜ç™½çš„ï¼Œå¯ä»¥ç®—æ˜¯ä¸€ç¨® self speculative decodingã€‚ç¬¬ä¸€å¼µåœ–å³å´ç•«çš„æ˜¯å¤§é«”æ¡†æ¶ï¼Œå…·é«”çš„ç”Ÿæˆèˆ‡é©—è­‰çš„ç´°ç¯€åœ¨ç¬¬äºŒå¼µåœ–ä¸Šã€‚

![img](https://pic3.zhimg.com/80/v2-62b00eb60e76b1079d155f77d9f3b972_720w.webp)

å…·é«”ä¾†èªªï¼Œå°±æ˜¯æ‰€æœ‰çš„ ğ‘˜ å€‹ candidate token åœ¨ verification çš„æ™‚å€™ä¸¦ä¸æ˜¯ç·Šé„°ç€çš„ï¼Œå…©å…©ä¸­é–“ç©¿æ’äº† ğ‘˜ å€‹ mask tokenï¼Œverification é€²è¡Œåˆ°ç¬¬ä¸€å€‹è¢« reject çš„ token æ™‚ï¼Œé€™å€‹ token ä¹‹å¾Œæ‰€ç”Ÿæˆçš„ ğ‘˜ å€‹ token ä½œçˆ²ä¸‹ä¸€æ¬¡ verification çš„ candidate tokenã€‚

![img](https://pic2.zhimg.com/80/v2-0c7d347d17b337e13dc3ef80ac0c904d_720w.webp)

æœ¬æ–‡çš„æ–¹æ³•å¾ˆå¥¢ä¾ˆï¼Œé™¤äº†è¢« reject çš„é‚£ä¸€å€‹ token ä¹‹å¾Œçš„ ğ‘˜ å€‹ tokenï¼Œå…¶å®ƒçš„ mask token æ˜¯è¨»å®šæ˜¯æœƒè¢«æµªè²»çš„ï¼Œé€™åœ¨å¸¶å¯¬å—é™çš„éè¨ˆç®—å¡ä¸Šå°¤çˆ²è‡´å‘½ã€‚è€Œ Medusa çš„è™•ç†æ–¹æ³•å’Œæ­¤æ–¹æ³•å¾ˆåƒï¼Œå»æ²’æœ‰é€™å€‹å•é¡Œã€‚åŒæ™‚ï¼ŒMedusa ä½¿ç”¨äº† Tree Verification è€Œæ­¤æ–¹æ³•å¾ˆé›£ä½¿ç”¨ï¼Œå› æ­¤æ­¤æ–¹æ³•åœ¨æ€§èƒ½ä¸Šèƒ½å¦æ¯”é Medusa ä¹Ÿæ˜¯è¦æ‰“å•è™Ÿçš„ã€‚





### LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding

[[paper](https://arxiv.org/pdf/2404.16710)] 

1. Self-drafting
2. parallel verification



<img src="/media/image-20240613222006507.png" alt="image-20240613222006507" style="zoom:50%;" />



### Lookahead Key Technology

$\boldsymbol{x}$ : prompt, $\boldsymbol{y}=\left[y_1, y_2, \ldots, y_m\right]: m$ tokens to decode, $p(\boldsymbol{y} \mid \boldsymbol{x}):$ LLM distribution 

Define: $f\left(y_i, \boldsymbol{y}_{1: i-1}, \boldsymbol{x}\right)=y_i-\operatorname{arg max} p\left(y_i \mid \boldsymbol{y}_{1: i-1}, \boldsymbol{x}\right)$
$$
\left\{\begin{array} { l } 
{ y _ { 1 } = \operatorname { arg max } p ( y _ { 1 } | \boldsymbol { x } ) } \\
{ y _ { 2 } = \operatorname { arg max } p ( y _ { 2 } | y _ { 1 } , \boldsymbol { x } ) } \\
{ \vdots } \\
{ y _ { m } = \operatorname { arg max } p ( y _ { m } | \boldsymbol { y } _ { 1 : m - 1 } , \boldsymbol { x } ) } 
\end{array} 
\quad \equiv \quad
\left\{\begin{array}{l}
f\left(y_1, \boldsymbol{x}\right)=0 \\
f\left(y_2, y_1, \boldsymbol{x}\right)=0 \\
\vdots \\
f\left(y_m, \boldsymbol{y}_{1: m-1}, \boldsymbol{x}\right)=0
\end{array}\right.\right.
\\
\text{Autoregressive decoding}\quad \text{Nonlinear system with m variables and m equations}
$$
$m$ ä»£è¡¨ $m$-gram? No,  $m$ æ˜¯ token number.

An alternative approach based on Jacobi iteration can solve all $[y_1,y_2,...,y_m]$ of this nonlinear system in parallel as follows:

- Start with an initial guess for all variables $y = [y_1,y_2,...,y_m]$.
- Calculate new yâ€² values for each equation with the previous y.
- Update y to the newly calculated yâ€².
- Repeat this process until a certain stopping condition is achieved (e.g., y=yâ€²).



We illustrate this parallel decoding process (also referred to as [*Jacobi decoding*](https://arxiv.org/pdf/2305.10427.pdf)) in Figure 3. Jacobi decoding can guarantee solving all $m$ variables in at most $m$ steps (i.e., the same number of steps as autoregressive decoding) because each step guarantees at least the very first token is correctly decoded. Sometimes, multiple tokens might converge in a single iteration, potentially reducing the overall number of decoding steps.



#### Jacob Decode

Jacob decode åŸç†å¦‚ä¸‹ï¼š

* å·¦åœ–æ˜¯å‚³çµ±çš„ autoregressive decode. éœ€è¦ $m$ å€‹ step æ‰èƒ½å¾—åˆ° $m$ tokens.  
* å³åœ–æ˜¯ Jocob parallel decode.  å¯ä»¥æƒ³åƒ parallel decoding çš„ input æ˜¯ guess tokens, ç¶“é parallel decoding ç”¢ç”Ÿ output tokens.  Output tokens ç¶“é decoder åš parallel verification.   ç¶“é $k$ æ¬¡ iteration å¾—åˆ° $m$ tokens.   **å¦‚æœç®—æ³•å¤ è°æ˜**ï¼Œè®“ $k < m$, åŸºæœ¬å°±è³ºåˆ°ã€‚Speed up = $m/k$.

<img src="/media/image-20231206202336903.png" alt="image-20231206202336903" style="zoom:80%;" />

* Jacob decode å°±æ˜¯é€™å€‹â€è°æ˜çš„ç®—æ³•â€œã€‚
* Autoregressive decoding çš„ç®—æ³•å°±æ˜¯ä¸‹è¡¨å·¦ã€‚

<img src="/media/image-20231204222150619.png" alt="image-20231204222150619" style="zoom:80%;" />

* Jocob ç®—æ³•å°±æ˜¯åˆ©ç”¨ä¸Šè¡¨å³ï¼Œå¾—åˆ°çš„ä¸‹è¡¨ï¼š
  * Stop criterion:  å°±æ˜¯ input m tokens ç­‰æ–¼ output m tokens => fully verified.  å¦‚æœ k æ¬¡é”æˆè€Œä¸” $k < m$ å°±æœ‰ speed up.

<img src="/media/image-20231206203947655.png" alt="image-20231206203947655" style="zoom: 67%;" />



å¦‚ä½•å¾—åˆ° guess tokens?  Jacob decode.  å•é¡Œï¼š**å¦‚ä½•å¾—åˆ° $p_{\theta}()$ conditional probability.**

**Transformer model æœ€å¤§çš„å¥½è™•ï¼ï¼**

* **Distribution probability ç›´æ¥å°±åœ¨ softmax ä¹‹å¾Œï¼ï¼**



Parallel verified: å’Œ speculative decode ä¸€æ¨£

<img src="/media/jacobi-iteration.gif" alt="jacobi-iteration" style="zoom: 50%;" />

#### N-gram

* 2-gram to N-gram å¯ä»¥å¹«å¿™ Jacob decode æ›´æœ‰æ•ˆç‡ï¼Ÿ





#### Lookahead = Jacob + N-gram



<img src="/media/lookahead-decoding.gif" alt="lookahead-decoding" style="zoom: 50%;" />



#### Lookahead Branch + Verification Branch



<img src="/media/image-20231204221232479.png" alt="image-20231204221232479" style="zoom: 67%;" />





### Speed Up

* å° model ä¼¼ä¹æ•ˆæœæœ€å¥½ã€‚é€™å’Œ speculative decode å‰›å¥½ç›¸åï¼Ÿ



![image-20231204221819433](/media/image-20231204221819433.png)



## Appendix

1. å¯¹äº prompt $x_1, \ldots, x_n$ ï¼Œå…ˆç”¨ draft æ¨¡å‹ (å°æ¨¡å‹) å» autoregressive åœ°ç”Ÿæˆ $\tilde{x}_{n+1}, \ldots, \tilde{x}_{n+K}$ ï¼Œé¡ºä¾¿å¾—åˆ° $\mathbf{P}_{\text {draft }}\left(X \mid x_1, \ldots, x_n, \tilde{x}_{n+1}, \ldots, \tilde{x}_{n+i-1}\right) ï¼Œ(1 \leq i \leq K)$ ï¼›
2. ç„¶åæŠŠ $x_1, \ldots, x_n, \tilde{x}_{n+1}, \ldots \tilde{x}_{n+K}$ ä½œä¸º target model (å¤§æ¨¡å‹) çš„è¾“å…¥ï¼Œä¸€æ¬¡æ€§å¾—åˆ° $\mathbf{P}_{\text {target }}\left(X \mid x_1, \ldots, x_n, \tilde{x}_{n+1}, \ldots, \tilde{x}_{n+i-1}\right) ï¼Œ(1 \leq i \leq K)$ ï¼›
3. for $\mathrm{t}$ in range( $\mathrm{k}+1)$
- éšæœºç”Ÿæˆ $r \sim U[0,1]$ ï¼Œå¦‚æœ
$r<\min \left(1, \mathbf{P}_{\text {target }}\left(\tilde{x}_{n+t} \mid x_1, \ldots \tilde{x}_{n+t-1}\right) / \mathbf{P}_{\mathrm{draft}}\left(\tilde{x}_{n+t} \mid x_1, \ldots \tilde{x}_{n+t-1}\right)\right)$ ï¼Œé‚£ä¹ˆ $n+t$ ä½ç½®å°±ç”¨ $\tilde{x}_{n+t}$ ï¼Œ
- ä¸ç„¶ï¼Œè®¤ä¸º draft æ¨¡å‹å’Œ target æ¨¡å‹åå·®æœ‰äº›å¤§äº†ï¼Œå°±é€€å‡ºå¾ªç¯ï¼Œå¹¶ç”¨ä¹‹å‰å¾—åˆ°çš„ç»“æœæ¥éšæœºå‡º $x_{n+t}$ :
$$
x_{n+t} \sim\left(\mathbf{P}_{\text {target }}\left(X \mid x_1, \ldots, \tilde{x}_{n+t-1}\right)-\mathbf{P}_{\text {draft }}\left(X \mid x_1, \ldots, \tilde{x}_{n+t-1}\right)\right)_{+}
$$

è¿™ä¸ªæ–¹æ³•çš„ä¸€ä¸ªé‡ç‚¹åœ¨äºï¼Œä»–æ˜¯ç²¾ç¡®è§£ï¼Œä¸æ˜¯è¿‘ä¼¼è§£ã€‚ç®€å•æ¨ä¸€ä¸‹å…¬å¼ï¼Œæœ‰:
$$
\begin{aligned}
\mathbf{P}\left(\text { target é€‰ } x_i\right)= & \sum_j \mathbf{P}\left(\text { target é€‰ } x_i \mid \text { draft é€‰ } x_j\right) \mathbf{P}\left(\text { draft é€‰ } x_j\right) \\
= & \mathbf{P}\left(\text { target é€‰ } x_i \mid \text { draft é€‰ } x_i\right) \mathbf{P}\left(\text { draft é€‰ } x_i\right) \\
& +\sum_{j \neq i} \mathbf{P}\left(\text { target é€‰ } x_i \mid \text { draft é€‰ } x_j\right) \mathbf{P}\left(\text { draft é€‰ } x_j\right) \\
= & \min \left(1, \frac{\mathbf{P}_{\mathrm{T}}\left(x_i\right)}{\mathbf{P}_{\mathrm{D}}\left(x_i\right)}\right) \mathbf{P}_{\mathrm{D}}\left(x_i\right)+ \\
& \sum_{j\ne i}\left(1-\min \left(1, \frac{\mathbf{P}_{\mathrm{T}}\left(x_j\right)}{\mathbf{P}_{\mathrm{D}}\left(x_j\right)}\right)\right) \frac{\max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right)}{\sum_k \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_k\right)-\mathbf{P}_{\mathrm{D}}\left(x_k\right)\right)} \mathbf{P}_{\mathrm{D}}(x_j) \\
= & \min \left(\mathbf{P}_{\mathrm{D}}\left(x_i\right), \mathbf{P}_{\mathrm{T}}\left(x_i\right)\right)+ \\
& \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right) \frac{\sum_{j\ne i}\left(\mathbf{P}_{\mathrm{D}}\left(x_j\right)-\min \left(\mathbf{P}_{\mathrm{D}}\left(x_j\right), \mathbf{P}_{\mathrm{T}}\left(x_j\right)\right)\right.}{\sum_k \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_k\right)-\mathbf{P}_{\mathrm{D}}\left(x_k\right)\right)} \\
= & \min \left(\mathbf{P}_{\mathrm{D}}\left(x_i\right), \mathbf{P}_{\mathrm{T}}\left(x_i\right)\right)+ \\
& \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right) \frac{\left.\sum_{j\ne i} \max \left(0, \mathbf{P}_{\mathrm{D}}\left(x_j\right)-\mathbf{P}_{\mathrm{T}}\left(x_j\right)\right)\right)}{\sum_k \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_k\right)-\mathbf{P}_{\mathrm{D}}\left(x_k\right)\right)} \\
= & \min \left(\mathbf{P}_{\mathrm{D}}\left(x_i\right), \mathbf{P}_{\mathrm{T}}\left(x_i\right)\right)+ \\
& \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right) \frac{\left.\sum_{j} \max \left(0, \mathbf{P}_{\mathrm{D}}\left(x_j\right)-\mathbf{P}_{\mathrm{T}}\left(x_j\right)\right)\right)}{\sum_k \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_k\right)-\mathbf{P}_{\mathrm{D}}\left(x_k\right)\right)} \\
= & \min \left(\mathbf{P}_{\mathrm{D}}\left(x_i\right), \mathbf{P}_{\mathrm{T}}\left(x_i\right)\right)+\max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right) \\
= & \mathbf{P}_{\mathrm{T}}\left(x_i\right)=\mathbf{P}_{\mathrm{target}}\left(x_i \mid x_1, \ldots x_{i-1}\right)
\end{aligned}
$$
è¿™é‡Œçš„å€’æ•°ç¬¬å››è¡Œåˆ°å€’æ•¸ç¬¬ä¸‰è¡Œçš„æ¨å°åˆ©ç”¨ $\max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right)\max \left(0, \mathbf{P}_{\mathrm{D}}\left(x_i\right)-\mathbf{P}_{\mathrm{T}}\left(x_i\right)\right)=0$

è¿™é‡Œçš„å€’æ•°ç¬¬ä¸‰è¡Œåˆ°å€’æ•°ç¬¬äºŒè¡Œçš„æ¨å¯¼éœ€è¦è€ƒè™‘åˆ°ï¼Œå› ä¸º $\sum_i \mathbf{P}_{\mathrm{D}}\left(x_i\right)=\sum_i \mathbf{P}_{\mathrm{T}}\left(x_i\right)=1$ ï¼Œè‹¥ä»¤ $\mathcal{I}=\left\{i \mid \mathbf{P}_{\mathrm{D}}\left(x_i\right) \leq \mathbf{P}_{\mathrm{T}}\left(x_i\right)\right\}$ ï¼Œé‚£ä¹ˆä¼šæœ‰:
$$
\sum_{i \in \mathcal{I}} \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)=\sum_{i \notin \mathcal{I}} \mathbf{P}_{\mathrm{D}}\left(x_i\right)-\mathbf{P}_{\mathrm{T}}\left(x_i\right)
$$

ä¹Ÿå°±æ˜¯:
$$
\sum_i \max \left(0, \mathbf{P}_{\mathrm{T}}\left(x_i\right)-\mathbf{P}_{\mathrm{D}}\left(x_i\right)\right)=\sum_i \max \left(0, \mathbf{P}_{\mathrm{D}}\left(x_i\right)-\mathbf{P}_{\mathrm{T}}\left(x_i\right)\right)
$$


## Reference

[å¤§èªè¨€æ¨¡å‹é‡åŒ–æ–¹æ³•å°æ¯”ï¼šGPTQã€GGUFã€AWQ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/667109491)

[QLoRAâ€”â€”æŠ€è¡“æ–¹æ¡ˆç¸½çµç¯‡ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/632717605)

[@guodongLLMTokenizer2023]

