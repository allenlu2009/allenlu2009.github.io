---
title: LLM Tokenizer Code
date: 2024-02-21 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
description: LLM Tokenizer
typora-root-url: ../../allenlu2009.github.io



---





## Source

* [Tiktokenizer](https://tiktokenizer.vercel.app/)   éå¸¸æœ‰ç”¨ online tokenizer!!

* https://juejin.cn/post/7234795667477561402

* https://zhuanlan.zhihu.com/p/424631681

* https://zhuanlan.zhihu.com/p/654745411

* Excellent YouTube video from Karpathy: https://www.youtube.com/watch?v=zduSFxRajkE&t=505s



## é–‹å ´



![image-20240221222405213](/media/image-20240221222405213.png)

GPT2 tokenizer (**GPT2=r50K_base**) sucks!!!!

* æ•ˆç‡ä¸å¥½ (ä»¥ä¸‹æ–‡å­—è¦ 300 tokens)
* æ•¸å­—å®Œå…¨éš¨æ„
* ç©ºç™½éš¨æ„åˆ†ï¼Œpython un-friendly
* éè‹±èªç³»åŸºæœ¬æ˜¯ä¸€å­—ä¸€ token

<img src="/media/image-20240225082417732.png" alt="image-20240225082417732" style="zoom:67%;" />

GPT4 tokenizer (C**100K**_base)

* 40%-50% more efficient than GPT2 tokenizer (185 vs. 300) becasuse 100K vs. 50K vocab.
* 3 å€‹æ•¸å­—ä¸€çµ„ï¼Œå¹¶é random.
* ç©ºç™½åˆç‚ºä¸€å€‹ token, python friendly

<img src="/media/image-20240225082343679.png" alt="image-20240225082343679" style="zoom:67%;" />

é‚„æœ‰ä¸€å€‹ç³»åˆ— (p50k_base)

<img src="/media/image-20240225082310113.png" alt="image-20240225082310113" style="zoom:67%;" />



* ### Larger token number (2x, 100K vs. 50K) trade-off  

  * è³‡è¨Šè¢« 2x å£“ç¸®ï¼ŒåŸä¾†çš„ context length è®Šä¸€åŠã€‚ç­‰æ•ˆå¯ä»¥çœ‹åˆ° 2x longer input context.
  * ä½†æ˜¯, vocab size å’Œ embedding table ä¹Ÿè®Šæˆ 2x,  bigger softmax operation

<img src="/media/image-20240225084038126.png" alt="image-20240225084038126" style="zoom:70%;" />



## Unicode

**Unicode sequence** in python is:  

```python
print(ord('h'))   # 104
print(ord('çš„'))  # 30340
```

æ­¤è™•å°šæœª encode æˆ bytes.

```
[ord(x) for x in "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)"]
[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]
```

### UTF-8/UTF-16/UTF-32 Encode (UTF - Unicode Transformation Format)

Summary: 

UTF-8 encode is preferred

* backward compatible to ASCII
* å¦‚æœæ˜¯ English èªç³»ï¼Œencode æ¯”è¼ƒ compact
* ç¼ºé»å°±æ˜¯ variable length.

UTF-16 åœ¨ English æœƒå¤šäº†ä¸€å€‹ 0.

UTF-32 åœ¨ English æœƒå¤šäº†ä¸‰å€‹ 0.  å¥½è™•æ˜¯ fixed length.

```python
a = list("ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)".encode("utf-8"))   # list to convert utf-8 encode to byte stream
print(a)
[236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 240, 159, 145, 139, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]

a = list("ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)".encode("utf-16"))   # list to convert utf-8 encode to byte stream
print(a)
[255, 254, 72, 197, 85, 177, 88, 213, 56, 193, 148, 198, 32, 0, 61, 216, 75, 220, 32, 0, 40, 0, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 32, 0, 105, 0, 110, 0, 32, 0, 75, 0, 111, 0, 114, 0, 101, 0, 97, 0, 110, 0, 33, 0, 41, 0]

a = list("ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ (hello in Korean!)".encode("utf-32"))   # list to convert utf-8 encode to byte stream
print(a)
[255, 254, 0, 0, 72, 197, 0, 0, 85, 177, 0, 0, 88, 213, 0, 0, 56, 193, 0, 0, 148, 198, 0, 0, 32, 0, 0, 0, 75, 244, 1, 0, 32, 0, 0, 0, 40, 0, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 0, 0, 105, 0, 0, 0, 110, 0, 0, 0, 32, 0, 0, 0, 75, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 101, 0, 0, 0, 97, 0, 0, 0, 110, 0, 0, 0, 33, 0, 0, 0, 41, 0, 0, 0]
```



## BPE (Byte-Pair Encode, ä¹Ÿå°±æ˜¯ tokenizer)

GPT2:  reference:  Language Models are Unsupervised Multitask Learners





## **åˆè¯†BPE** (from Karpathy Let's build the GPT Tokenizer)

BPE æ˜¯ä¸€ç§ç®€å•çš„æ•°æ®å‹ç¼©ç®—æ³•ï¼Œå®ƒåœ¨ 1994 å¹´å‘è¡¨çš„æ–‡ç« â€œA New Algorithm for Data Compressionâ€ä¸­è¢«é¦–æ¬¡æå‡ºã€‚ä¸‹é¢çš„ç¤ºä¾‹å°†è§£é‡Š BPEã€‚è€è§„çŸ©ï¼Œæˆ‘ä»¬å…ˆç”¨ä¸€å¥è¯æ¦‚æ‹¬å®ƒçš„æ ¸å¿ƒæ€æƒ³ï¼š

**BPEæ¯ä¸€æ­¥éƒ½å°†æœ€å¸¸è§çš„ä¸€å¯¹\*ç›¸é‚»æ•°æ®å•ä½\*æ›¿æ¢ä¸ºè¯¥æ•°æ®ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„ä¸€ä¸ª\*æ–°å•ä½\*ï¼Œåå¤è¿­ä»£ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚**

æ˜¯ä¸æ˜¯å¬èµ·æ¥æ‡‚äº†å´æ„Ÿè§‰æ²¡æœ‰å®Œå…¨æ‡‚ï¼Ÿä¸‹é¢ä¸¾ä¸ªä¾‹å­ã€‚

å‡è®¾æˆ‘ä»¬æœ‰éœ€è¦ç¼–ç ï¼ˆå‹ç¼©ï¼‰çš„æ•°æ® aaabdaaabacã€‚ç›¸é‚»å­—èŠ‚å¯¹ï¼ˆç›¸é‚»æ•°æ®å•ä½åœ¨BPEä¸­çœ‹ä½œç›¸é‚»å­—èŠ‚å¯¹ï¼‰ aa æœ€å¸¸å‡ºç°ï¼Œå› æ­¤æˆ‘ä»¬å°†ç”¨ä¸€ä¸ªæ–°å­—èŠ‚ Z æ›¿æ¢å®ƒã€‚æˆ‘ä»¬ç°åœ¨æœ‰äº† ZabdZabacï¼Œå…¶ä¸­ Z = aaã€‚ä¸‹ä¸€ä¸ªå¸¸è§çš„å­—èŠ‚å¯¹æ˜¯ abï¼Œè®©æˆ‘ä»¬ç”¨ Y æ›¿æ¢å®ƒã€‚æˆ‘ä»¬ç°åœ¨æœ‰ ZYdZYacï¼Œå…¶ä¸­ Z = aa ï¼ŒY = abã€‚å‰©ä¸‹çš„å”¯ä¸€å­—èŠ‚å¯¹æ˜¯ acï¼Œå®ƒåªæœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸å¯¹å®ƒè¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬å¯ä»¥é€’å½’åœ°ä½¿ç”¨å­—èŠ‚å¯¹ç¼–ç å°† ZY ç¼–ç ä¸º Xã€‚æˆ‘ä»¬çš„æ•°æ®ç°åœ¨å·²è½¬æ¢ä¸º XdXacï¼Œå…¶ä¸­ X = ZYï¼ŒY = abï¼ŒZ = aaã€‚å®ƒä¸èƒ½è¢«è¿›ä¸€æ­¥å‹ç¼©ï¼Œå› ä¸ºæ²¡æœ‰å‡ºç°å¤šæ¬¡çš„å­—èŠ‚å¯¹ã€‚é‚£å¦‚ä½•æŠŠå‹ç¼©çš„ç¼–ç å¤åŸå‘¢ï¼Ÿåå‘æ‰§è¡Œä»¥ä¸Šè¿‡ç¨‹å°±è¡Œäº†ã€‚

#### Toy example summary

é–‹å§‹ aaabdaaabac:   4 vocabulary or token size {a:0, b:1, c:2, d:3},   11 token length 

æœ€çµ‚ XdXac:     7 vocabulary or token size {a:0, b:1, c:2, d:3, X=ZY:4, Y=ab:5, Z=aa:6},  5 token length



#### ä¸€èˆ¬ä¾‹å­ BPE "Training"

##### ç”¨ Karpathy çš„ä¾‹ä¸€ (token = 533, vocab_size~336, entropy 6.6bit)ï¼š

é€™å€‹å­—ä¸²åŒ…å« 533 å­—ç¬¦ã€‚ç¶“é UTF-8 encode ä¹‹å¾Œè®Šæˆ 616 byte sequenceã€‚å¢åŠ ä¾†è‡ª non-English å­—ç¬¦ã€‚

> ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.



ç•¶æˆ‘å€‘é€æ­¥ä½¿ç”¨ BPE å¯ä»¥é™ä½ byte length, å¦‚ä¸‹åœ–ä¸Šã€‚å¦‚æœæƒ³çœ‹æ¸›å°‘å¤šå°‘ byte,  å¦‚ä¸‹åœ–ä¸­ï¼›æœƒæ˜¯ä¸€å€‹éæ¸›çš„æ”¶ç›Šå‡½æ•¸ã€‚æœ€å¾Œæ˜¯ incremental byte reduction.  åŸºæœ¬å¾ 80 iterations ä¹‹å¾Œéƒ½æ˜¯å¢åŠ ä¸€å€‹ vocab (token) size æ›ä¸€å€‹ token length reduction,  æ²’æœ‰ä»»ä½•çš„ gain!!  åè€Œå¢åŠ  encode å’Œ decode çš„è¨ˆç®—é‡!!  æˆ‘å€‘ç”¨ entropy æ›´æ¸…æ¥šã€‚

<img src="/media/image-20240228103908303.png" alt="image-20240228103908303" style="zoom:50%;" />

ä¸‹åœ–æ˜¯ self entropy å° iteration (å°±æ˜¯é¡å¤–çš„å­—ç¬¦) çš„è¶¨å‹¢ï¼š

* åŸå§‹ token distribution (iteration = 0) çš„ entropy æœ€å° (5-bits): ç›´è§€ä¸Šåªæœ‰å­—ç¬¦å‡ºç¾çš„é »ç‡ information.  ç†è«–ä¸Š 1-byte uniform distribution çš„ entropy æ˜¯ 8-bit, ä¸éå› çˆ²é uniform distribution, UTF-8 å¤§ç´„æ˜¯ 5-bit.  
* éš¨è‘— iteration å¢åŠ ï¼Œ**total entropy = åŸä¾†å­—ç¬¦ information (ä¸‹é™) +  æ–°çš„ sequence (order) information (ä¸Šå‡)**, æ‰€ä»¥ total entropy å¢åŠ ï¼  
* åŸä¾†å­—ç¬¦ information å’Œæ–‡ç« é•·çŸ­é—œä¿‚ä¸å¤§ï¼Œåªè¦ token æ•¸ç›®é å¤§æ–¼ 1-byte (256 tokens), å¤§ç´„å°±æ˜¯ 5-bit.  
* ç­‰åˆ°æ–°çš„ sequence information æ¶ˆå¤± (å°±æ˜¯ iteration=80, Incremental byte reduction = 1),  å¢åŠ  iteration æ²’æœ‰ä»»ä½•æ„ç¾©ï¼Œpair åè€Œæ¸›å°‘ token æ•¸ç›®ï¼Œentropy é–‹å§‹ä¸‹é™ï¼Œæœ€å¾Œåˆ° 1-bit.

* Entropy çš„ peak å€¼å¤§ç´„æ˜¯ 6.6bit.  åœ¨ increamental byte reduction = 1 ä¹‹å¾Œæœƒæ‰ä¸‹å»ã€‚Vocab size = 256+80 ~ 336.

<img src="/media/image-20240228103055666.png" alt="image-20240228103055666" style="zoom:67%;" />



é–‹å§‹ 256 (1-byte) vocabulary byte sequence ->  å½¢æˆ byte-pair (é€™ä¹Ÿæ˜¯ BPE çš„ä¾†æº)ï¼Œéå›æ¬¡æ•¸æ±ºå®šå¤šå°‘æ–°çš„ vocabulary

256, 257, .....  ç”± training code book:  {0x0:0, 0x1:1, 0x2:2, 0x3:3, ....0xff: 255,  '0x1 0x9': 256, '0x70 0x9': 257, ....} 

*  **vocab_size = 256 + iterations**

* ä¸‹è¡¨å¯è¦‹ appendix

|                 | Token Length | Vocab Size <br>(256+peak entropy iteration) | Initial Entropy | Peak Entropy |
| --------------- | ------------ | ------------------------------------------- | --------------- | ------------ |
| Unicode article | 616          | 336                                         | 5 bit           | 6.6 bit      |
| Unicode article | 24.6K        | 2.2K                                        | 5 bit           | 10.2 bit     |
| Shakespeare     | 1.1M         | 20~50K                                      | 4.8 bit         | 13~14 bit    |

Summary

* ä¸Šé¢éƒ½æ˜¯ training ä¸€å€‹ dataset.   æ‡‰è©²æœ‰ overfit çš„å•é¡Œã€‚è¨ˆç®— entropy æ‡‰è©²è¦ç”¨ä¸åŒçš„ dataset é©—è­‰ï¼Ÿ
* Inference åŒ…å« encode å’Œ decode å…©å€‹éƒ¨åˆ†ã€‚éƒ½æ˜¯ç”¨æŸ¥è¡¨ï¼Ÿ



#### BPE Encode and Decode



Tokenizer is independent of the LLM!!  Can be tuned or trained separately, ä½†æ˜¯ç”¨éå¸¸ç°¡å–®çš„ algorithm + statistics, not ai algorithm!

![image-20240221222453811](/media/image-20240221222453811.png)



Q: Chinese characters tokenizer counts?

Q: trans-tokenizer

Q: çµ±è¨ˆå„ç¨® tokenzier å°æ–¼å£“ç¸®çš„å°æ¯”ï¼Œå€Ÿæ­¤ä¾†åš transcoder.  like bigram in Karpathy's example.

Q: ä½¿ç”¨ Viterbi æˆ–æ˜¯ç§‘å­¸ç©ºé–“çš„æ–¹æ³•ä¾†è™•ç† tokenizer.





## å…©é¡ BPE

ä»¥ä¸Šçš„ BPE ä¾‹å­æ˜¯ OpenAI çš„ tiktoken.   é‚„æœ‰å¦ä¸€å€‹ç³»åˆ—æ˜¯ Google å’Œ Meta ä½¿ç”¨çš„ sentencepiece tokenizer.  ä¸‹åœ–æ˜¯ llama ä½¿ç”¨çš„ tokenzier.  

* æœ€é‡è¦çš„ç‰¹é»æ˜¯ character coverage ä¸¦ä¸æ˜¯ 100%,  è€Œæ˜¯å¯ä»¥è¨­å®šçš„å€¼ï¼
* å°æ–¼ä¸ç›´æ¥ encode çš„ character,  æ­é… byte_fallback, æœƒæ”¹æˆ utf-8 bytes å¦‚ä¸‹åœ–ã€‚
  * â€œé€™æ˜¯ä¸€å¥å¥½è©±â€ï¼šâ€œé€™â€ â€œå¥â€ éƒ½ fall back æˆ byte è¡¨ç¤ºã€‚  
* è‡ªå‹•åŠ  space token (åœ¨ Hello ä¹‹å‰) ä»¥åŠå…¶ä»– extra tokens (e.g. <s>).  ç¨±çˆ² normalization.



<img src="/media/image-20240302114244098.png" alt="image-20240302114244098" style="zoom:50%;" />

<img src="/media/image-20240302125935955.png" alt="image-20240302125935955" style="zoom:50%;" />



|                                                              | GPT-like                                         | Llama-like                                      |
| ------------------------------------------------------------ | ------------------------------------------------ | ----------------------------------------------- |
| Use case                                                     | GPT2/4, minbpe                                   | Llama series, Mistral series                    |
| Pip                                                          | tiktoken                                         | sentencepiece                                   |
| Vocab_size                                                   | 50257 (GPT2), 100257 (GPT4)                      | 32000 (Llama),  ~250K (Gemma)                   |
| Training/Inference tokenizer<br>é€™è£¡æŒ‡ tokenizer, å’Œ LLM ç„¡é—œ | ä¸åŒ (OpenAI åª disclose inference tokenizer)    | ç›¸åŒ (å¯ä»¥ training è‡ªå·± tokenizer)             |
| Normalization (æ”¹å‹• token)                                   | No                                               | Yes                                             |
| Legacy                                                       | å¾ˆå°‘ï¼Œcode is clean                              | å¾ˆå¤šï¼Œcode is messy                             |
| Training éç¨‹                                                | Text -> utf8 code (1-4 byte number) -> BPE bytes | Text -> BPE raw byte -> fall back to utf-8 byte |



- tiktoken encodes to utf-8 and then BPEs bytes
- sentencepiece BPEs the code points and **optionally falls back to utf-8 bytes** for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.

ä¸æ˜¯å¾ˆç¢ºå®šå…©è€…çš„å·®åˆ¥ï¼Ÿä¼¼ä¹åœ¨ GPT-like tokenizer åœ¨å° English / non-English çš„å·®ç•°æ¯”è¼ƒå°ã€‚ä½†åœ¨ Llama tokenizer å° English / non-English å·®ç•°å¾ˆå¤§?   æ‰€ä»¥ GPT å°ä¸­æ—¥éŸ“æ–‡æ¯”è¼ƒå¥½ï¼Ÿ







## Appendix

##### ç”¨ Karpathy çš„ä¾‹ä¸€ (token = 533, vocab_size~336, entropy 6.6bit)ï¼š

é€™å€‹å­—ä¸²åŒ…å« 533 å­—ç¬¦ã€‚ç¶“é UTF-8 encode ä¹‹å¾Œè®Šæˆ 616 byte sequenceã€‚å¢åŠ ä¾†è‡ª non-English å­—ç¬¦ã€‚

> ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.



ç•¶æˆ‘å€‘é€æ­¥ä½¿ç”¨ BPE å¯ä»¥é™ä½ byte length, å¦‚ä¸‹åœ–ä¸Šã€‚å¦‚æœæƒ³çœ‹æ¸›å°‘å¤šå°‘ byte,  å¦‚ä¸‹åœ–ä¸­ï¼›æœƒæ˜¯ä¸€å€‹éæ¸›çš„æ”¶ç›Šå‡½æ•¸ã€‚æœ€å¾Œæ˜¯ incremental byte reduction.  åŸºæœ¬å¾ 80 iterations ä¹‹å¾Œéƒ½æ˜¯å¢åŠ ä¸€å€‹ vocab (token) size æ›ä¸€å€‹ token length reduction,  æ²’æœ‰ä»»ä½•çš„ gain!!  åè€Œå¢åŠ  encode å’Œ decode çš„è¨ˆç®—é‡!! 

<img src="/media/image-20240228103908303.png" alt="image-20240228103908303" style="zoom:50%;" />

Entropy çš„ peak å€¼å¤§ç´„æ˜¯ 6.6bit.  åœ¨ increamental byte reduction = 1 ä¹‹å¾Œæœƒæ‰ä¸‹å»ã€‚Vocab size = 256+80 ~ 336.

<img src="/media/image-20240228103055666.png" alt="image-20240228103055666" style="zoom:67%;" />



##### ç”¨ Karpathy çš„ä¾‹äºŒ (token = 24.6K, vocab_size~2.2K, entropy~10.2bit)ï¼š

<img src="/media/image-20240228105517943.png" alt="image-20240228105517943" style="zoom:50%;" />

å†è©¦ä¸Šæ–‡çš„å…¨æ–‡ï¼Œç´„ 24597 bytes (UTF-8)ã€‚ç¶“é 2000 iterations:  æœ€å¤§ entropy ç´„åœ¨10.2 bits@1900 iteration.  Vocab size ~ 1900+256=2156. 

<img src="/media/image-20240228105218254.png" alt="image-20240228105218254" style="zoom:50%;" />



##### ç”¨ Karpathy çš„ä¾‹ä¸‰ (token = 1.1M, vocab_size > 1K, entropy > 10.2bit)ï¼š

èå£«æ¯”äºçš„æ–‡ç« ï¼Œä¸€å…±æœ‰ 1,115,393 å­—æ¯ã€‚å‰ 100 å€‹å­—æ¯ ï¼ˆå«ç©ºç™½ï¼‰å¦‚ä¸‹ï¼š

> ```
> First Citizen:
> Before we proceed any further, hear me speak.
> 
> All:
> Speak, speak.
> 
> First Citizen:
> You
> ```

ç¶“é UTF-8 encode ä¹‹å¾Œçš„ byte number ä»ç„¶æ˜¯ 1,115,393.  æˆ‘å€‘ä¾æ¨£ç•«è‘«è˜†ï¼Œå¯ä»¥å¾—åˆ°ä»¥ä¸‹çš„åœ–ã€‚

<img src="/media/image-20240227224111340.png" alt="image-20240227224111340" style="zoom:60%;" />



Entropy > 13 bits.

<img src="/media/image-20240228160143494.png" alt="image-20240228160143494" style="zoom:70%;" />

1. **ä½¿ç”¨ normalization æ”¹æˆ percentage or entropy çœ‹æ˜¯å¦æœ‰ç‰©ç†æ„ç¾©** English, Chinese, ....













### Tokenizer

1. character level encoder:  codebook 65
2. BPE (byte-pair encoder)
   * GPT2-3:   codebook 50541?
   * Tiktoken (OpenAI)

åŸºæœ¬æ˜¯ trade-off of the codebook vs. the token length!

[Hii, hello world]:  character tokenizer: 12 tokens;  BPE:  3 tokens

@guodongLLMTokenizer2023



## èƒŒæ™¯

éš¨ç€ChatGPTè¿…é€Ÿå‡ºåœˆï¼Œæœ€è¿‘å¹¾å€‹æœˆé–‹æºçš„å¤§æ¨¡å‹ä¹Ÿæ˜¯éåœ°é–‹èŠ±ã€‚ç›®å‰ï¼Œé–‹æºçš„å¤§èªè¨€æ¨¡å‹ä¸»è¦æœ‰ä¸‰å¤§é¡ï¼šChatGLMè¡ç”Ÿçš„å¤§æ¨¡å‹ï¼ˆwendaã€[ChatSQL](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fyysirs%2FChatSQL)ç­‰ï¼‰ã€LLaMAè¡ç”Ÿçš„å¤§æ¨¡å‹ï¼ˆAlpacaã€Vicunaã€BELLEã€Phoenixã€Chimeraç­‰ï¼‰ã€Bloomè¡ç”Ÿçš„å¤§æ¨¡å‹ï¼ˆBloomzã€BELLEã€Phoenixç­‰ï¼‰ã€‚å…¶ä¸­ï¼ŒChatGLM-6Bä¸»è¦ä»¥ä¸­è‹±é›™èªé€²è¡Œè¨“ç·´ï¼ŒLLaMAä¸»è¦ä»¥è‹±èªçˆ²ä¸»è¦èªè¨€çš„æ‹‰ä¸èªç³»é€²è¡Œè¨“ç·´ï¼Œè€ŒBloomä½¿ç”¨äº†46ç¨®è‡ªç„¶èªè¨€ã€13ç¨®ç·¨ç¨‹èªè¨€é€²è¡Œè¨“ç·´ã€‚

| æ¨¡å‹       | è¨“ç·´æ•¸æ“šé‡                                           | æ¨¡å‹åƒæ•¸  | è¨“ç·´æ•¸æ“šç¯„åœ               | è©è¡¨å¤§å° | åˆ†è©ç®—æ³• |
| ---------- | ---------------------------------------------------- | --------- | -------------------------- | -------- | -------- |
| LLaMA      | 1Tï½1.4T tokens(å…¶ä¸­ï¼Œ7B/13Bä½¿ç”¨1Tï¼Œ33B/65Bä½¿ç”¨1.4T) | 7Bï½65B   | ä»¥è‹±èªçˆ²ä¸»è¦èªè¨€çš„æ‹‰ä¸èªç³» | 32K      | BBPE     |
| ChatGLM-6B | ç´„ 1T tokens                                         | 6B        | ä¸­è‹±é›™èª                   | 130K     | BBPE     |
| Bloom      | 1.6TBé è™•ç†æ–‡æœ¬ï¼Œè½‰æ›çˆ² 350B å”¯ä¸€ tokens             | 300M~176B | 46ç¨®è‡ªç„¶èªè¨€ï¼Œ13ç¨®ç·¨ç¨‹èªè¨€ | 250K     | BBPE     |
| GPT2       |                                                      | ?         |                            | 50K      | Tiktoken |

ç›®å‰ä¾†çœ‹ï¼Œåœ¨é–‹æºå¤§æ¨¡å‹ä¸­ï¼ŒLLaMAç„¡ç–‘æ˜¯å…¶ä¸­æœ€é–ƒäº®çš„æ˜Ÿã€‚ä½†æ˜¯ï¼Œèˆ‡ChatGLM-6Bå’ŒBloomåŸç”Ÿæ”¯æŒä¸­æ–‡ä¸åŒã€‚LLaMA åŸç”Ÿåƒ…æ”¯æŒ Latin æˆ– Cyrillic èªç³»ï¼Œå°æ–¼ä¸­æ–‡æ”¯æŒä¸æ˜¯ç‰¹åˆ¥ç†æƒ³ã€‚åŸç‰ˆLLaMAæ¨¡å‹çš„è©è¡¨å¤§å°æ˜¯32Kï¼Œè€Œå¤šèªè¨€æ¨¡å‹ï¼ˆå¦‚ï¼šXLM-Rã€Bloomï¼‰çš„è©è¡¨å¤§å°ç´„çˆ²250Kã€‚ä»¥ä¸­æ–‡çˆ²ä¾‹ï¼ŒLLaMAè©è¡¨ä¸­çš„ä¸­æ–‡tokenæ¯”è¼ƒå°‘ï¼ˆåªæœ‰å¹¾ç™¾å€‹ï¼‰ã€‚é€™å°‡å°è‡´äº†å…©å€‹å•é¡Œï¼š

- LLaMA åŸç”Ÿtokenizerè©è¡¨ä¸­åƒ…åŒ…å«å°‘é‡ä¸­æ–‡å­—ç¬¦ï¼Œåœ¨å°ä¸­æ–‡å­—é€²è¡Œtokenzationæ™‚ï¼Œä¸€ç®‡ä¸­æ–‡æ¼¢å­—å¾€å¾€è¢«åˆ‡åˆ†æˆå¤šå€‹tokenï¼ˆ2-3å€‹Tokenæ‰èƒ½çµ„åˆæˆä¸€å€‹æ¼¢å­—ï¼‰ï¼Œé¡¯è‘—é™ä½ç·¨è§£ç¢¼çš„æ•ˆç‡ã€‚
- é è¨“ç·´ä¸­æ²’æœ‰å‡ºç¾éæˆ–è€…å‡ºç¾å¾—å¾ˆå°‘çš„èªè¨€å­¸ç¿’å¾—ä¸å……åˆ†ã€‚

çˆ²äº†è§£æ±ºé€™äº›å•é¡Œï¼Œæˆ‘å€‘å¯èƒ½å°±éœ€è¦é€²è¡Œä¸­æ–‡è©è¡¨æ“´å±•ã€‚æ¯”å¦‚ï¼šåœ¨ä¸­æ–‡èªæ–™åº«ä¸Šè¨“ç·´ä¸€ç®‡ä¸­æ–‡tokenizeræ¨¡å‹ï¼Œç„¶å¾Œå°‡ä¸­æ–‡ tokenizer èˆ‡ LLaMA åŸç”Ÿçš„ tokenizer é€²è¡Œåˆä½µï¼Œé€šéçµ„åˆå®ƒå€‘çš„è©å½™è¡¨ï¼Œæœ€çµ‚ç²å¾—ä¸€å€‹åˆä½µå¾Œçš„ tokenizer æ¨¡å‹ã€‚

æœ¬æ–‡å°‡ä»‹ç´¹ä½¿ç”¨`SentencePiece`å·¥å…·å¦‚ä½•ä½¿ç”¨ä¸­æ–‡èªæ–™è¨“ç·´ä¸€å€‹åˆ†è©æ¨¡å‹ã€‚



#### Coding ä¾‹å­

```python
from transformers import AutoTokenizer

bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
gpt_tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "This is a sample sentence for encoding."

inputs = gpt_tokenizer(text, return_tensors="pt")
```



å¦‚ä½•å¾—åˆ° tokenizer?  ç•¶ç„¶ä¹Ÿæ˜¯ pretrain å¾—ä¾†çš„ã€‚ä¸éä¸€èˆ¬æˆ‘å€‘åªå¼•ç”¨å›ºå®šçš„ tokenizer.

ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼ŒTokenizerä¹Ÿæ˜¯åœ¨åºå¤§çš„é¢„è®­ç»ƒè¯­æ–™ä¸Šè®­ç»ƒå‡ºæ¥çš„ï¼Œåªä¸è¿‡ç”±äºè®¡ç®—éœ€æ±‚ç›¸å¯¹è®­ç»ƒæ¨¡å‹å°‘å¾ˆå¤šã€‚
è€Œåœ¨LLAMAä¸­ï¼Œä½œè€…ä½¿ç”¨çš„æ˜¯SentencePieceå¾—åˆ°æœ€åçš„è¯å…¸ï¼ŒTokenizerçš„åŸºåº§ä¹Ÿæ˜¯SentencePieceä¸­çš„`SentencePieceProcessor`ç±»[[2\]](https://zhuanlan.zhihu.com/p/654745411#ref_2), å¦‚ä¸‹å›¾çº¢æ¡†æ‰€ç¤ºï¼ŒLLAMAçš„encodeè¿‡ç¨‹å®é™…ä¸Šæ˜¯é€šè¿‡SentencePieceå®ç°çš„ã€‚









## é å‚™çŸ¥è­˜

è¬›è§£ SentencePiece ä¹‹å‰ï¼Œæˆ‘å€‘å…ˆè¬›è§£ä¸‹åˆ†è©å™¨ï¼ˆTokenizerï¼‰ã€‚

é‚£ä»€éº¼æ˜¯åˆ†è©å™¨ï¼Ÿç°¡å–®é»èªªå°±æ˜¯å°‡å­—ç¬¦åºåˆ—è½‰åŒ–çˆ²æ•¸å­—åºåˆ—ï¼Œå°æ‡‰æ¨¡å‹çš„è¼¸å…¥ã€‚

é€šå¸¸æƒ…æ³ä¸‹ï¼ŒTokenizeræœ‰ä¸‰ç¨®ç²’åº¦ï¼šword/char/subword

- word: æŒ‰ç…§è©é€²è¡Œåˆ†è©ï¼Œå¦‚: `Today is sunday`. å‰‡æ ¹æ“šç©ºæ ¼æˆ–æ¨™é»é€²è¡Œåˆ†å‰²`[today, is, sunday, .]`
- characterï¼šæŒ‰ç…§å–®å­—ç¬¦é€²è¡Œåˆ†è©ï¼Œå°±æ˜¯ä»¥charçˆ²æœ€å°ç²’åº¦ã€‚ å¦‚ï¼š`Today is sunday.` å‰‡æœƒåˆ†å‰²æˆ`[tï¼Œ oï¼Œ dï¼Œaï¼Œyï¼Œ .... ï¼Œsï¼Œuï¼Œnï¼Œdï¼Œaï¼Œyï¼Œ .]`
- subwordï¼šæŒ‰ç…§è©çš„subwordé€²è¡Œåˆ†è©ã€‚å¦‚ï¼š`Today is sunday.` å‰‡æœƒåˆ†å‰²æˆ`[toï¼Œ dayï¼Œis ï¼Œ sï¼Œunï¼Œdayï¼Œ .]`

å¯ä»¥çœ‹åˆ°é€™ä¸‰ç¨®ç²’åº¦åˆ†è©æˆªç„¶ä¸åŒï¼Œå„æœ‰åˆ©å¼Šã€‚

å°æ–¼wordç²’åº¦åˆ†è©ï¼š

- å„ªé»ï¼šè©çš„é‚Šç•Œå’Œå«ç¾©å¾—åˆ°ä¿ç•™ï¼›
- ç¼ºé»ï¼š1ï¼‰è©è¡¨å¤§ï¼Œç¨€æœ‰è©å­¸ä¸å¥½ï¼›2ï¼‰OOVï¼ˆå¯èƒ½è¶…å‡ºè©è¡¨å¤–çš„è©ï¼‰ï¼›3ï¼‰ç„¡æ³•è™•ç†å–®è©å½¢æ…‹é—œä¿‚å’Œè©ç¶´é—œä¿‚ï¼Œæœƒå°‡å…©å€‹æœ¬èº«æ„æ€ä¸€è‡´çš„è©åˆ†æˆå…©å€‹æ¯«ä¸ç›¸åŒçš„IDï¼Œåœ¨è‹±æ–‡ä¸­å°¤çˆ²æ˜é¡¯ï¼Œå¦‚ï¼šcatï¼Œ catsã€‚

å°æ–¼characterç²’åº¦åˆ†è©ï¼š

- å„ªé»ï¼šè©è¡¨æ¥µå°ï¼Œæ¯”å¦‚ï¼š26å€‹è‹±æ–‡å­—æ¯å¹¾ä¹å¯ä»¥çµ„åˆå‡ºæ‰€æœ‰è©ï¼Œ5000å¤šç®‡ä¸­æ–‡å¸¸ç”¨å­—åŸºæœ¬ä¹Ÿèƒ½çµ„åˆå‡ºè¶³å¤ çš„è©å½™ï¼›
- ç¼ºé»ï¼š1ï¼‰ç„¡æ³•æ‰¿è¼‰è±å¯Œçš„èªç¾©ï¼Œè‹±æ–‡ä¸­å°¤çˆ²æ˜é¡¯ï¼Œä½†ä¸­æ–‡å»æ˜¯è¼ƒçˆ²åˆç†ï¼Œä¸­æ–‡ä¸­ç”¨æ­¤ç¨®æ–¹å¼è¼ƒå¤šã€‚2ï¼‰åºåˆ—é•·åº¦å¤§å¹…å¢é•·ï¼›

æœ€å¾Œçˆ²äº†å¹³è¡¡ä»¥ä¸Šå…©ç¨®æ–¹æ³•ï¼Œ åˆæå‡ºäº†åŸºæ–¼ subword é€²è¡Œåˆ†è©ï¼šå®ƒå¯ä»¥è¼ƒå¥½çš„å¹³è¡¡è©è¡¨å¤§å°èˆ‡èªç¾©è¡¨é”èƒ½åŠ›ï¼›å¸¸è¦‹çš„å­è©ç®—æ³•æœ‰Byte-Pair Encoding (BPE) / Byte-level BPEï¼ˆBBPEï¼‰ã€Unigram LMã€WordPieceã€SentencePieceç­‰ã€‚

- BPEï¼šå³å­—ç¯€å°ç·¨ç¢¼ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å¾å­—æ¯é–‹å§‹ï¼Œä¸æ–·æ‰¾è©é »æœ€é«˜ã€ä¸”é€£çºŒçš„å…©å€‹tokenåˆä½µï¼Œç›´åˆ°é”åˆ°ç›®æ¨™è©æ•¸ã€‚
- BBPEï¼šBBPEæ ¸å¿ƒæ€æƒ³å°‡BPEçš„å¾å­—ç¬¦ç´šåˆ¥æ“´å±•åˆ°å­ç¯€ï¼ˆByteï¼‰ç´šåˆ¥ã€‚BPEçš„ä¸€å€‹å•é¡Œæ˜¯å¦‚æœé‡åˆ°äº†unicodeç·¨ç¢¼ï¼ŒåŸºæœ¬å­—ç¬¦é›†å¯èƒ½æœƒå¾ˆå¤§ã€‚BBPEå°±æ˜¯ä»¥ä¸€å€‹å­—ç¯€çˆ²ä¸€ç¨®â€œå­—ç¬¦â€ï¼Œä¸ç®¡å¯¦éš›å­—ç¬¦é›†ç”¨äº†å¹¾å€‹å­—ç¯€ä¾†è¡¨ç¤ºä¸€å€‹å­—ç¬¦ã€‚é€™æ¨£çš„è©±ï¼ŒåŸºç¤å­—ç¬¦é›†çš„å¤§å°å°±é–å®šåœ¨äº†256ï¼ˆ2^8ï¼‰ã€‚æ¡ç”¨BBPEçš„å¥½è™•æ˜¯å¯ä»¥è·¨èªè¨€å…±ç”¨è©è¡¨ï¼Œé¡¯è‘—å£“ç¸®è©è¡¨çš„å¤§å°ã€‚è€Œå£è™•å°±æ˜¯ï¼Œå°æ–¼é¡ä¼¼ä¸­æ–‡é€™æ¨£çš„èªè¨€ï¼Œä¸€æ®µæ–‡å­—çš„åºåˆ—é•·åº¦æœƒé¡¯è‘—å¢é•·ã€‚å› æ­¤ï¼ŒBBPE basedæ¨¡å‹å¯èƒ½æ¯”BPE basedæ¨¡å‹è¡¨ç¾çš„æ›´å¥½ã€‚ç„¶è€Œï¼ŒBBPE sequenceæ¯”èµ·BPEä¾†èªªç•¥é•·ï¼Œé€™ä¹Ÿå°è‡´äº†æ›´é•·çš„è¨“ç·´/æ¨ç†æ™‚é–“ã€‚BBPEå…¶å¯¦èˆ‡BPEåœ¨å¯¦ç¾ä¸Šä¸¦ç„¡å¤§çš„ä¸åŒï¼Œåªä¸éåŸºç¤è©è¡¨ä½¿ç”¨256çš„å­—ç¯€é›†ã€‚
- WordPieceï¼šWordPieceç®—æ³•å¯ä»¥çœ‹ä½œæ˜¯BPEçš„è®Šç¨®ã€‚ä¸åŒçš„æ˜¯ï¼ŒWordPieceåŸºæ–¼æ¦‚ç‡ç”Ÿæˆæ–°çš„subwordè€Œä¸æ˜¯ä¸‹ä¸€æœ€é«˜é »å­—ç¯€å°ã€‚WordPieceç®—æ³•ä¹Ÿæ˜¯æ¯æ¬¡å¾è©è¡¨ä¸­é¸å‡ºå…©å€‹å­è©åˆä½µæˆæ–°çš„å­è©ã€‚BPEé¸æ“‡é »æ•¸æœ€é«˜çš„ç›¸é„°å­è©åˆä½µï¼Œè€ŒWordPieceé¸æ“‡ä½¿å¾—èªè¨€æ¨¡å‹æ¦‚ç‡æœ€å¤§çš„ç›¸é„°å­è©åŠ å…¥è©è¡¨ã€‚
- Unigramï¼šå®ƒå’Œ BPE ä»¥åŠ WordPiece å¾è¡¨é¢ä¸Šçœ‹ä¸€å€‹å¤§çš„ä¸åŒæ˜¯ï¼Œå‰å…©è€…éƒ½æ˜¯åˆå§‹åŒ–ä¸€å€‹å°è©è¡¨ï¼Œç„¶å¾Œä¸€å€‹å€‹å¢åŠ åˆ°é™å®šçš„è©å½™é‡ï¼Œè€Œ Unigram Language Model å»æ˜¯å…ˆåˆå§‹ä¸€å€‹å¤§è©è¡¨ï¼Œæ¥ç€é€šéèªè¨€æ¨¡å‹è©•ä¼°ä¸æ–·æ¸›å°‘è©è¡¨ï¼Œç›´åˆ°é™å®šè©å½™é‡ã€‚
- SentencePieceï¼šSentencePieceå®ƒæ˜¯è°·æ­Œæ¨å‡ºçš„å­è©é–‹æºå·¥å…·åŒ…ï¼Œå®ƒæ˜¯æŠŠä¸€å€‹å¥å­çœ‹ä½œä¸€å€‹æ•´é«”ï¼Œå†æ‹†æˆç‰‡æ®µï¼Œè€Œæ²’æœ‰ä¿ç•™å¤©ç„¶çš„è©èªçš„æ¦‚å¿µã€‚ä¸€èˆ¬åœ°ï¼Œå®ƒæŠŠç©ºæ ¼ä¹Ÿç•¶ä½œä¸€ç¨®ç‰¹æ®Šå­—ç¬¦ä¾†è™•ç†ï¼Œå†ç”¨BPEæˆ–è€…Unigramç®—æ³•ä¾†æ§‹é€ è©å½™è¡¨ã€‚SentencePieceé™¤äº†é›†æˆäº†BPEã€ULMå­è©ç®—æ³•ä¹‹å¤–ï¼ŒSentencePieceé‚„èƒ½æ”¯æŒå­—ç¬¦å’Œè©ç´šåˆ¥çš„åˆ†è©ã€‚

ä¸‹åœ–æ˜¯ä¸€äº›ä¸»æµæ¨¡å‹ä½¿ç”¨çš„åˆ†è©ç®—æ³•ï¼Œæ¯”å¦‚ï¼šGPT-1 ä½¿ç”¨çš„BPEå¯¦ç¾åˆ†è©ï¼ŒLLaMA/BLOOM/GPT2/ChatGLMä½¿ç”¨BBPEå¯¦ç¾åˆ†è©ã€‚BERT/DistilBERT/Electraä½¿ç”¨WordPieceé€²è¡Œåˆ†è©ï¼ŒXLNetå‰‡æ¡ç”¨äº†SentencePieceé€²è¡Œåˆ†è©ã€‚

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c7c54cee78754cda9c4cebaf4f82dc43~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

å¾ä¸Šé¢çš„è¡¨æ ¼ä¸­æˆ‘å€‘ä¹Ÿå¯ä»¥çœ‹åˆ°ç•¶å‰ä¸»æµçš„ä¸€äº›é–‹æºå¤§æ¨¡å‹æœ‰å¾ˆå¤šåŸºæ–¼ BBPE ç®—æ³•ä½¿ç”¨ SentencePiece å¯¦ç¾



![image-20240115211055180](/media/image-20240115211055180.png)







## Some Topics

* Tokenizer conversion:  ä¾‹å¦‚æŠŠ Llama tokenizer è½‰æ›æˆ GPT tokenizer.  å¥½è™•æ˜¯ (1) å¯ä»¥ cascade ä¸åŒ tokenizer models.  (2) mixed (parallel) ä¸åŒ tokenizer models. 

* Tokenizer merge:  English tokenizer and Chinese tokenizer for English and Chinese.



#### Tokenizer Conversion for Cascaded Model

ä¸‹åœ–ä¸€æ˜¯å…©å€‹ä¸åŒ tokenizer LLM model ä¸²æ¥ã€‚ä¸€å€‹ä¾‹å­å°±æ˜¯ speculative decode çš„ draft mode å’Œ original model,  Draft model å¯èƒ½ä½¿ç”¨ä¸€å€‹å°æ¨¡å‹ä½¿ç”¨å’Œ original æ¨¡å‹ä¸åŒçš„ tokenizer.    

æœ€ç›´è¦ºçš„æ–¹æ³•å°±æ˜¯æŠŠ GPT transformer çš„ GPT token å…ˆè½‰æ›æˆ textï¼Œå†ç”± Llama tokenizer è½‰æ›æˆ Llama tokens.   é€™å¥½åƒæ˜¯ä¸€å€‹å¯è¡Œçš„æ–¹æ³•ï¼ŒåŸºæœ¬æ˜¯å…©æ¬¡çš„ table lookup (de-tokenizer + tokenizer)ã€‚

ä¸€å€‹å•é¡Œæ˜¯æˆ‘å€‘æ˜¯å¦å¯ä»¥è¨“ç·´ä¸€å€‹ transcoder model, é¡ä¼¼ç¿»è­¯ã€‚å¾ GPT token è½‰æ›æˆ Llama token? 

å…ˆä¸è«–æ˜¯å¦åˆ’ç®—ï¼Œå› ç‚ºä¸€å€‹ transcoder å¯èƒ½æ¯”å…©æ¬¡çš„ table lookup æ›´è¤‡é›œã€‚ä½†ä¹Ÿæœ‰å¯èƒ½æ›´ç°¡å–®ã€‚å› ç‚º table éƒ½æ˜¯å¾ˆå¤§çš„ table (32K or 50K åƒæ•¸)ã€‚ä¹Ÿè¨±ç°¡å–®çš„ transcoder (CNN, RNN, or transformer) å¯ä»¥é”æˆåŒæ¨£çš„çµæœã€‚

<img src="/media/image-20240116214824827.png" alt="image-20240116214824827" style="zoom:80%;" />



#### å¦‚ä½• train é€™å€‹ transcoder?

ä¸Šåœ– transcoder å¾ˆé›£è¨“ç·´ï¼Œå› ç‚º input token å’Œ output token å¾ˆé›£ç”¢ç”Ÿã€‚å¦‚æœå‹‰å¼·ç”¨å®Œæ•´çš„åœ–ä¸€ï¼Œé‚„æœƒèˆ‡ transformer æœ‰é—œã€‚æ˜¯å¦å¯ä»¥æœ‰å®¹æ˜“çš„ input and output tokens, ä¸¦ä¸”å’Œ transformer ç„¡é—œçš„æ–¹æ³•ï¼Ÿ YES! 

ä¸‹åœ–æ˜¯ transcoder ç­‰åƒ¹çš„æ–¹å¡Šåœ–ã€‚å¦‚æœè¦å¾—åˆ°ä¸€å€‹ GPT de-tokenizer to Llama tokenizer çš„transcoderã€‚ ç›¸ç•¶æ–¼è¨“ç·´ä¸€å€‹ input text to GPT tokenizer è€Œ output æ˜¯ç›¸åŒ text to Llama tokenizer çš„ transcoder.    

å› ç‚º text, Llama tokenizer, GPT tokenizer éƒ½æ˜¯å·²çŸ¥ã€‚ä¸¦ä¸”æ•´å€‹è¨“ç·´å’Œ transformer å®Œå…¨ç„¡é—œï¼

<img src="/media/image-20240116221239870.png" alt="image-20240116221239870" style="zoom:80%;" />



å¯ä»¥æ¨å»£å¦‚æœè¦åšä¸€å€‹  A de-tokenizer to B tokenizer çš„ transcoder,  å¯ä»¥ç”¨ text to A tokenizer ç‚º input, åŒæ¨£ text to B tokenizer ç‚º output.  ç”¨é€™æ¨£ (input, output) å°è¨“ç·´ model (CNN, RNN, or transformer)

* ä¸€å€‹ trick æ˜¯ transcoder çš„ vocab size éœ€è¦æ˜¯å…©å€‹ tokenizers çš„å¤§è€…ã€‚ä»¥å… transcoder æœƒ out of range.



#### Merge Tokenizer

https://discuss.tensorflow.org/t/is-there-an-existing-tokenizer-model-for-chinese-to-english-translation/4520

<img src="/media/image-20240116225813905.png" alt="image-20240116225813905" style="zoom:80%;" />

<img src="/media/image-20240204012558713.png" alt="image-20240204012558713" style="zoom:80%;" />



#### En-Tranformer å’Œ Ch-Transformer å¯ä»¥ç”¨ MoE è§£æ±ºï¼ï¼ï¼





#### RAG:  LLM encoder and LLM generator å¯ä»¥ç”¨ä¸åŒçš„ tokenizers!

![image-20240203200318769](/media/image-20240203200318769.png)



## Appendix
