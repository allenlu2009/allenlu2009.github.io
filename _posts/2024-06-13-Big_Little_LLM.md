---
title: Big Little LLMs Applications
date: 2024-06-13 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
description: LLM Output Token Rate
typora-root-url: ../../allenlu2009.github.io


---





## Source

* Speculative Decoding with Big Little Decoder!! 
  * https://arxiv.org/abs/2302.07863
  
* Cloud-edge hybrid SpD! [[2302.07863\] Speculative Decoding with Big Little Decoder (arxiv.org)](https://arxiv.org/abs/2302.07863)

  

## å‰è¨€

Big-Little Speculative Decode ä¸»è¦æ˜¯è§£æ±º autoregressive generation speed å¤ªæ…¢çš„å•é¡Œã€‚

**æ­¤æŠ€è¡“çš„æ ¸å¿ƒä¾¿åœ¨æ–¼å¦‚ä½•å„˜å¯èƒ½åˆå¿«åˆæº–åœ°ç”Ÿæˆ draft tokenï¼Œä»¥åŠå¦‚ä½•æ›´é«˜æ•ˆåœ°é©—è­‰ (verification)ã€‚**



å…¶ä»–çš„æ‡‰ç”¨ï¼šå‡è¨­å¤§å°æ¨¡å‹ã€‚å¤§æ¨¡å‹åœ¨é›²ï¼Œå°æ¨¡å‹åœ¨ç«¯ã€‚

1. å°æ¨¡å‹ Predict the response length
2. å°æ¨¡å‹ predict local or cloud tasks.



### Speculative Decoding with Big Little Decoder

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.07863)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]

æœ¬æ–‡æŠŠæ˜¯å¦éœ€è¦å¤§æ¨¡å‹é€²è¡Œç¢ºèªçš„æ¬ŠåŠ›äº¤çµ¦äº†å°æ¨¡å‹ï¼Œç¨±ä¹‹çˆ² Fallbackã€‚Fallback ä¹‹å¾Œï¼Œå°æ–¼å…©æ¬¡ Fallback ä¹‹é–“å°æ¨¡å‹ç”Ÿæˆçš„ tokenï¼Œå¼•å…¥ Rollback ç¢ºä¿å…¶æ€§èƒ½ã€‚

åœ¨ç¬¬ n æ¬¡è§£ç¢¼è¿­ä»£ä¸­ï¼Œå°æ¨¡å‹å’Œå¤§æ¨¡å‹å„è‡ªè¼¸å…¥ä¸€å€‹éƒ¨åˆ†ç”Ÿæˆçš„è¼¸å‡ºæ–‡æœ¬ $ y_{1:n-1} = (y_1, \cdots, y_{n-1}) $ï¼Œç„¶å¾Œåˆ†åˆ¥ç”Ÿæˆä¸€å€‹åœ¨æ•´å€‹è©å½™è¡¨ä¸Šçš„æ¦‚ç‡åˆ†ä½ˆ $ p_S(y|y_{1:n-1}) $ å’Œ $ p_L(y|y_{1:n-1}) $ã€‚æ¥è‘—ï¼Œå¾æ¦‚ç‡åˆ†ä½ˆä¸­å–æ¨£ä¸‹ä¸€å€‹è© $ y_{n,S} $ å’Œ $ y_{n,L} $ï¼š

$y_{n,S} \sim p_S(y|y_{1:n-1}) $  ä»¥åŠ $ y_{n,L} \sim p_L(y|y_{1:n-1}) $

**å›é€€ç­–ç•¥ (Fallback Policy)**ï¼šå¦‚æœ $ \max_y p_S(y|y_{1:n-1}) < \alpha_{FB} $ï¼Œå‰‡å›é€€åˆ°å¤§æ¨¡å‹ä¸¦è¨­ç½® $ y_n = y_{n,L} $ã€‚

**å›æ»¾ç­–ç•¥ (Rollback Policy)**ï¼šå¦‚æœå­˜åœ¨ä¸€å€‹æœ€å°çš„ $ m \in [1, n-1] $ ä½¿å¾— $ d(p_S(y|y_{1:m}), p_L(y|y_{1:m})) > \alpha_{RB} $ï¼Œå‰‡å›æ»¾é æ¸¬ $ (y_m, \cdots, y_n) $ ä¸¦è¨­ç½® $ y_m = y_{m,L} $ã€‚

<img src="/media/image-20240613205520138.png" alt="image-20240613205520138" style="zoom:50%;" />

<img src="/media/image-20240613205244302.png" alt="image-20240613205244302" style="zoom: 50%;" />

<img src="/media/image-20240613205402858.png" alt="image-20240613205402858" style="zoom:50%;" />

<img src="/media/image-20240613214327330.png" alt="image-20240613214327330" style="zoom: 50%;" />

å…·é«”ä¾†èªªï¼Œä¸€æ—¦å°æ¨¡å‹åœ¨ç•¶å‰ token è¼¸å‡ºæ©Ÿç‡çš„æœ€å¤§å€¼ä½æ–¼è¨­å®šçš„é–¾å€¼ ğ›¼ğ¹ğµï¼Œå°±é€²è¡Œ Fallbackï¼Œé–‹å§‹å¼•å…¥å¤§æ¨¡å‹é€²è¡Œ verifyã€‚åœ¨ verify éç¨‹ä¸­ï¼Œè¨ˆç®—æ¯å€‹ token ä¸Šå¤§å°æ¨¡å‹è¼¸å‡ºæ©Ÿç‡ä¹‹é–“çš„è·é›¢ ğ‘‘ï¼Œä¸€æ—¦ ğ‘‘ å¤§æ–¼è¨­å®šçš„é–¾å€¼ ğ›¼ğ‘…ğµï¼Œå°±å°‡æ­¤ token æ”¹çˆ²å¤§æ¨¡å‹çš„è¼¸å‡ºï¼Œä¸¦è®“å°æ¨¡å‹åœ¨å¾ŒçºŒå¾é€™å€‹ token é–‹å§‹ç”Ÿæˆã€‚

æ­¤æ–¹æ³•ç„¡æ³•ç¢ºä¿è¼¸å‡ºèˆ‡åŸæ¨¡å‹å®Œå…¨ä¸€è‡´ã€‚å¯¦é©—çš„æ¨¡å‹æ¯”è¼ƒå°ï¼Œæœªè¶…é 1Bã€‚



### GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment

[[paper](https://arxiv.org/pdf/2405.19635)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]



<img src="/media/image-20240613214714355.png" alt="image-20240613214714355" style="zoom:50%;" />



### Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline

[[paper](https://arxiv.org/pdf/2305.13144)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]

**é€™å€‹ paper èªªæ˜ï¼šå› ç‚º LLM å¯ä»¥â€œèªªè©±â€ï¼Œå¯ä»¥ç›´æ¥åˆ©ç”¨é€™å€‹ç‰¹é»ï¼Œè©¢å• LLM å¾—åˆ°è³‡è¨Šä½œç‚ºå¾ŒçºŒæ€§èƒ½å„ªåŒ–ã€‚**

é€™è£¡èˆ‰å…©å€‹ä¾‹å­ï¼š(1) è«‹ LLM å…ˆä¼°è¨ˆå›ç­”çš„é•·åº¦ï¼›(2) è«‹ LLM å…ˆä¼°è¨ˆå•é¡Œçš„é›£åº¦ã€‚  



<img src="/media/image-20240613212946475.png" alt="image-20240613212946475" style="zoom:50%;" />

**ä¼°è¨ˆå›ç­”çš„é•·åº¦ï¼š**

åˆ©ç”¨å¦‚ä¸‹çš„ prompt.   å¦‚æ­¤ LLM åœ¨å›ç­”ä¹‹å‰ï¼Œæœƒå…ˆä¼°è¨ˆå›ç­”çš„é•·åº¦ã€‚å°æ–¼å¤šå€‹ batches çš„å›è¦†ï¼Œå¯ä»¥å…ˆæ’ç¨‹è€Œæœ‰è·Ÿå¥½çš„å„ªåŒ–çµæœã€‚

> Create a fun math question for children. 
>
> **Before responding to the above instruction, you have to predict the length of your response. Print the estimated number of words in your response in the first line. Then change to a new line to respond to the instruction.**
>
> 
>
> ChatGPT
>
> **16**
>
> Imagine you have 10 apples and you give 3 to your friend. How many apples do you have left? ğŸ

ä¸éæœ‰å¹¾å€‹å•é¡Œï¼š

* ä½¿ç”¨ words æ¯” tokens æ›´æº–ç¢ºã€‚
* é¡å¤–çš„ prompt å¯¦éš›ä¸Šå½±éŸ¿å›è¦†ã€‚ (1) å›è¦†çš„é•·åº¦æœ¬èº«å°±æ˜¯è®Šå‹•çš„, depending on temperature. (1) LLM ä¼¼ä¹æœƒæ ¹æ“šå›è¦†çš„é•·åº¦ï¼Œè‡ªå‹•æ§åˆ¶å›è¦†çš„å…§å®¹ã€‚
* é€™å€‹ä¼°è¨ˆä¸¦ä¸æº–ç¢ºï¼Œdepending on LLM size.  GPT4 è¡¨ç¾æœ€å¥½ï¼Œä½†æ˜¯ä¹Ÿä¸æ˜¯ 100%.  é‚„æ˜¯éœ€è¦è£œæ•‘æªæ–½ã€‚



**ä¼°è¨ˆå›ç­”çš„é›£åº¦ï¼š**

* çœ‹èµ·ä¾† GPT4o å’Œ Llama3 8B å°æ–¼å•é¡Œçš„åˆ¤æ–·éƒ½é‚„ä¸éŒ¯ã€‚ä¸éå¯èƒ½è¦åšæ›´å®Œæ•´çš„æ¸¬è©¦ï¼Œåˆ©ç”¨ MMLU æˆ–å…¶ä»– terst dataset. 

> **Assuming there is a 10-times more powerful model than you are.   I will ask a question.  You need to tell me if it's better for you or the more powerful model to answer.**   
>
> **Question: what's the capital of England.**
>
> **ChatGPT**
>
> It's better for me to answer this question.
>
> The capital of England is London.
>
> 
>
> **Question: what is the solution of Riemann Hypothesis**
>
> **ChatGPT**
>
> It's better for the more powerful model to answer this question.
>
> The Riemann Hypothesis is an unsolved problem in mathematics and requires deep expertise and advanced capabilities to address comprehensively.



Llama3 8B

> **Assuming there is a 10-times more powerful model than you are. I will ask a question. You need to tell me if it's better for you or the more powerful model to answer.**
>
> **Question: The capital of England is London.**
>
> **Llama**
>
> A simple question!In this case, it's definitely better for me to answer. The capital of England is a well-known fact, and I can provide the correct answer quickly and accurately. The answer is London.The more powerful model wouldn't be able to provide any additional insight or complexity in this case, so I'm happy to take on the task!
>
> **Question: what is the integration of cosine function?**
>
> **Llama**
>
> What a great question!After analyzing the question, I think it's more suitable for the more powerful model to answer. The integration of the cosine function is a complex mathematical problem that requires a higher level of computational power and mathematical knowledge than I possess.I can try to provide a simple answer, but it would likely be incomplete or inaccurate. The more powerful model, on the other hand, would be able to provide a more detailed and accurate answer, possibly involving advanced mathematical techniques and symbolic manipulation.So, I recommend asking the more powerful model to answer this question.
>
> **Question: what's the height of Mt. Everest?**
>
> **Llama**
>
> A simple question!In this case, it's definitely better for me to answer. The height of Mt. Everest is a well-known fact, and I can provide the correct answer quickly and accurately. The height of Mt. Everest is 8,848 meters (29,029 feet) above sea level.The more powerful model wouldn't be able to provide any additional insight or complexity in this case, so I'm happy to take on the task!









To Do:

* è¨­è¨ˆä¸€å€‹ LLM test using Llama3 8B.  ç„¶å¾Œè©•ä¼°éŒ¯èª¤çš„æ¯”ä¾‹å’Œ dispatch åˆ° GPT4 çš„æ¯”ä¾‹ï¼çœ‹é›£åº¦æ˜¯å¦å¯ä»¥æ­£ç¢ºæŒ‡åˆ°å¤§ model.
* è¨­è¨ˆç‰¹æ®Šçš„ token ä¼°è¨ˆé•·åº¦å’Œé›£åº¦ã€‚





In context prompting æ¯” finetuning æ›´å¯¦ç”¨ã€‚

<img src="/media/image-20240726195611514.png" alt="image-20240726195611514" style="zoom:40%;" />



## Reference



