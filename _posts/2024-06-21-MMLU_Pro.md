---
title: MMLU and MMLU Pro
date: 2024-06-21 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
description: LLM Output Token Rate
typora-root-url: ../../allenlu2009.github.io



---



## Source

* MMLU

  * æ¸¬è©¦é›† dataset: hendrycks  [`MMLU`æ•°æ®é›†](https://github.com/hendrycks/test)ï¼šhttps://github.com/hendrycks/test 
  * æ¸¬è©¦ code: ollmer:  [GitHub - ollmer/mmlu: Measuring Massive Multitask Language Understanding | ICLR 2021](https://github.com/ollmer/mmlu)
  * æ¸¬è©¦ code:  deepeval (JUNK!):  [GitHub - confident-ai/deepeval: The LLM Evaluation Framework](https://github.com/confident-ai/deepeval)
  * Code: in ml_code/llm_evaluation_4_mmlu/evaluate_hf.ipynb and evaluate_llama.ipynb



* MMLU Pro

  * æ¸¬è©¦é›†ï¼šTiger-Lab:   [TIGER-Lab/MMLU-Pro Â· Datasets at Hugging Face](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)
  * [[2406.01574\] MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark (arxiv.org)](https://arxiv.org/abs/2406.01574)
  * æ¸¬è©¦ code:  ollama + ...:   Colab/mmlu_pro_gpt.ipynb and Colab/mmlu_pro_ollama_llama3.ipynb

  * æ¸¬è©¦ code: https://github.com/TIGER-AI-Lab/MMLU-Pro/tree/main
  
  * Sebastian å¥½åƒä¹Ÿæœ‰ code to evaluate MMLU performance?  NO, ä¸æ˜¯ MMLU!!  Some simple examples
    * LLM-from-scratch/ch07/03_model-evaluation/llm-instruction-eval-openi/ollama.ipynb



### MMLUå’ŒMMLU-Proçš„æ¯”è¼ƒ

| **ç‰¹å¾µ**           | **MMLU**                                                     | **MMLU-Pro**                                                 |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **ç¯„åœå’Œå…§å®¹**     | åŒ…å«å„ç¨®é ˜åŸŸçš„å»£æ³›å•é¡Œé›†ï¼Œä¸»è¦ä»¥çŸ¥è­˜ç‚ºä¸»ã€‚è©•ä¼°æ¨¡å‹çš„è¨˜æ†¶å’Œç†è§£èƒ½åŠ›ã€‚ | åœ¨MMLUçš„åŸºç¤ä¸Šæ·»åŠ äº†æ›´è¤‡é›œçš„æ¨ç†å•é¡Œã€‚é‡é»åœ¨æ–¼è©•ä¼°é«˜éšèªçŸ¥æŠ€èƒ½ï¼Œå¦‚å•é¡Œè§£æ±ºå’Œæ‰¹åˆ¤æ€§æ€ç¶­ã€‚ |
| **é›£åº¦ç­‰ç´š**       | åŒ…å«æ··åˆé›£åº¦çš„å•é¡Œï¼Œå…¶ä¸­ä¸€äº›ç›¸å°ç°¡å–®æˆ–ç‘£ç¢ã€‚                 | é€šéå»é™¤ç°¡å–®å’Œå™ªè²å•é¡Œä¸¦æ•´åˆéœ€è¦æ›´æ·±å±¤æ¨ç†çš„å•é¡Œï¼Œé¡¯è‘—æé«˜äº†æŒ‘æˆ°æ€§ã€‚ |
| **é¸é …æ•¸é‡**       | æ¯å€‹å•é¡Œæä¾›å››å€‹é¸é …ã€‚                                       | é¸é …æ“´å±•åˆ°åå€‹ï¼Œå¢åŠ äº†é›£åº¦ï¼Œæ¸›å°‘äº†éš¨æ©ŸçŒœå°çš„å¯èƒ½æ€§ã€‚         |
| **æº–ç¢ºæ€§å’Œæ•æ„Ÿæ€§** | ç•¶å‰æ¨¡å‹å·²é”åˆ°é«˜æº–ç¢ºåº¦ï¼Œå°è‡´æ€§èƒ½è¶¨æ–¼å¹³ç·©ï¼Œå°æç¤ºè®ŠåŒ–æ•æ„Ÿï¼ˆ4-5%æ•æ„Ÿæ€§ï¼‰ã€‚ | ç”±æ–¼é›£åº¦å¢åŠ ï¼Œæº–ç¢ºç‡é¡¯è‘—ä¸‹é™ï¼ˆæ¯”MMLUä½16%åˆ°33%ï¼‰ã€‚å°æç¤ºè®ŠåŒ–çš„æ•æ„Ÿæ€§æ¸›å°‘åˆ°åƒ…2%ï¼Œé¡¯ç¤ºå‡ºæ›´å¤§çš„ç©©å®šæ€§å’Œç©©å¥æ€§ã€‚ |
| **æ¨ç†èˆ‡ç›´æ¥å›ç­”** | æ¨¡å‹é€šå¸¸åœ¨ç›´æ¥å›ç­”æŠ€è¡“ä¸Šè¡¨ç¾è‰¯å¥½ã€‚                           | ä½¿ç”¨éˆå¼æ€è€ƒï¼ˆCoTï¼‰æ¨ç†çš„æ¨¡å‹è¡¨ç¾å„ªæ–¼ç›´æ¥å›ç­”çš„æ¨¡å‹ï¼Œå¼·èª¿æ•¸æ“šé›†å°è¤‡é›œæ¨ç†ä»»å‹™çš„é—œæ³¨ã€‚ |



## Introduction

å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„æµ‹æ˜¯è¡¡é‡å¤§æ¨¡å‹æ•ˆæœçš„å…³é”®æ­¥éª¤ï¼Œä¹Ÿæ˜¯æ¨¡å‹æµæ°´çº¿ä¸­å¿…ä¸å¯å°‘çš„è¿‡ç¨‹ã€‚å¸¸è§çš„å¤§æ¨¡å‹æ’è¡Œæ¦œæˆ–å¹³å°æœ‰[ğŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ã€[OpenCompass](https://opencompass.org.cn/leaderboard-llm)ã€[Chatbot Arena Leaderboard](https://lmsys.org/blog/2023-05-25-leaderboard/).

é‚£ä¹ˆï¼Œå¤§æ¨¡å‹çš„è¯„æµ‹æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ

æœ¬æ–‡å°†ä¼šä»¥`MMLU`æ•°æ®é›†ä¸ºä¾‹ï¼Œè€ƒå¯Ÿä¸»æµå¼€æºå¤§æ¨¡å‹ï¼Œå¦‚LLAMA-2, BaiChuan-2ç­‰æ¨¡å‹çš„è¯„ä¼°å®ç°åŠç»“æœï¼Œå¸Œæœ›èƒ½ç®¡ä¸­è§„è±¹ï¼Œä¸€æ¢ç©¶ç«Ÿã€‚

[NLPï¼ˆä¸ƒåå…«ï¼‰å¤§æ¨¡å‹æ¢ç´¢ï¼šMMLUæ•°æ®é›†è¯„æµ‹ - My Github Blog (percent4.github.io)](https://percent4.github.io/NLPï¼ˆä¸ƒåå…«ï¼‰å¤§æ¨¡å‹æ¢ç´¢ï¼šMMLUæ•°æ®é›†è¯„æµ‹/)





## MMLU Pro æ•°æ®é›†

éš¨è‘—æ¨¡å‹çš„æŒçºŒæ”¹é€²ï¼Œå®ƒå€‘åœ¨ MMLU æ¸¬è©¦ä¸Šçš„è¡¨ç¾é–‹å§‹è¶¨æ–¼å¹³ç·©ï¼Œä½¿å¾—åˆ†è¾¨æ¨¡å‹èƒ½åŠ›çš„å·®ç•°è®Šå¾—è¶Šä¾†è¶Šå›°é›£ã€‚æœ¬æ–‡ä»‹ç´¹äº†MMLU-Proï¼Œä¸€å€‹å¢å¼·çš„æ•¸æ“šé›†ï¼Œæ—¨åœ¨é€šéå¼•å…¥æ›´å…·æŒ‘æˆ°æ€§çš„æ¨ç†å•é¡Œå’Œå°‡**é¸é …å¾å››å€‹æ“´å±•åˆ°åå€‹**ï¼Œä¾†æ“´å±•ä»¥çŸ¥è­˜ç‚ºä¸»çš„MMLUåŸºæº–ã€‚æ­¤å¤–ï¼ŒMMLU-Proæ¶ˆé™¤äº†MMLUä¸­ç‘£ç¢å’Œå™ªè²å•é¡Œã€‚æˆ‘å€‘çš„å¯¦é©—çµæœè¡¨æ˜ï¼Œèˆ‡MMLUç›¸æ¯”ï¼ŒMMLU-Proä¸åƒ…æé«˜äº†æŒ‘æˆ°æ€§ï¼Œ**å°è‡´æº–ç¢ºç‡é¡¯è‘—ä¸‹é™16%åˆ°33%**ï¼Œè€Œä¸”åœ¨ä¸åŒæç¤ºä¸‹é¡¯ç¤ºå‡ºæ›´å¤§çš„ç©©å®šæ€§ã€‚æ¸¬è©¦äº†24ç¨®ä¸åŒçš„æç¤ºæ¨£å¼å¾Œï¼Œæ¨¡å‹åˆ†æ•¸å°æç¤ºè®ŠåŒ–çš„æ•æ„Ÿæ€§å¾MMLUä¸­çš„4-5%é™ä½åˆ°MMLU-Proä¸­çš„åƒ…2%ã€‚æ­¤å¤–ï¼Œæˆ‘å€‘ç™¼ç¾ä½¿ç”¨éˆå¼æ€è€ƒï¼ˆCoTï¼‰æ¨ç†çš„æ¨¡å‹åœ¨MMLU-Proä¸Šæ¯”ç›´æ¥å›ç­”çš„è¡¨ç¾æ›´å¥½ï¼Œé€™èˆ‡åœ¨åŸå§‹MMLUä¸Šçš„ç™¼ç¾å½¢æˆé®®æ˜å°æ¯”ï¼Œè¡¨æ˜MMLU-ProåŒ…å«äº†æ›´å¤šè¤‡é›œçš„æ¨ç†å•é¡Œã€‚æˆ‘å€‘çš„è©•ä¼°ç¢ºèªï¼ŒMMLU-Proæ˜¯ä¸€å€‹æ›´å…·å€åˆ†æ€§çš„åŸºæº–æ¸¬è©¦ï¼Œå¯ä»¥æ›´å¥½åœ°è¿½è¸ªè©²é ˜åŸŸçš„é€²å±•ã€‚



MMLU Pro å¾ 17 å­åˆ†é¡ç°¡åŒ–æˆ 14 å€‹å­åˆ†é¡ã€‚ä¸€å…±æœ‰12032 å€‹å•é¡Œã€‚æ¯”ä¾‹å¦‚ä¸‹åœ–ã€‚

å…¶ä¸­ MMLU åŸä¾†çš„å•é¡Œå äº† 56.6%ã€‚å¦å¤–çš„éƒ¨åˆ†æ˜¯æ–°åŠ çš„å•é¡Œã€‚

<img src="/media/image-20240702113632562.png" alt="image-20240702113632562" style="zoom:100%;" />



MMLU Pro æ•¸æ“šé›†çš„é¡åˆ¥å’Œå­é¡åˆ¥çµæ§‹ï¼š

- **STEM** (6 sub-categories,  å’Œ MMLU ç›¸åŒ)
  - Mathematics
  - Physics
  - Chemistry
  - Biology
  - Computer Science
  - Engineering
- **Humanities** (3 sub-categories, å’Œ MMLU ç›¸åŒ)
  - History
  - Philosophy
  - Law
- **Social Sciences** (2 sub-categoriesï¼Œç§»é™¤ Geography, Politics)
  - Economics
  - Psychology
- **Other **(3 sub-categoriesï¼Œç§»é™¤ Culture)
  - Health
  - Business
  - Other





## MMLU Pro Dataset

ç¬¬ä¸€æ­¥æ˜¯ load dataset.    é€™è£ä½¿ç”¨ TIGER-Lab/MMLU-Pro,  åŸºæœ¬æ˜¯ä¸€å€‹ MMLU çš„ subset.

```python
!pip install datasets
import datasets
#from datasets import load_dataset

dataset = datasets.load_dataset('TIGER-Lab/MMLU-Pro')
```

dataset çš„çµæ§‹å¦‚ä¸‹ï¼š

```python
DatasetDict({
    test: Dataset({
        features: ['question_id', 'question', 'options', 'answer', 'answer_index', 'cot_content', 'category', 'src'],
        num_rows: 12032
    })
    validation: Dataset({
        features: ['question_id', 'question', 'options', 'answer', 'answer_index', 'cot_content', 'category', 'src'],
        num_rows: 70
    })
})
```



Llama3-8B INT4



```
accuracy:  computer science 0.3097560975609756
accuracy:  math 0.18134715025906736
accuracy:  chemistry 0.23586572438162545
accuracy:  engineering 0.29205366357069146
accuracy:  law 0.2561307901907357
accuracy:  biology 0.5564853556485355
accuracy:  health 0.3374083129584352
accuracy:  physics 0.2317167051578137
accuracy:  business 0.2623574144486692
accuracy:  philosophy 0.30060120240480964
accuracy:  economics 0.41706161137440756
accuracy:  other 0.2987012987012987
accuracy:  psychology 0.5275689223057645
accuracy:  history 0.3123359580052493
```

To compare with paper:

|      | Overall | Math | Physics | Engineering | History | Law  | Psychology |
| ---- | ------- | ---- | ------- | ----------- | ------- | ---- | ---------- |
|      | 30.8    | 18.1 | 23.1    | 29.2        | 31.2    | 25.6 | 52.8       |
|      |         |      |         |             |         |      |            |
|      |         |      |         |             |         |      |            |

![image-20240703101052696](/media/image-20240703101052696.png)



ä½¿ç”¨ dataviewer 

<img src="/media/image-20240622174142179.png" alt="image-20240622174142179" style="zoom:80%;" />



ä¸€å€‹è¨˜éŒ„å¦‚ä¸‹

```python
print(dataset['test']['question_id'][0])
print(dataset['test']['question'][0])
print(dataset['test']['options'][0])
print(dataset['test']['answer'][0])
print(dataset['test']['answer_index'][0])
print(dataset['test']['category'][0])
#############################################
70 # id
Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must not cause _______ offence.
['Safe practices, Fear, Jealousy, Trivial', 'Unsafe practices, Distress, Joy, Trivial', 'Safe practices, Wants, Jealousy, Trivial', 'Safe practices, Distress, Fear, Trivial', 'Unsafe practices, Wants, Jealousy, Serious', 'Safe practices, Distress, Jealousy, Serious', 'Safe practices, Wants, Fear, Serious', 'Unsafe practices, Wants, Fear, Trivial', 'Unsafe practices, Distress, Fear, Serious']
I
8
business # category
```



## Open AI API

2024/6/19 GPT-4o å’Œ GPT-3.5 Turbo çš„åƒ¹æ ¼å·®ã€‚

GPT-3.5 Turbo ä¾¿å®œ 10 å€ï¼Œä½†æ˜¯æº–ç¢ºç‡ä¹Ÿæ¯”è¼ƒå·®ã€‚

<img src="/media/image-20240619083358544.png" alt="image-20240619083358544" style="zoom:80%;" />



## GPT è¨­å®š

ç¬¬äºŒæ­¥æ˜¯èª¿ç”¨ gpt APIï¼Œå¹¾å€‹é‡é»ï¼š

* **è¨­å®šæ¨¡å‹æœ¬èº«ï¼š**
  * model = "gpt-4o" or "gpt-3.5-turbo-0125"
  * æº«åº¦:  T = 0 æ˜¯ greedy decode.   T = 0.1 é‚„æ˜¯ä»¥ç©©å®šçˆ²ä¸»ã€‚å¦‚æœ T = 1 å‰‡æ˜¯å‰µæ„çˆ²ä¸»ã€‚
  * max_tokens:  æœ€å¤§çš„ context length, ä¹Ÿå°±æ˜¯ KV cache size
  * top_p, frequency_penalty, presence_penalty: ?

* **ä½¿ç”¨çµæ§‹åŒ– message çš„ input prompt:** 
  * role:  è¨‚äººè¨­
  * content:  text çš„ question (å¯¦éš›çš„ input)ã€‚æ‡‰è©²å¯ä»¥åŠ ä¸Š image çš„content.

* **å›å‚³ reponse, å…¶çµæ§‹æ‡‰è©²å’Œ input prompt ä¸€æ¨£**

  * message.content:  é€™æ˜¯ answer

  * choices?  æ˜¯åŒæ™‚ç”¢ç”Ÿå¹¾å€‹ä¸åŒçš„ choices?  



```python
!pip install openai
from openai import OpenAI
client = OpenAI(api_key='put the key')

def run_one_question(question: str):
    response = client.chat.completions.create(
        model="gpt-4o",  # "gpt-3.5-turbo"
        messages=[
            {
                "role": "system",
                "content": "You are an knowledge expert, you are supposed to answer the multi-choice question to derive your final answer as `The answer is ...`."
            },
            {
                "role": "user",
                "content": [
                    {
                    "type": "text",
                    "text": question
                    }
                ]
            }
        ],
        temperature=0.1,
        max_tokens=4096,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
    )

    return response.choices[0].message.content
```



```python
def form_options(options: list):
    option_str = 'Options are:\n'
    opts = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
    for opt, o in zip(options, opts):
        option_str += f'({o}): {opt}' + '\n'
    return option_str


def get_prediction(output):
    pattern = r"answer is \(?([ABCDEFGHIJ])\)?"
    match = re.search(pattern, output)
    if match:
        return match.group(1)
    else:
        print("extraction failed, do a random guess")
        return random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])
    
    
from tqdm import tqdm

print('----------------- Start Answering -------------------')
pbar = tqdm(dataset['test'], desc='Processing', unit='question')
for entry in pbar:
    prefix = prompts[entry['category']]  # prefix consists of examples, 4 shots
    query = prefix + 'Q: ' + entry['question'] + '\n' + form_options(entry['options']) + '\n'
    answer = run_one_question(query)  # answer is from GPT
    entry['solution'] = answer  # store the GPT answer, because entry['answer'] is used for ground truth, so use 'answer'
    answers.append(entry)
    prediction = get_prediction(answer) # get exact letter A, B, .. from GPT answer
    if entry["answer"] == prediction:   # compare ground truth with GPT answer
        success += 1
        per_category_accuracy[entry['category']][0] += 1
    else:
        fail += 1
        per_category_accuracy[entry['category']][1] += 1

    json_string = json.dumps(entry)
    file.write(json_string + '\n')

    success_rate = success / (success + fail)
    pbar.set_description(f'Processing (Success rate: {success_rate:.4f})')

for k, v in per_category_accuracy.items():
    if v[0] + v[1] > 0:
      print('accuracy: ', k, v[0] / (v[0] + v[1]))    
```











