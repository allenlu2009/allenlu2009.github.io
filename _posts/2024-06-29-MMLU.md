---
title: MMLU Dataset and Performance
date: 2024-06-29 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
description: LLM Output Token Rate
typora-root-url: ../../allenlu2009.github.io


---





MMLU æ˜¯å–®é¸é‚„æ˜¯å¾©é¸ï¼Ÿæ‡‰è©²æ˜¯å–®é¸é¡Œã€‚ä½†æ˜¯æ¨™æº–çš„ prompt ç¢ºæ˜¯èªª multiple choices.  æ˜¯ç‚ºäº† confuse LLM?

## Source

MMLU

* æ¸¬è©¦é›† dataset: hendrycks  [`MMLU`æ•°æ®é›†](https://github.com/hendrycks/test)ï¼šhttps://github.com/hendrycks/test 
* æ¸¬è©¦ code: ollmer:  [GitHub - ollmer/mmlu: Measuring Massive Multitask Language Understanding | ICLR 2021](https://github.com/ollmer/mmlu)
* æ¸¬è©¦ code:  deepeval (JUNK!):  [GitHub - confident-ai/deepeval: The LLM Evaluation Framework](https://github.com/confident-ai/deepeval)
* Code: in ml_code/llm_evaluation_4_mmlu/evaluate_hf.ipynb and evaluate_llama.ipynb



MMLU Pro

* æ¸¬è©¦é›†ï¼šTiger-Lab:   [TIGER-Lab/MMLU-Pro Â· Datasets at Hugging Face](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)
*   [[2406.01574\] MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark (arxiv.org)](https://arxiv.org/abs/2406.01574)
* æ¸¬è©¦ code:  ollama + ...:   Colab/mmlu_pro_gpt.ipynb and Colab/mmlu_pro_ollama_llama3.ipynb

* Sebastian å¥½åƒä¹Ÿæœ‰ code to evaluate MMLU performance?  NO, ä¸æ˜¯ MMLU!!  Some simple examples
  * LLM-from-scratch/ch07/03_model-evaluation/llm-instruction-eval-openi/ollama.ipynb



### MMLUå’ŒMMLU-Proçš„æ¯”è¼ƒ

| **ç‰¹å¾µ**           | **MMLU**                                                     | **MMLU-Pro**                                                 |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **ç¯„åœå’Œå…§å®¹**     | åŒ…å«å„ç¨®é ˜åŸŸçš„å»£æ³›å•é¡Œé›†ï¼Œä¸»è¦ä»¥çŸ¥è­˜ç‚ºä¸»ã€‚è©•ä¼°æ¨¡å‹çš„è¨˜æ†¶å’Œç†è§£èƒ½åŠ›ã€‚ | åœ¨MMLUçš„åŸºç¤ä¸Šæ·»åŠ äº†æ›´è¤‡é›œçš„æ¨ç†å•é¡Œã€‚é‡é»åœ¨æ–¼è©•ä¼°é«˜éšèªçŸ¥æŠ€èƒ½ï¼Œå¦‚å•é¡Œè§£æ±ºå’Œæ‰¹åˆ¤æ€§æ€ç¶­ã€‚ |
| **é›£åº¦ç­‰ç´š**       | åŒ…å«æ··åˆé›£åº¦çš„å•é¡Œï¼Œå…¶ä¸­ä¸€äº›ç›¸å°ç°¡å–®æˆ–ç‘£ç¢ã€‚                 | é€šéå»é™¤ç°¡å–®å’Œå™ªè²å•é¡Œä¸¦æ•´åˆéœ€è¦æ›´æ·±å±¤æ¨ç†çš„å•é¡Œï¼Œé¡¯è‘—æé«˜äº†æŒ‘æˆ°æ€§ã€‚ |
| **é¸é …æ•¸é‡**       | æ¯å€‹å•é¡Œæä¾›å››å€‹é¸é …ã€‚                                       | é¸é …æ“´å±•åˆ°åå€‹ï¼Œå¢åŠ äº†é›£åº¦ï¼Œæ¸›å°‘äº†éš¨æ©ŸçŒœå°çš„å¯èƒ½æ€§ã€‚         |
| **æº–ç¢ºæ€§å’Œæ•æ„Ÿæ€§** | ç•¶å‰æ¨¡å‹å·²é”åˆ°é«˜æº–ç¢ºåº¦ï¼Œå°è‡´æ€§èƒ½è¶¨æ–¼å¹³ç·©ï¼Œå°æç¤ºè®ŠåŒ–æ•æ„Ÿï¼ˆ4-5%æ•æ„Ÿæ€§ï¼‰ã€‚ | ç”±æ–¼é›£åº¦å¢åŠ ï¼Œæº–ç¢ºç‡é¡¯è‘—ä¸‹é™ï¼ˆæ¯”MMLUä½16%åˆ°33%ï¼‰ã€‚å°æç¤ºè®ŠåŒ–çš„æ•æ„Ÿæ€§æ¸›å°‘åˆ°åƒ…2%ï¼Œé¡¯ç¤ºå‡ºæ›´å¤§çš„ç©©å®šæ€§å’Œç©©å¥æ€§ã€‚ |
| **æ¨ç†èˆ‡ç›´æ¥å›ç­”** | æ¨¡å‹é€šå¸¸åœ¨ç›´æ¥å›ç­”æŠ€è¡“ä¸Šè¡¨ç¾è‰¯å¥½ã€‚                           | ä½¿ç”¨éˆå¼æ€è€ƒï¼ˆCoTï¼‰æ¨ç†çš„æ¨¡å‹è¡¨ç¾å„ªæ–¼ç›´æ¥å›ç­”çš„æ¨¡å‹ï¼Œå¼·èª¿æ•¸æ“šé›†å°è¤‡é›œæ¨ç†ä»»å‹™çš„é—œæ³¨ã€‚ |



## Introduction

å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„æµ‹æ˜¯è¡¡é‡å¤§æ¨¡å‹æ•ˆæœçš„å…³é”®æ­¥éª¤ï¼Œä¹Ÿæ˜¯æ¨¡å‹æµæ°´çº¿ä¸­å¿…ä¸å¯å°‘çš„è¿‡ç¨‹ã€‚å¸¸è§çš„å¤§æ¨¡å‹æ’è¡Œæ¦œæˆ–å¹³å°æœ‰[ğŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ã€[OpenCompass](https://opencompass.org.cn/leaderboard-llm)ã€[Chatbot Arena Leaderboard](https://lmsys.org/blog/2023-05-25-leaderboard/).

é‚£ä¹ˆï¼Œå¤§æ¨¡å‹çš„è¯„æµ‹æ˜¯å¦‚ä½•å®ç°çš„å‘¢ï¼Ÿ

æœ¬æ–‡å°†ä¼šä»¥`MMLU`æ•°æ®é›†ä¸ºä¾‹ï¼Œè€ƒå¯Ÿä¸»æµå¼€æºå¤§æ¨¡å‹ï¼Œå¦‚LLAMA-2, BaiChuan-2ç­‰æ¨¡å‹çš„è¯„ä¼°å®ç°åŠç»“æœï¼Œå¸Œæœ›èƒ½ç®¡ä¸­è§„è±¹ï¼Œä¸€æ¢ç©¶ç«Ÿã€‚

[NLPï¼ˆä¸ƒåå…«ï¼‰å¤§æ¨¡å‹æ¢ç´¢ï¼šMMLUæ•°æ®é›†è¯„æµ‹ - My Github Blog (percent4.github.io)](https://percent4.github.io/NLPï¼ˆä¸ƒåå…«ï¼‰å¤§æ¨¡å‹æ¢ç´¢ï¼šMMLUæ•°æ®é›†è¯„æµ‹/)



## MMLUæ•°æ®é›†

[`MMLU`æ•°æ®é›†](https://github.com/hendrycks/test)å·²å¼€æºè‡³Githubå¹³å°ï¼Œè®¿é—®ç½‘å€ä¸ºï¼šhttps://github.com/hendrycks/test .

**MMLU**ï¼ˆMassive Multitask Language Understandingï¼‰æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¡¡é‡åœ¨**é›¶æ ·æœ¬**ï¼ˆzero-shotï¼‰å’Œ**å°‘æ ·æœ¬**ï¼ˆfew-shotï¼‰æƒ…å½¢ä¸‹ï¼Œå¤§æ¨¡å‹åœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„ä¸–ç•ŒçŸ¥è¯†ã€‚è¿™ä½¿å¾—è¯¥åŸºå‡†æµ‹è¯•æ›´å…·æŒ‘æˆ˜æ€§ï¼Œä¹Ÿæ›´ç±»ä¼¼äºæˆ‘ä»¬è¯„ä¼°äººç±»çš„æ–¹å¼ã€‚è¯¥åŸºå‡†æ¶µç›– STEMã€äººæ–‡ï¼ˆhumanitiesï¼‰ã€ç¤¾ä¼šç§‘å­¦ï¼ˆsocial sciencesï¼‰ç­‰é¢†åŸŸçš„ **57 ä¸ªå­¦ç§‘**ï¼ˆsubjectï¼‰ã€‚ å®ƒçš„éš¾åº¦ä»åˆçº§åˆ°é«˜çº§ï¼Œæ—¢è€ƒéªŒä¸–ç•ŒçŸ¥è¯†ï¼Œåˆè€ƒéªŒè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ å­¦ç§‘èŒƒå›´ä»æ•°å­¦å’Œå†å²ç­‰ä¼ ç»Ÿé¢†åŸŸåˆ°æ³•å¾‹å’Œä¼¦ç†ç­‰æ›´ä¸ºä¸“ä¸šçš„é¢†åŸŸã€‚å­¦ç§‘çš„ç²’åº¦å’Œå¹¿åº¦ä½¿è¯¥åŸºå‡†æˆä¸ºè¯†åˆ«æ¨¡å‹ç›²ç‚¹çš„ç†æƒ³é€‰æ‹©ã€‚

### é¡åˆ¥å’Œå­é¡åˆ¥

MMLU æ•¸æ“šé›†çš„é¡åˆ¥å’Œå­é¡åˆ¥çµæ§‹ï¼š

- **STEM** (6 sub-categories)
  - Mathematics
  - Physics
  - Chemistry
  - Biology
  - Computer Science
  - Engineering
- **Humanities** (3 sub-categories)
  - History
  - Philosophy
  - Law
- **Social Sciences** (4 sub-categories)
  - Economics
  - Psychology
  - Political Science
  - Geography
- **Other **(4 sub-categories)
  - Health
  - Culture
  - Business
  - Other

MMLUæ•°æ®é›†å…±æ”¶é›†äº†**15908ä¸ªé—®é¢˜**ï¼Œå¹¶å°†å…¶åˆ†ä¸ºfew-shotå¼€å‘é›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚ few-shotå¼€å‘é›†æ¯ä¸ªå­¦ç§‘æœ‰5ä¸ªé—®é¢˜ï¼ŒéªŒè¯é›†å¯ç”¨äºé€‰æ‹©è¶…å‚æ•°ï¼Œç”±1540ä¸ªé—®é¢˜ç»„æˆï¼Œæµ‹è¯•é›†æœ‰14079ä¸ªé—®é¢˜ã€‚ æ¯ä¸ªå­¦ç§‘è‡³å°‘åŒ…å«100ä¸ªæµ‹è¯•é—®é¢˜ï¼Œè¿™æ¯”å¤§å¤šæ•°æ—¨åœ¨è¯„ä¼°äººç±»çš„è€ƒè¯•éƒ½è¦é•¿ã€‚

æˆ‘ä»¬æ¥çœ‹å…¶ä¸­ä¸€ä¸ªç¤ºä¾‹ï¼š

```
Question: Glucose is transported into the muscle cell:

Choices:
A. via protein transporters called GLUT4.
B. only in the presence of insulin.
C. via hexokinase.
D. via monocarbylic acid transporters.

Correct answer: A
```



Dataset çš„æ ¼å¼:   [question, subject, choices, answer]

* question:  å°±æ˜¯å•é¡Œæœ¬èº«
* subject: å°±æ˜¯ categoryï¼Œä¸€å…±æœ‰ 57 åˆ†é¡
* choices:  4 å€‹å¯èƒ½çš„é¸æ“‡ï¼Œå°æ‡‰ {A, B, C, D}
* answer: çœŸæ­£çš„ç­”æ¡ˆ

<img src="/media/image-20240622213641055.png" alt="image-20240622213641055" style="zoom:80%;" />





## ä¸åŒå¤§èªè¨€æ¨¡å‹ MMLU æ€§èƒ½



57 é¡åˆ¥ä¸€èˆ¬åˆ†æˆ 4 å¤§é¡:  STEM (Science, Technology, ? ç§‘å­¸æŠ€è¡“ç›¸é—œ),  Humanities (Law, Philosophy,  å¾äºé‡Œå£«å¤šå¾·å°±å­˜åœ¨),  Social Science (Economics, Psychology, ?,  æƒ³ä¾é™„åˆ°ç§‘å­¸),  Others (é†«å­¸ç›¸é—œ)

| STEM (19 å­¸é–€, 1800 é¡Œ)          | Humanities (13 å­¸é–€, 1638 é¡Œ)    | Social Sciences (12 å­¸é–€, 1368 é¡Œ)      | Other (13 å­¸é–€, 1890 é¡Œ) |
| -------------------------------- | -------------------------------- | --------------------------------------- | ------------------------ |
| ABSTRACT_ALGEBRA                 | FORMAL_LOGIC                     | ECONOMETRICS                            | BUSINESS_ETHICS          |
| ANATOMY                          | HIGH_SCHOOL_<br>EUROPEAN_HISTORY | HIGH_SCHOOL_<br>GEOGRAPHY               | CLINICAL_KNOWLEDGE       |
| ASTRONOMY                        | HIGH_SCHOOL_<br>US_HISTORY       | HIGH_SCHOOL_<br>GOVERNMENT_AND_POLITICS | COLLEGE_MEDICINE         |
| COLLEGE_BIOLOGY                  | HIGH_SCHOOL_<br>WORLD_HISTORY    | HIGH_SCHOOL<br>_MACROECONOMICS          | GLOBAL_FACTS             |
| COLLEGE_CHEMISTRY                | INTERNATIONAL_LAW                | HIGH_SCHOOL<br>_MICROECONOMICS          | HUMAN_AGING              |
| COLLEGE_COMPUTER_SCIENCE         | JURISPRUDENCE                    | HIGH_SCHOOL_<br>PSYCHOLOGY              | MANAGEMENT               |
| COLLEGE_MATHEMATICS              | LOGICAL_FALLACIES                | HUMAN_SEXUALITY                         | MARKETING                |
| COLLEGE_PHYSICS                  | MORAL_DISPUTES                   | PROFESSIONAL_PSYCHOLOGY                 | MEDICAL_GENETICS         |
| COMPUTER_SECURITY                | MORAL_SCENARIOS                  | PUBLIC_RELATIONS                        | MISCELLANEOUS            |
| CONCEPTUAL_PHYSICS               | PHILOSOPHY                       | SECURITY_STUDIES                        | NUTRITION                |
| ELECTRICAL_ENGINEERING           | PREHISTORY                       | SOCIOLOGY                               | PROFESSIONAL_ACCOUNTING  |
| ELEMENTARY_MATHEMATICS           | PROFESSIONAL_LAW                 | US_FOREIGN_POLICY                       | PROFESSIONAL_MEDICINE    |
| HIGH_SCHOOL_BIOLOGY              | WORLD_RELIGIONS                  |                                         | VIROLOGY                 |
| HIGH_SCHOOL_<br>CHEMISTRY        |                                  |                                         |                          |
| HIGH_SCHOOL_<br>COMPUTER_SCIENCE |                                  |                                         |                          |
| HIGH_SCHOOL_<br>MATHEMATICS      |                                  |                                         |                          |
| HIGH_SCHOOL_<br>PHYSICS          |                                  |                                         |                          |
| HIGH_SCHOOL_<br>STATISTICS       |                                  |                                         |                          |
| MACHINE_LEARNING                 |                                  |                                         |                          |

4 å¤§é¡ (categories), 17 å­é¡ (sub-categories)

```python
categories = {
    "STEM": ["physics", "chemistry", "biology", "computer science", "math", "engineering"],
    "humanities": ["history", "philosophy", "law"],
    "social sciences": ["politics", "culture", "economics", "geography", "psychology"],
    "other (business, health, misc.)": ["other", "business", "health"],
```





## Use LLM_EVALUATION_4_MMLU! (official)

å®˜æ–¹æ•¸æ“š

<img src="/media/image-20240629113533732.png" alt="image-20240629113533732" style="zoom:67%;" />

è‡ªå·±åŸ·è¡Œçš„çµæœï¼š

| æ¨¡å‹                  | Accuracy | STEM (18,1800) | Humanities (13,1638) | Social Sciences (12, 1368) | Others (14, 1890) |
| --------------------- | -------- | -------------- | -------------------- | -------------------------- | ----------------- |
| Llama3-8B (5 shot)    | **65.0** | 55.8           | 58.9                 | 76.3                       | 71.6              |
| Llama2-7B (5 shot)    | 46.0     | 37.0           | 43.3                 | 51.8                       | 52.4              |
| Mistral-7B (5 shot)   | 62.6     | 52.6           | 56.5                 | 73.5                       | 70.4              |
| Phi3-3.8B-4K (5 shot) | **69.2** | 59.8           | 65.4                 | 80.1                       | 73.1              |

* Phi-3-3.8B è¡¨ç¾å‹é Llama3-8B?  ä¸ç¢ºå®šæ˜¯å¦å› ç‚º training é
* All models: æœ€å·®çš„æ˜¯ STEM, æœ€å¥½çš„æ˜¯ Social Sciences.
* All models: STEM æœ€å·®çš„æ˜¯ **math**, Humanities æœ€å·®çš„æ˜¯ **law**. 



#### Microsoft Phi3

**Category and Sub-category**

```
Average accuracy 0.483 - math
Average accuracy 0.707 - health
Average accuracy 0.606 - physics
Average accuracy 0.826 - business
Average accuracy 0.837 - biology
Average accuracy 0.558 - chemistry
Average accuracy 0.653 - computer science
Average accuracy 0.733 - economics
Average accuracy 0.593 - engineering
Average accuracy 0.681 - philosophy
Average accuracy 0.729 - other
Average accuracy 0.792 - history
Average accuracy 0.869 - geography
Average accuracy 0.818 - politics
Average accuracy 0.817 - psychology
Average accuracy 0.828 - culture
Average accuracy 0.551 - law
Average accuracy 0.598 - STEM
Average accuracy 0.654 - humanities
Average accuracy 0.801 - social sciences
Average accuracy 0.731 - other (business, health, misc.)
Average accuracy: 0.692
```

**Subject**

```
Average accuracy 0.370 - abstract_algebra
Average accuracy 0.667 - anatomy
Average accuracy 0.776 - astronomy
Average accuracy 0.680 - business_ethics
Average accuracy 0.743 - clinical_knowledge
Average accuracy 0.826 - college_biology
Average accuracy 0.470 - college_chemistry
Average accuracy 0.560 - college_computer_science
Average accuracy 0.360 - college_mathematics
Average accuracy 0.688 - college_medicine
Average accuracy 0.363 - college_physics
Average accuracy 0.780 - computer_security
Average accuracy 0.706 - conceptual_physics
Average accuracy 0.456 - econometrics
Average accuracy 0.593 - electrical_engineering
Average accuracy 0.524 - elementary_mathematics
Average accuracy 0.603 - formal_logic
Average accuracy 0.390 - global_facts
Average accuracy 0.842 - high_school_biology
Average accuracy 0.601 - high_school_chemistry
Average accuracy 0.730 - high_school_computer_science
Average accuracy 0.812 - high_school_european_history
Average accuracy 0.869 - high_school_geography
Average accuracy 0.912 - high_school_government_and_politics
Average accuracy 0.749 - high_school_macroeconomics
Average accuracy 0.400 - high_school_mathematics
Average accuracy 0.840 - high_school_microeconomics
Average accuracy 0.444 - high_school_physics
Average accuracy 0.883 - high_school_psychology
Average accuracy 0.625 - high_school_statistics
Average accuracy 0.804 - high_school_us_history
Average accuracy 0.793 - high_school_world_history
Average accuracy 0.691 - human_aging
Average accuracy 0.771 - human_sexuality
Average accuracy 0.851 - international_law
Average accuracy 0.796 - jurisprudence
Average accuracy 0.804 - logical_fallacies
Average accuracy 0.554 - machine_learning
Average accuracy 0.806 - management
Average accuracy 0.897 - marketing
Average accuracy 0.810 - medical_genetics
Average accuracy 0.825 - miscellaneous
Average accuracy 0.757 - moral_disputes
Average accuracy 0.583 - moral_scenarios
Average accuracy 0.752 - nutrition
Average accuracy 0.765 - philosophy
Average accuracy 0.775 - prehistory
Average accuracy 0.582 - professional_accounting
Average accuracy 0.510 - professional_law
Average accuracy 0.761 - professional_medicine
Average accuracy 0.758 - professional_psychology
Average accuracy 0.736 - public_relations
Average accuracy 0.763 - security_studies
Average accuracy 0.866 - sociology
Average accuracy 0.860 - us_foreign_policy
Average accuracy 0.494 - virology
Average accuracy 0.825 - world_religions
```



#### Llama3-8B

**Category and Sub-category**

```
Average accuracy 0.444 - math
Average accuracy 0.705 - health
Average accuracy 0.548 - physics
Average accuracy 0.826 - business
Average accuracy 0.769 - biology
Average accuracy 0.521 - chemistry
Average accuracy 0.636 - computer science
Average accuracy 0.655 - economics
Average accuracy 0.634 - engineering
Average accuracy 0.580 - philosophy
Average accuracy 0.689 - other
Average accuracy 0.780 - history
Average accuracy 0.813 - geography
Average accuracy 0.810 - politics
Average accuracy 0.780 - psychology
Average accuracy 0.825 - culture
Average accuracy 0.499 - law
Average accuracy 0.558 - STEM
Average accuracy 0.589 - humanities
Average accuracy 0.763 - social sciences
Average accuracy 0.716 - other (business, health, misc.)
Average accuracy: 0.650
```

**Subject**

```
Average accuracy 0.320 - abstract_algebra
Average accuracy 0.644 - anatomy
Average accuracy 0.697 - astronomy
Average accuracy 0.630 - business_ethics
Average accuracy 0.751 - clinical_knowledge
Average accuracy 0.757 - college_biology
Average accuracy 0.500 - college_chemistry
Average accuracy 0.560 - college_computer_science
Average accuracy 0.360 - college_mathematics
Average accuracy 0.636 - college_medicine
Average accuracy 0.471 - college_physics
Average accuracy 0.800 - computer_security
Average accuracy 0.574 - conceptual_physics
Average accuracy 0.412 - econometrics
Average accuracy 0.634 - electrical_engineering
Average accuracy 0.437 - elementary_mathematics
Average accuracy 0.508 - formal_logic
Average accuracy 0.310 - global_facts
Average accuracy 0.774 - high_school_biology
Average accuracy 0.532 - high_school_chemistry
Average accuracy 0.680 - high_school_computer_science
Average accuracy 0.758 - high_school_european_history
Average accuracy 0.813 - high_school_geography
Average accuracy 0.896 - high_school_government_and_politics
Average accuracy 0.659 - high_school_macroeconomics
Average accuracy 0.374 - high_school_mathematics
Average accuracy 0.765 - high_school_microeconomics
Average accuracy 0.411 - high_school_physics
Average accuracy 0.846 - high_school_psychology
Average accuracy 0.639 - high_school_statistics
Average accuracy 0.828 - high_school_us_history
Average accuracy 0.827 - high_school_world_history
Average accuracy 0.700 - human_aging
Average accuracy 0.771 - human_sexuality
Average accuracy 0.860 - international_law
Average accuracy 0.759 - jurisprudence
Average accuracy 0.724 - logical_fallacies
Average accuracy 0.518 - machine_learning
Average accuracy 0.874 - management
Average accuracy 0.889 - marketing
Average accuracy 0.790 - medical_genetics
Average accuracy 0.814 - miscellaneous
Average accuracy 0.717 - moral_disputes
Average accuracy 0.411 - moral_scenarios
Average accuracy 0.755 - nutrition
Average accuracy 0.723 - philosophy
Average accuracy 0.725 - prehistory
Average accuracy 0.479 - professional_accounting
Average accuracy 0.452 - professional_law
Average accuracy 0.739 - professional_medicine
Average accuracy 0.721 - professional_psychology
Average accuracy 0.745 - public_relations
Average accuracy 0.755 - security_studies
Average accuracy 0.861 - sociology
Average accuracy 0.850 - us_foreign_policy
Average accuracy 0.566 - virology
Average accuracy 0.836 - world_religions
```



#### Mistral-7B

**Category and Sub-category**

```
Average accuracy 0.401 - math
Average accuracy 0.684 - health
Average accuracy 0.506 - physics
Average accuracy 0.796 - business
Average accuracy 0.756 - biology
Average accuracy 0.518 - chemistry
Average accuracy 0.612 - computer science
Average accuracy 0.636 - economics
Average accuracy 0.579 - engineering
Average accuracy 0.535 - philosophy
Average accuracy 0.699 - other
Average accuracy 0.766 - history
Average accuracy 0.768 - geography
Average accuracy 0.779 - politics
Average accuracy 0.747 - psychology
Average accuracy 0.813 - culture
Average accuracy 0.492 - law
Average accuracy 0.526 - STEM
Average accuracy 0.565 - humanities
Average accuracy 0.735 - social sciences
Average accuracy 0.704 - other (business, health, misc.)
Average accuracy: 0.626
```

**Subject**

```
Average accuracy 0.270 - abstract_algebra
Average accuracy 0.630 - anatomy
Average accuracy 0.658 - astronomy
Average accuracy 0.570 - business_ethics
Average accuracy 0.691 - clinical_knowledge
Average accuracy 0.729 - college_biology
Average accuracy 0.500 - college_chemistry
Average accuracy 0.520 - college_computer_science
Average accuracy 0.400 - college_mathematics
Average accuracy 0.653 - college_medicine
Average accuracy 0.382 - college_physics
Average accuracy 0.770 - computer_security
Average accuracy 0.574 - conceptual_physics
Average accuracy 0.491 - econometrics
Average accuracy 0.579 - electrical_engineering
Average accuracy 0.378 - elementary_mathematics
Average accuracy 0.405 - formal_logic
Average accuracy 0.360 - global_facts
Average accuracy 0.768 - high_school_biology
Average accuracy 0.527 - high_school_chemistry
Average accuracy 0.680 - high_school_computer_science
Average accuracy 0.788 - high_school_european_history
Average accuracy 0.768 - high_school_geography
Average accuracy 0.865 - high_school_government_and_politics
Average accuracy 0.662 - high_school_macroeconomics
Average accuracy 0.341 - high_school_mathematics
Average accuracy 0.664 - high_school_microeconomics
Average accuracy 0.331 - high_school_physics
Average accuracy 0.824 - high_school_psychology
Average accuracy 0.579 - high_school_statistics
Average accuracy 0.789 - high_school_us_history
Average accuracy 0.772 - high_school_world_history
Average accuracy 0.700 - human_aging
Average accuracy 0.786 - human_sexuality
Average accuracy 0.777 - international_law
Average accuracy 0.778 - jurisprudence
Average accuracy 0.791 - logical_fallacies
Average accuracy 0.491 - machine_learning
Average accuracy 0.825 - management
Average accuracy 0.880 - marketing
Average accuracy 0.740 - medical_genetics
Average accuracy 0.816 - miscellaneous
Average accuracy 0.708 - moral_disputes
Average accuracy 0.328 - moral_scenarios
Average accuracy 0.758 - nutrition
Average accuracy 0.695 - philosophy
Average accuracy 0.735 - prehistory
Average accuracy 0.493 - professional_accounting
Average accuracy 0.450 - professional_law
Average accuracy 0.688 - professional_medicine
Average accuracy 0.678 - professional_psychology
Average accuracy 0.673 - public_relations
Average accuracy 0.727 - security_studies
Average accuracy 0.831 - sociology
Average accuracy 0.860 - us_foreign_policy
Average accuracy 0.548 - virology
Average accuracy 0.830 - world_religions
```



## Appendix

## è¯„æµ‹ä»£ç 

æ”¾åœ¨ ml_code/MMLU/.../.pyh

å¼•å…¥ä¸€èˆ¬çš„åº«ã€‚

```python
# -*- encoding: utf-8 -*-

import argparse
import json
import os
import time

import numpy as np
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
```


é€™è£å¼•å…¥äº† categories å’Œ subcategories æ¨¡å¡Šä¸­çš„é¡åˆ¥å’Œå­é¡åˆ¥ä¿¡æ¯ã€‚å®šç¾©äº†å¤šé¸é¡Œçš„é¸é …ï¼Œåˆ†åˆ¥ç‚º A, B, C, Dã€‚


```python
from categories import categories, subcategories

choices = ["A", "B", "C", "D"]
```



å°‡ç§‘ç›®åç¨±ä¸­çš„ä¸‹åŠƒç·šæ›¿æ›ç‚ºç©ºæ ¼ï¼Œä½¿å…¶æ›´å…·å¯è®€æ€§ã€‚

```python
def format_subject(subject):
    l = subject.split("_")
    s = ""
    for entry in l:
        s += " " + entry
    return s
```



ç”¢ç”Ÿ 1-shot çš„ä¾‹å­ã€‚å°‡ pandas data frame (df) ä¸­çš„å–®å€‹å•é¡Œæ ¼å¼åŒ–ç‚ºæ–‡æœ¬æç¤ºã€‚å°±æ˜¯æŠŠ A, B, C, D å’Œå¯é¸æ“‡çš„ç­”æ¡ˆçµåˆã€‚å¦‚æœ include_answer=True, æ–‡æœ¬æç¤ºåŒ…æ‹¬ç­”æ¡ˆã€‚


```python
def format_example(df, idx, include_answer=True):
    prompt = df.iloc[idx, 0]  # question
    k = df.shape[1] - 2
    for j in range(k):
        prompt += "\n{}. {}".format(choices[j], df.iloc[idx, j + 1]) # combine A/B/C/D with choices
    prompt += "\nAnswer:"
    if include_answer:
        prompt += " {}\n\n".format(df.iloc[idx, k + 1]) # combine Answer for in-context learning
    return prompt
```



ç”ŸæˆåŒ…å«ç§‘ç›®å•é¡Œçš„è¨“ç·´æç¤ºï¼Œä¸¦é™„ä¸Š k-shot ä¾‹å­

```python
def gen_prompt(train_df, subject, k=-1):
    prompt = "The following are multiple choice questions (with answers) about {}.\n\n".format(
        format_subject(subject)
    )
    if k == -1:
        k = train_df.shape[0]
    for i in range(k):
        prompt += format_example(train_df, i)  # add k-shots
    return prompt
```



**è©•ä¼°æ¨¡å‹**

```python
@torch.no_grad()
def eval(args, subject, model, tokenizer, dev_df, test_df):
    cors = []
    all_probs = []
    answers = choices[: test_df.shape[1] - 2]

    for i in range(test_df.shape[0]):
        # get prompt and make sure it fits
        k = args.ntrain  
        prompt_end = format_example(test_df, i, include_answer=False)
        train_prompt = gen_prompt(dev_df, subject, k)
        prompt = train_prompt + prompt_end

        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

        while input_ids.shape[-1] > 2048:
            k -= 1
            train_prompt = gen_prompt(dev_df, subject, k)
            prompt = train_prompt + prompt_end
            input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(
                model.device
            )

        label = test_df.iloc[i, test_df.shape[1] - 1]

        logits = model(input_ids=input_ids).logits[0, -1]  # get the last element

        probs = (
            torch.nn.functional.softmax(
                torch.tensor(
                    [
                        logits[tokenizer("A").input_ids[-1]],
                        logits[tokenizer("B").input_ids[-1]],
                        logits[tokenizer("C").input_ids[-1]],
                        logits[tokenizer("D").input_ids[-1]],
                    ]
                ).float(),
                dim=0,
            )
            .detach()
            .cpu()
            .numpy()
        )
        pred = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]

        cor = pred == label
        cors.append(cor)
        all_probs.append(probs)

    acc = np.mean(cors)
    cors = np.array(cors)

    all_probs = np.array(all_probs)
    print("Average accuracy {:.3f} - {}".format(acc, subject))

    return cors, acc, all_probs
```



**ä¸»ç¨‹å¼**: åˆ©ç”¨ -m åŸ·è¡Œ model.

```python
def main(args):
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.float16,
        load_in_8bit=False,
        low_cpu_mem_usage=True,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
    model.eval()
    subjects = sorted(
        [
            f.split("_test.csv")[0]
            for f in os.listdir(os.path.join(args.data_dir, "test"))
            if "_test.csv" in f
        ]
    )

    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
    if not os.path.exists(os.path.join(args.save_dir, "results_{}".format(args.model.split("/")[-1]))):
        os.makedirs(os.path.join(args.save_dir, "results_{}".format(args.model.split("/")[-1])))

    all_cors = []
    subcat_cors = {
        subcat: [] for subcat_lists in subcategories.values() for subcat in subcat_lists
    }
    cat_cors = {cat: [] for cat in categories}

    start_time = time.time()
    for subject in subjects:
        dev_df = pd.read_csv(
            os.path.join(args.data_dir, "dev", subject + "_dev.csv"), header=None
        )[: args.ntrain]
        test_df = pd.read_csv(
            os.path.join(args.data_dir, "test", subject + "_test.csv"), header=None
        )

        cors, acc, probs = eval(args, subject, model, tokenizer, dev_df, test_df)
        subcats = subcategories[subject]
        for subcat in subcats:
            subcat_cors[subcat].append(cors)
            for key in categories.keys():
                if subcat in categories[key]:
                    cat_cors[key].append(cors)
        all_cors.append(cors)

        test_df["{}_correct".format(args.model)] = cors
        for j in range(probs.shape[1]):
            choice = choices[j]
            test_df["{}_choice{}_probs".format(args.model, choice)] = probs[:, j]
        test_df.to_csv(
            os.path.join(
                args.save_dir, "results_{}".format(args.model.split("/")[-1]), "{}.csv".format(subject)
            ),
            index=None,
        )

    results = {"subcategories": {}, "categories": {}}
    for subcat in subcat_cors:
        subcat_acc = np.mean(np.concatenate(subcat_cors[subcat]))
        results["subcategories"][subcat] = subcat_acc
        print("Average accuracy {:.3f} - {}".format(subcat_acc, subcat))

    for cat in cat_cors:
        cat_acc = np.mean(np.concatenate(cat_cors[cat]))
        results["categories"][cat] = cat_acc
        print("Average accuracy {:.3f} - {}".format(cat_acc, cat))
    weighted_acc = np.mean(np.concatenate(all_cors))
    results["weighted_accuracy"] = weighted_acc
    print("Average accuracy: {:.3f}".format(weighted_acc))

    results_file = os.path.join(
        args.save_dir, "accuracies_{}.json".format(args.model.replace("/", "_"))
    )
    end_time = time.time()
    results["cost_time"] = end_time - start_time
    with open(results_file, "w") as f:
        f.write(json.dumps(results, indent=4))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ntrain", "-k", type=int, default=5)  # ?
    parser.add_argument("--data_dir", "-d", type=str, default="data")
    parser.add_argument("--save_dir", "-s", type=str, default="results")
    parser.add_argument("--model", "-m", type=str)  # model
    args = parser.parse_args()
    main(args)
```



## DONOT USE Deepeval (JUNK!)!!

* å…ˆç”¨ç°¡å–®å•é¡Œæ¸¬è©¦ä¸€ä¸‹

* å®¹æ˜“å¡å‰é ­ã€‚è¦æª¢æŸ¥æ˜¯å¦éƒ½æ˜¯ **A**,  å› çˆ²æ˜¯ "**A**nswer: B"
* å®¹æ˜“å¡å¾Œé ­ï¼Œè¦æª¢æŸ¥ä¸€ä¸‹ï¼Œå¦‚ä¸‹åœ– (all D)
* Single choice or multiple choice questions?  Singl choice.



å‰é¢ ok, å¾Œé¢å¥½åƒæœ‰å•é¡Œ

<img src="/media/image-20240629145034361.png" alt="image-20240629145034361" style="zoom: 67%;" />



**The following results seem completely worng!!**

| æ¨¡å‹                            | Accuracy | STEM (18,1800) | Humanities (13,1638) | Social Sciences (12, 1368) |
| ------------------------------- | -------- | -------------- | -------------------- | -------------------------- |
| Llama3-8B-Chinese-Chat (3 shot) |          | **48**         | **65**               | **69**                     |
| Llama3-8B (3 shot)              |          | 43             | 60                   | 61                         |
| Llama2-7B (3 shot)              |          | 30             | 37                   | 37                         |
| Mistral-7B (NG!)                |          | NA             | NA                   | NA                         |
| Phi3-4K (nshot=0!)              |          | 41             | 54                   | 55                         |

















## Remove below



## Open AI API

2024/6/19 GPT-4o å’Œ GPT-3.5 Turbo çš„åƒ¹æ ¼å·®ã€‚

GPT-3.5 Turbo ä¾¿å®œ 10 å€ï¼Œä½†æ˜¯æº–ç¢ºç‡ä¹Ÿæ¯”è¼ƒå·®ã€‚

<img src="/media/image-20240619083358544.png" alt="image-20240619083358544" style="zoom:80%;" />



## GPT è¨­å®š

ç¬¬äºŒæ­¥æ˜¯èª¿ç”¨ gpt APIï¼Œå¹¾å€‹é‡é»ï¼š

* **è¨­å®šæ¨¡å‹æœ¬èº«ï¼š**
  * model = "gpt-4o" or "gpt-3.5-turbo-0125"
  * æº«åº¦:  T = 0 æ˜¯ greedy decode.   T = 0.1 é‚„æ˜¯ä»¥ç©©å®šçˆ²ä¸»ã€‚å¦‚æœ T = 1 å‰‡æ˜¯å‰µæ„çˆ²ä¸»ã€‚
  * max_tokens:  æœ€å¤§çš„ context length, ä¹Ÿå°±æ˜¯ KV cache size
  * top_p, frequency_penalty, presence_penalty: ?

* **ä½¿ç”¨çµæ§‹åŒ– message çš„ input prompt:** 
  * role:  è¨‚äººè¨­
  * content:  text çš„ question (å¯¦éš›çš„ input)ã€‚æ‡‰è©²å¯ä»¥åŠ ä¸Š image çš„content.

* **å›å‚³ reponse, å…¶çµæ§‹æ‡‰è©²å’Œ input prompt ä¸€æ¨£**

  * message.content:  é€™æ˜¯ answer

  * choices?  æ˜¯åŒæ™‚ç”¢ç”Ÿå¹¾å€‹ä¸åŒçš„ choices?  



```python
!pip install openai
from openai import OpenAI
client = OpenAI(api_key='put the key')

def run_one_question(question: str):
    response = client.chat.completions.create(
        model="gpt-4o",  # "gpt-3.5-turbo"
        messages=[
            {
                "role": "system",
                "content": "You are an knowledge expert, you are supposed to answer the multi-choice question to derive your final answer as `The answer is ...`."
            },
            {
                "role": "user",
                "content": [
                    {
                    "type": "text",
                    "text": question
                    }
                ]
            }
        ],
        temperature=0.1,
        max_tokens=4096,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
    )

    return response.choices[0].message.content
```



```python
def form_options(options: list):
    option_str = 'Options are:\n'
    opts = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
    for opt, o in zip(options, opts):
        option_str += f'({o}): {opt}' + '\n'
    return option_str


def get_prediction(output):
    pattern = r"answer is \(?([ABCDEFGHIJ])\)?"
    match = re.search(pattern, output)
    if match:
        return match.group(1)
    else:
        print("extraction failed, do a random guess")
        return random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])
    
    
from tqdm import tqdm

print('----------------- Start Answering -------------------')
pbar = tqdm(dataset['test'], desc='Processing', unit='question')
for entry in pbar:
    prefix = prompts[entry['category']]  # prefix consists of examples, 4 shots
    query = prefix + 'Q: ' + entry['question'] + '\n' + form_options(entry['options']) + '\n'
    answer = run_one_question(query)  # answer is from GPT
    entry['solution'] = answer  # store the GPT answer, because entry['answer'] is used for ground truth, so use 'answer'
    answers.append(entry)
    prediction = get_prediction(answer) # get exact letter A, B, .. from GPT answer
    if entry["answer"] == prediction:   # compare ground truth with GPT answer
        success += 1
        per_category_accuracy[entry['category']][0] += 1
    else:
        fail += 1
        per_category_accuracy[entry['category']][1] += 1

    json_string = json.dumps(entry)
    file.write(json_string + '\n')

    success_rate = success / (success + fail)
    pbar.set_description(f'Processing (Success rate: {success_rate:.4f})')

for k, v in per_category_accuracy.items():
    if v[0] + v[1] > 0:
      print('accuracy: ', k, v[0] / (v[0] + v[1]))    
```











