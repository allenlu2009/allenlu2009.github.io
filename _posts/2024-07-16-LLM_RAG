---
title: LLM RAG - Basic Graph
date: 2024-07-16 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
description: 採樣的常見性和重要性不言而喻
typora-root-url: ../../allenlu2009.github.io


---







https://www.youtube.com/watch?v=n8CIlK_mO2g&ab_channel=huangyihe

Graph RAG example

AnythingLLM: https://hackmd.io/@chrish0729/Hkgggr9WC?utm_source=preview-mode&utm_medium=rec

Obsidian:



## 前言

在語言模型中，溫度參數會影響下一個 token 的概率分佈，從而改變模型輸出的隨機性。數學上，這是通過在應用 softmax 函數以獲得概率之前縮放 logits（未歸一化的對數概率）來實現的。以下是其工作原理的詳細解釋：

### Logits 和 Softmax
在語言模型中，模型輸出一個 logits 向量 \( $\mathbf{z}$ \)，表示每個可能的下一個 token 的未歸一化對數概率。

為了將這些 logits 轉換為概率，我們應用 softmax 函數：

\[ $P(\text{token}_i) = \frac{\exp(z_i)}{\sum_j \exp(z_j)} $\]

其中 \( $z_i$ \) 是 token \( $i$ \) 的 logit。

### 引入溫度
引入溫度參數 \( $T$ \) 以控制概率分佈的銳度或平坦度。數學上，我們在應用 softmax 函數之前將 logits 除以溫度：

$P(\text{token}_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)} $

### 不同溫度值的效果
- **低溫度 ( T < 1 \)**：當 \( T \) 小於 1 時，logits 被除以一個較小的數，使得 logits 之間的差異變大。這會使概率分佈更加銳利，使得高概率的 token 更加可能被選中，而低概率的 token 更加不可能被選中。這樣會產生更具決定性和聚焦的輸出。

  - 例如：\( T = 0.5 \)
    $P(\text{token}_i) = \frac{\exp(z_i / 0.5)}{\sum_j \exp(z_j / 0.5)}$

  - 特例是 T = 0,  只選擇最大概率 $z_i$，也稱為 Greedy decode

    $P(\text{token}_i) = \max{(z_i)}$

- **高溫度 \( T > 1 \)**：當 \( T \) 大於 1 時，logits 被除以一個較大的數，使得 logits 之間的差異變小。這會使概率分佈更加平坦，使得高概率的 token 不那麼占主導地位，並且低概率的 token 更有可能被選中。這樣會產生更隨機和多樣的輸出。

  - 例如：\( T = 2 \)
    $P(\text{token}_i) = \frac{\exp(z_i / 2)}{\sum_j \exp(z_j / 2)}$
  
- **溫度 \( T = 1 \)**：當 \( T \) 等於 1 時，logits 保持不變，softmax 函數會產生原始的概率分佈。

  * $P(\text{token}_i) = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$

### 小結
總結來說，溫度參數修改了語言模型的輸出概率分佈如下：

- **\( T < 1 \)**：分佈更銳利，隨機性更小，輸出更具決定性。
- **\( T > 1 \)**：分佈更平坦，隨機性更大，輸出更具多樣性。
- **\( T = 1 \)**：原始分佈，隨機性不變。

這種調整允許控制模型輸出的創造性和決定性，根據應用需求提供靈活性。





溫度調整通常僅應用於 transformer 的最後一層 softmax。以下是詳細解釋：

### 專注於最後的 Softmax 層

在基於 transformer 的語言模型中，logits 是在應用 softmax 函數生成輸出概率之前的最後一層產生的。溫度縮放在這些最終 logits 上進行，以調整輸出分佈。這是因為最終 logits 直接決定了生成序列中下一個 token 的概率。

### 為什麼只調整最後的 Softmax 層？

- **直接控制輸出**：調整最終 logits 的溫度可以直接控制下一個 token 預測的隨機性。早期的層不直接生成輸出概率，而是通過注意力和前饋機制貢獻輸入數據的轉換。

- **簡單性和效率**：僅在最終 logits 上應用溫度縮放簡化了實施和計算過程。修改內部層將更加複雜且計算成本高，並且沒有明顯的收益。

- **分工合作**：早期層的設計是編碼輸入信息並捕捉複雜模式和依賴性。最終層的目的是將這些編碼信息轉換為下一個 token 的概率分佈。溫度縮放自然地適合這個調整概率分佈的最終步驟。

### 總結

實際上，溫度調整僅應用於在 softmax 函數之前由最後一層產生的 logits。這允許直接控制模型輸出的隨機性和多樣性，而不干擾 transformer 層的內部編碼和處理。
