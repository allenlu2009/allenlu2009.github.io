---
title: Graph RAG
date: 2023-11-29 23:10:08
categories:
- Language
tags: [GPT, LLM, HuggingFace, prompt]
typora-root-url: ../../allenlu2009.github.io


---





## Source

* What is Retrieval Augmented Generation (RAG)  https://www.youtube.com/watch?v=T-D1OfcDW1M
* [Advanced RAG Techniques: an Illustrated Overview | by IVAN ILIN | Dec, 2023 | Towards AI](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)

* 百川


## 引言

RAG 整合到 LLM 可以改善困惑度、事實準確性、下游任務準確性，和 in-context learning。結合獨立的檢索器，獨立的 RAG + LLM已被廣泛應用於處理具有長文檔問答。在先前的研究中，語言模型已在推論、微調, 和預訓練時進行了檢索增強。也有一些方法嘗試將 LLM 和 RAG 集成到單一模型中並建立端到端的解決方案。本文主要討論最常用的獨立 RAG + LLM 形式。

## 概要

圖形和檢索增強生成（RAG）。圖形和檢索增強生成（RAG）是自然語言處理（NLP）中兩個重要的技術，但它們可以結合在一起。當結合時，圖形和RAG可以通過同時整合結構化數據和非結構化文本來增強NLP系統的功能。這種整合可以實現更全面和準確的信息檢索，以及生成更相關和連貫的回答。通過利用圖形的力量，它以結構化格式表示實體之間的關係，並利用RAG從非結構化文本來源中檢索相關信息，NLP系統可以為用戶提供更豐富和個性化的上下文體驗。


Graph和RAG（檢索增強生成）。Graph和RAG（檢索增強生成）是自然語言處理（NLP）中兩個重要的技術，但它們可以結合在一起。當結合時，Graph和RAG可以通過將結構化數據和非結構化文本相結合來增強NLP系統的能力。這種整合允許更全面和準確的信息檢索，以及生成更相關和連貫的回答。通過利用Graph的力量，該格式表示了結構化格式中實體之間的關係，以及RAG從非結構化文本源中提取相關信息，NLP系統可以為用戶提供更豐富上下文並個性化的體驗。


圖形和RAG（檢索增強生成）是自然語言處理（NLP）中兩種重要的技術。但它們可以結合在一起。當結合時，圖形和RAG可以通過將結構化數據和非結構化文本相結合來增強NLP系統的功能。


Summary: 
      
In this study, we explored the integration of Graph Neural Networks (GNNs) with Retrieval-Augmented Generation (RAG) models for improved performance in natural language processing tasks. 
LLM 是唬爛大師. 他總是會說一些荒謬的話，讓人難以置信。無論是在工作上還是生活中，他都能輕易地說服別人相信他的謊言。有時候，他的唬爛甚至會讓人感到傷心或困惑。儘管如此，他卻總是能夠避免承擔任何後果，讓人難以理解他如何能夠如此得心應手地操控局面。無論如何，LLM 總是會持續不斷地唬爛下去，讓人難以預料接下來會發生什麼事情。.  他的唬爛技巧簡直就是無人能及，每次都能輕易地搞定任何困難。有時候，他的謊言甚至會讓人忘記了他究竟是真心還是假意。不過，無論怎樣，LLM 總是能夠掌控場面，讓人難以捉摸他接下來的舉動。或許這就是他的魅力所在吧，讓人愛恨交織，卻又難以抗拒。.
// 這是一句// 
/

* LLM 是唬爛 (hallucination) 大師，自己非常相信 generate 出來的東西。有兩個問題 : (1) 沒有信源; (2) LLM training dateset 可能 out-of-date.
* 一個方法是 RAG, 就是在 prompt 給 LLM 之前，先 query credible source (e.g. NASA for space related question, NY times for latest news), 再把 retrieval information augment to prompt, 再送給 LLM.   此時有兩個好處 (1) 有 credible 信源頭; (2)  information 是 up-to-date.
* 這和 AutoGPT 有什麼差異?  AutoGPT 是 LLM 決定是否要 (或者不要) search internet, 以及 search 那些 website.  比較不確定，也無法保證 search 結果是否正確或是 up-to-date。   
* 這和 cloud-edge collaborative approach 有什麼關係?  三種模式：
  * Cloud large LLM, Edge small LLM: prompt to both large LLM and small LLM.  以 cloud LLM 為 golden.  Edge LLM 只是快但不一定正確 answer.  可以用 cloud answer 再修正或是 train edge LLM
  * Cloud large LLM, Edge small LLM: prompt to the small LLM first.  Small LLM 決定是否要 pass to cloud LLM.  好處是: 快 and cost saving, 也有機會 train edge LLM.  但是 edge LLM 自己是唬爛大師，不知道自己不知道 (don't know what don't know).  可能小錯 (cloud LLM) 加大錯 (edge LLM)
  * Cloud credible source or database, edge small LLM with RAG.   好處是:  準確, 如果網路快，query credible source 可能又快又省，不用 cloud LLM.   Edge LLM 的準確度可能大增！




|              | LLM only (cloud or edge)                                     | Cloud+Edge LLM<br>post arbitration                           | Cloud+Edge LLM<br/>pre arbitration             | Cloud info+<br>edge LLM RAG | Cloud info+<br>edge AutoGPT |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------- | --------------------------- | --------------------------- |
| Accuracy     | 1. Based of pre-trained knowledge, <br>could be out-of-date. <br>2. Hallucination without credible source | 1. Based of pre-trained knowledge, <br/>could be out-of-date. <br/>2. Hallucination without credible source | Worst. <br>Edge LLM error<br>+ cloud LLM error | High                        | Med-High                    |
| Cost         | Edge:low;  Cloud:high                                        | High                                                         | Medium                                         | Low                         | Med                         |
| Latency      | Edge: fast; Cloud: slow                                      | Fast                                                         | Fast                                           | Slow?                       | Slow                        |
| Self-improve | You don't know you don't know.<br>Ceiling: cloud LLM         | Yes, use cloud LLM<br>fine-tune edge LLM                     | Maybe                                          | Yes                         | Maybe                       |



|      | 僅LLM（雲端或邊緣）                       | 雲端+邊緣LLM<br>後仲裁                    | 雲端+邊緣LLM<br/>前仲裁        | 雲端資訊+<br>邊緣LLM RAG | 雲端資訊+<br>邊緣AutoGPT |
| ---- | --------------------------------- | ---------------------------------- | ----------------------- | ------------------ | ------------------ |
| 準確度  | 1. 基於預訓練知識，可能已過時。<br>2. 幻覺缺乏可信源泉。 | 1. 基於預訓練知識，可能已過時。<br/>2. 幻覺缺乏可信源泉。 | 最差。邊緣LLM錯誤<br>+ 雲端LLM錯誤 | 高                  | 中高                 |
| 成本   | 邊緣: 低;  雲端:高                      | 高                                  | 中等                      | 低                  | 中                  |
| 延遲   | 邊緣: 快;  雲端: 慢                     | 快                                  | 快                       | 慢？                 | 慢                  |
| 自我改進 | 不知道自己不知道。<br>上限：雲端LLM             | 是的，使用雲端LLM<br>微調邊緣LLM              | 可能                      | 是                  | 可能                 |





### RAG

**Vanilla RAG case** in brief looks the following way: you split your texts into chunks, then you embed these chunks into vectors with some Transformer Encoder model, you put all those vectors into an index and finally you create a prompt for an LLM that tells the model to answers user’s query given the context we found on the search step.
In the runtime we vectorise user’s query with the same Encoder model and then execute search of this query vector against the index, find the top-k results, retrieve the corresponding text chunks from our database and feed them into the LLM prompt as context.



<img src="/media/image-20231222085752666.png" alt="image-20231222085752666" style="zoom:67%;" />

The prompt can look like:

```python
def question_answering(context, query):
    prompt = f"""
                Give the answer to the user query delimited by triple backticks ```{query}```\
                using the information given in context delimited by triple backticks ```{context}```.\
                If there is no relevant information in the provided context, try to answer yourself, 
                but tell user that you did not have any relevant context to base your answer on.
                Be concise and output the answer of size less than 80 tokens.
                """

    response = get_completion(instruction, prompt, model="gpt-3.5-turbo")
    answer = response.choices[0].message["content"]
    return answer
```



Advanced RAG

<img src="/media/image-20231222092039359.png" alt="image-20231222092039359" style="zoom:67%;" />
