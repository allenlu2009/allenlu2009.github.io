---
title: LLM 趨勢
date: 2024-08-03 23:10:08
categories:
  - Language
tags:
  - Graph
  - Laplacian
typora-root-url: ../../allenlu2009.github.io
---

## 引言

先説 LLM 趨勢：(1) 模型更大 (> trillion), (2) 上下文長度更長 (>million), (3) 推論成本更低 (每1M tokens <$1), (4) 多模態, (5) 代理。

我們知道人工智能的ABC，即 A. 算法 、B. 大數據和 C. 計算能力。對於 LLM 使用 transformer 模型，有一個簡單的公式：
	計算能力, C $\approx \alpha$ x Transformer模型大小, A  x  數據大小, B
	$\alpha$ 是一個常數。對於推理，$\alpha=2$，對於訓練，$\alpha=6$

舉例而言，Llama3.1-405B 使用 15T data size,  對應的算力是 $6 \times 0.4T \times 15T = 36\times 10^{24} = 3.6 \times 10^{25}$ 

GPU 芯片的算力每兩年提高2.25倍，或每年提高1.5倍。
![[Pasted image 20240804003108.png]]


## Why LLM Need a Bigger Model Size?

LLM is a data swallow monster.  Based on the **scaling law**, LLM ingest and process vast amounts of data can capture more high dimensional information from lanuage as well as different modality from voice, image, or video, resulting in more accurate predictions and insights.

Therefore, the simplest reason is **better accuracy** in **wide coverage**.  

## LLM Benchmark

There are two types of benchmarks, structured benchmark is like taking exams with defined questions and correct answers, such as MMLU - comprehensive knowledge Q&A, GSM8K - math.   Human evaluation benchmark is random question and answer based on subjective evaluation without defined questions, such as lmsys arena.   

A comparison of the two types of benchmarks for evaluating large language models (LLMs): 

| **Aspect**                | **Structured Benchmarks**                          | **Human Evaluation Benchmarks**                   |
|---------------------------|---------------------------------------------------|---------------------------------------------------|
| **Definition**            | Benchmarks with defined questions and correct answers. | Benchmarks based on subjective human evaluations of model outputs. |
| **Examples**              | MMLU (Massive Multitask Language Understanding), GSM8K (math problems). | LMSYS Arena, human judgment tasks.                |
| **Question Type**         | Fixed questions with specific answers.            | Open-ended questions or tasks evaluated by humans. |
| **Evaluation Method**     | Automated scoring based on correctness.           | Human raters assess quality, relevance, and coherence. |
| **Objective**             | Measure specific capabilities (e.g., knowledge, reasoning). | Assess overall performance, usability, and user satisfaction. |
| **Reproducibility**       | High reproducibility due to fixed questions.      | Lower reproducibility due to variability in human judgment. |
| **Scope of Evaluation**   | Focused on specific tasks or domains.             | Broader evaluation of model behavior in diverse contexts. |
| **Strengths**             | - Clear metrics and scoring. <br> - Objective evaluation. <br> - Easier to compare models quantitatively. | - Captures nuances of language and context. <br> - Reflects real-world usage and user experience. <br> - Can evaluate creativity and coherence. |
| **Limitations**           | - May not capture real-world performance. <br> - Limited to predefined tasks. <br> - Can be overly simplistic. | - Subjective and can vary between evaluators. <br> - Harder to quantify and compare results. <br> - Potential biases in human evaluation. |

#### Summary
- **Structured Benchmarks**: These are useful for assessing specific capabilities of LLMs in a controlled manner, providing clear metrics for comparison. However, they may not fully represent how models perform in real-world scenarios.
  
- **Human Evaluation Benchmarks**: These offer a more holistic view of model performance, capturing the subtleties of language and user experience. However, they introduce subjectivity and variability, making it challenging to derive consistent metrics.

Both types of benchmarks are valuable for understanding the strengths and weaknesses of LLMs, and they can complement each other in providing a comprehensive evaluation of model performance.

### MMLU (Massive Multitask Lanuage Understanding) Benchmark

**MMLU** (Massive Multitask Language Understanding) is a benchmark designed to evaluate large language models (LLMs) across a diverse set of natural language understanding tasks.

#### Key Features:

- **Task Categories**: MMLU includes **57 categories** spanning various domains, such as reading comprehension, mathematics, and common sense reasoning.
- **Dataset Size**: The benchmark consists of over **10,000 questions** derived from academic exams, standardized tests, and other educational materials.
- **Evaluation Format**: Questions are presented in a **multiple-choice format**, allowing for automated scoring.
- **Generalization Focus**: Assesses models' abilities to generalize knowledge to new and unseen tasks.
- **Human Baseline**: Provides a comparison against human performance, offering insights into how LLMs stack up against human capabilities.

#### MMLU Score Board (2020-2024):  Bigger is Better

![[Pasted image 20240804224326.png]]



<img src="/media/image-20240804224326.png" alt="image-20240804224326.png" style="zoom:50%;" />



[MMLU Benchmark (Multi-task Language Understanding) | Papers With Code](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)

#### MMLU Leaderboard (2024/7)
![[Pasted image 20240804222551.png]]

<img src="/media/image-20240804222551.png" alt="image-20240804222551.png" style="zoom:50%;" />


#### Lmsys Arena Leaderboard

Two premises: 
* Evalution (weak model, like discriminator in GAN) is easier than generation (strong model, generator)
* Relative evalution is easer than absolute evalution

Caveates
* Use ELO (distributed) to get the overall ranking
* Double blind evaluation: neither the participants nor the LLMs know who is evaluating which model.

![[Pasted image 20240804230253.png]]

<img src="/media/image-20240804230253.png" alt="image-20240804230253.png" style="zoom:50%;" />

[LMSys Chatbot Arena Leaderboard - a Hugging Face Space by lmsys](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
![[Pasted image 20240804134452.png]]

<img src="/media/image-20240804134452.png" alt="image-20240804134452.png" style="zoom:50%;" />

## Why LLM Need Longer Context?

Longer context length implies **longer (short-term) memory**.   This is important for accuracy and reduce LLM hallucination. 

Longer context has practical applications:
* **Long text summary**
* **Multi-run conversation**
* **Multimodality: image and video require longer context length**
* **RAG - Retrival-Augment-Generation**

Note extremely long context (e.g. > 1M tokens) actually enable full in-context database embedding and surpass the need of RAG.   However, the excess cost and delay of extremely long context is not a good replacement of RAG.   RAG is by-far the most efficient and practical solution for incorporating external knowledge into text generation tasks to (1) reduce the LLM halluciation; and (2) provide the citation of the source.  


## LLM Inference is cheaper  

We know the ABCs of artificial intelligence: A. Algorithms, B. Big Data, and C. Computing Power. For LLMs using transformer models, there is a simple formula:

Computing Power, C ≈ α × Size of Transformer Model, A × Size of Data, B

α is a constant. For inference, α = 2; for training, α = 6.

For example, Llama3.1-405B uses a data size of 15T, and the corresponding computing power is $6 \times 0.4T \times 15T = 36 \times 10^{24} = 3.6 \times 10^{25}$.

Assuming using H100 (2000 TOPS) $3/hour.   We can compute the cost of training a LLM and the cost of LLM inferencing.

#### Training Cost
$2M / 100B / 1T token.  
Ex:  Llama3-405B with 15T tokens:  $2M x 400B/100B x 15T/1T = $120M

#### Inferencing Cost
$1 / 100B / 1M token.  
Ex:  Llama3-405B with 1M tokens:  $1 x 400B/100B = $4 / 1M tokens

Sale price
OpenAI: GPT-4
Google: Gemini
Antropic: Caude
Baidu:



## How to Reduce the Inference Cost

It seems contradict to the bigger model size, but lower inference cost!
There are tricks:
* 3 different model size:  super size is for benchmark and the teacher models.  The big size and medium size are for student models.   The student models are distilled from the teach models to reduce the model size but keep relative close accuracy.
* Optimization to increase the GPU utilization rate.   
	* Batch size
	* Flash attention
* Better HW (TOPS / $)

![[Pasted image 20240805002138.png]]



<img src="/media/image-20240805002138.png" alt="image-20240805002138.png" style="zoom:50%;" />



![[Pasted image 20240805003337.png]]



<img src="/media/image-20240805003337.png" alt="image-20240805003337.png" style="zoom:50%;" />

[Trends in GPU Price-Performance – Epoch AI](https://epochai.org/blog/trends-in-gpu-price-performance)




### Notes:
- **SuperGLUE**: Measures natural language understanding across various tasks.
- **HumanEval**: Evaluates code generation capabilities.
- **MMLU**: Tests a model's ability to answer questions across a wide range of subjects.
- The scores for Gemini are based on early estimates and may not be fully representative of its capabilities as it continues to be evaluated.
Sure! Here is the translation of the provided text into Chinese:

**SuperGLUE**：衡量自然语言理解在各种任务中的表现。
- **HumanEval**：评估代码生成能力。
- **MMLU**：测试模型在广泛主题上回答问题的能力。
- Gemini的分数基于早期估计，可能无法完全代表其能力，因为它仍在继续评估中。
  For the most accurate and current data, please refer to the latest research publications or official reports from the developers.
  

<img src="https://pbs.twimg.com/media/Fuz4UrZaYAAE4ZS?format=jpg&name=900x900" alt="圖片" style="zoom:67%;" />

### Phase 1:  Big Pre-trained foundation model using SSL (Self-Supervised Learning)


### Phase 2:  少量 labelled data using  Supervised Fine-Tuning

​	Fine tuning 是複製



<img src="/image-20230326003003019.png" alt="image-20230326003003019" style="zoom: 67%;" />


另外 RLHF!!

![[Pasted image 20240803192928.png]]

## 第一部：Pre-trained Model



## Introduction

NLP 最有影響力的幾個觀念包含 embedding, attention (query, key, value).  之後也被其他領域使用。例如 computer vision, ....

<img src="/media/image-20230321221540623.png" alt="image-20230321221540623" style="zoom:50%;" />

Embedding 並非 transformer 開始。最早是 starting from word2vec paper.

QKV: query, key, value 這似乎最早是從 database 或是 recommendation system 而來。從 transformer paper 開始聲名大噪，廣汎用於 NLP, vision, voice, etc.



## Attention Flow and Interpretation

### A. Linear Combination Interpretation

Step 1:  **Text to Token Embedding**

* 1 個英文 word 大約對應 2 個 tokens.  這裡的 token embedding 基本是 (sub) word embedding, 而非 character 或是 sentence embedding.
* Token embedding 就是 vector.  每一個 token (sub-word) 都是一個 vector.   例如 GPT3 一次處理 512 tokens, vector 的 dimension 是 768.  也就是 token embeddings 是一個 512 x 768 的 matrix.

<img src="c:/users/allen/onedrive/allenlu2009.github.io/media/image-20230402205958430.png" alt="image-20230402205958430" style="zoom:100%;" />

Step 2:  **2nd token = attention weighted sum of “input token embedding”** 

<img src="/media/image-20230402212720463.png" alt="image-20230402212720463" style="zoom:100%;" />

* Caveat:  事實上 input 不一定是 "input token embedding".  一般更廣泛是用 value token.   Value token 是 input token embedding 的 linear projection,  如下圖：

<img src="/media/image-20230402214057090.png" alt="image-20230402214057090" style="zoom:100%;" />



Step 3:  **如何計算 attention weights?** 

* **產生 query token 和 key token.  兩者是相同 dimension.**

  <img src="/media/image-20230402214506198.png" alt="image-20230402214506198" style="zoom:100%;" />

* **計算所有 query token 和 key token 的點積 similarity**

<img src="/media/image-20230402214602717.png" alt="image-20230402214602717" style="zoom:100%;" />

* **Normalized to [0-1] weights using softmax.**

  <img src="/media/image-20230402214935387.png" alt="image-20230402214935387" style="zoom:100%;" /> 



Question?  為什麼要 Q, K, V?

* Q, K 基本是一組。但是 V 可以分開提供更多的 flexibility.  例如 cross-attention 的 V 和 Q,K 是不同的 domain (例如Q/K 是英文, V 是法文)。
* Attention weights 都是計算，並沒有 learning!!  如何加上 learning?  Multi-head attention!



Step 4:  **(Learnable) Feed-forward network** 

再做一個 feed-forward, 1 hidden layer with 4x parameters.





## B: Matrix Multiplication Rationale (Good!)

#### 先看不好的方法 for NLP

* NLP 的問題是 input text string 一般是變動的，例如：“Hello, world",  或是 "This is a test of natural language processing!"

* Input 是 text string, 切成 tokens ($\le$512).  儘量塞 sentence 或是 0 padding.  每個 token 是 768-dim (feature) vector. 也就是 (input) token embedding 是一個 arbitrary width ($\le$ 512) 2D matrix X.  最終希望做完 attention 運算還是得到同樣的 shape.
* Token size 不是固定 ($\le 512$) 有很多 implications:
  * Need to use layer norm instead of group norm! 因爲不同 sample 的長度不同。
  * 另外在 transformer for Computer vision 例如 ViT 應用：token length = 224x224/16x16 + 1 (CLS token) = 197!

* 如果 input-output 是 inter-token 和 inter-feature 的 fully-connected network, 顯然不可行！
  * 因為是一個 $(512\cdot 768)^2 = 154$ B weights，同時 computation 也要 154 T operation!
  * Input 是變動的長度, 所以固定的 154B weights 無法得到不同 width 的結果。


<img src="/media/image-20230402210446588.png" alt="image-20230402210446588" style="zoom:80%;" />

* 如果 input-output 只作用或處理在 embedding dimension (i.e shared weight for all input tokens!), 例如 1-dimension convolution, kernel 就是 1x768, channel length = 768.  假設 input 是 3-channel (e.g. RGB or KQV), parameter = 3 x 768^2 = 2M parameters.  顯然也不夠力。同時 each token 都是獨立處理，缺乏 temporal information!  

<img src="/media/image-20230402220358950.png" alt="image-20230402220358950" style="zoom:80%;" />



#### Catch

* **我們需要找一個方法介於 fully connected model and 1-d convolution network!!!**
  * Fully connect network size:  (512x768)^2 = 154B
  * 1-dimension network size: (768x768) < 1M
* 所以需要如同下面的方法！計算  $f(X) = X X^T X$.  假設 X 是 m x n -> (m x n)  (n x m) (m x n) = m x n 得到和原來一樣的 shape! 
* 此時 token 和 token 之間 interact, 但又不像 fully connected 這麼多 interaction!!!  這就是 attention 的原理！
* Rank = min(m, n)！ 如果 width 很小，例如短句。或是 attention 範圍小。rank 就小。計算量就小，也避免 overfit!
* **問題是以下的方法，沒有任何 trainable parameter!!!!**

<img src="/media/image-20230402220424409.png" alt="image-20230402220424409" style="zoom:80%;" />

#### 如何引入 Trainable Parameter?

* 如何做到？非常容易！  重組 input 引入 V (value) matrix.  引入 K, Q matrix and similarity/attention matrix!!
* 使用 K, Q 計算 similarity matrix (512x512)，then softmax for attention matrix.   V 通過 attention matrix 得到 output!
* 因為 attention matrix 的遠小於 768!  所以有類似 low rank 的功效。 
* V, K, Q 的 dimension:
  * V: 768x768,  K: 768x768,  Q: 768x768.  Total:  3 x (768x768)

<img src="/media/image-20230402220443383.png" alt="image-20230402220443383" style="zoom:80%;" />

* 最後再加上一個 MLP layer. Hidden layer 的 dimension 是 4x768!,  所以 parameter = 768^2 x 4 x 2? = 5M parameters!

<img src="/media/image-20230402220500305.png" alt="image-20230402220500305" style="zoom:80%;" />

* 一個 transformer block 的 parameter = V,K,Q x 768^2 = 3 x 768^2 = 2M param + 5 M= 7.1 M parameters

<img src="/media/image-20230402220521798.png" alt="image-20230402220521798" style="zoom:80%;" />

* BERT 有 12 blocks, giving ~ 85M parameters (再加上 25M for token embedding and position encoding, 30522 x 768 + 512 x 768 = 24M)
  * BERT 的 inputs are sequences of 512 consecutive tokens!
  * BERT 使用 uncased (not case sensitive) vocabulary size of 30522!   GPT 是 case sensitive unicode of 50257.
  * **BERT 參數大約是 85M (12 block transformer) + 24M (embedding+position encode) = 110M**
  <img src="/media/image-20230408144157907.png" alt="image-20230408144157907" style="zoom:67%;" />
  
* GPT2 : 以下是 Jay Alammar 對 GPT2 的參數估算，似乎一樣??? (2304 = 768x3 KQV,  3072=768x4 MLP)

  * GPT2 的 inputs are sequences of 1024 consecutive tokens!
  * GPT2 參數大約是 85M (12 block transformer) + 40M (embedding+position encode) = 125MB!!
  * 爲什麽 token embedding 如此大, 38.6M?  不是因爲 token length 太長！而是支持 byte-level version of  Byte Pair Encoding (BPE) for unicode characters and a **vocabulary size of 50257**.

* ![image-20230408113653936](/media/image-20230408113653936.png)

* **Token length 的影響**

  * **BERT 的 max token length = 512;  GPT2 的 max token length = 1024** 

  * **注意每個 transformer block 的 7.1M parameters 數目和 token length 完全無關 (無論是 512 or 1024)!!!!**

  * **注意 token embedding 的參數大小和支持的字符集 (vocabulary) 有関 (BERT:32K or GPT:50K)，和 token length 也無関！！！**只有 position encode 的參數大小和 token length 有関 (512x768 or 1024x768) 不過對參數量的影響很小。

  * **Token length 到底和什麽有関?  transformer 内部計算的中間 result 會有 512 (or 1024)x768 matrix, 所以直接和計算量 (TOPS) 以及内存大小有關。但是和參數無關。**、

    


* **除了 token length 不同, BERT 和 GPT2 training 方式也不同： BERT 使用 masked token.   GPT 使用 predicted token.  這也反應在他們的架構上:  BERT 是 transformer encoder;  GPT 是 transformer decoder.**

<img src="/media/image-20230402220542789.png" alt="image-20230402220542789" style="zoom:80%;" />



* ViT 原理和 BERT 一樣,  都是 transformer encoder！

* 1-patch = 16x16x3 = 768 dimension.  224x224 image, 一共有 196 patches + 1 classification token = 197 tokens (<512 token).

  <img src="/media/image-20230402220606024.png" alt="image-20230402220606024" style="zoom:80%;" />

* ViT transformer 部分參數量和 BERT 一樣都是 85M。不過 embedding project 部分不同：

  *  BERT embedding+position encoding: 30522 x 768 + 512 x 768 = 24M

  * ViT 是把 16x16(pixel)x3(RGB) = 768 重新 map 到 768 dimension, 所以只有 768x768+197x768=741120=0.75M.  所以 ViT 的全部參數基本就是 85+0.75~86M!

* ViT 另一個簡化是使用 1-convolution 可以 work!!  也就是 shared weight!

  

<img src="/media/image-20230402220640057.png" alt="image-20230402220640057" style="zoom:80%;" />

* VIT 的大小：

<img src="/media/image-20230408142000179.png" alt="image-20230408142000179" style="zoom:67%;" />



### C: Information Retrieval Interpretation

Query (search) and Key (Title)

<img src="/media/image-20230403003717886.png" alt="image-20230403003717886" style="zoom:67%;" />



Once match, then value (content)

<img src="/media/image-20230403003758142.png" alt="image-20230403003758142" style="zoom:67%;" />



Multi-head attention

<img src="/media/image-20230403003904323.png" alt="image-20230403003904323" style="zoom:80%;" />
