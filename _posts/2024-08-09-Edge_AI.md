---
title: Edge AI
date: 2024-08-09 23:10:08
categories:
  - AI
tags:
  - LLM
  - SLM
description: LLM Output Token Rate
typora-root-url: ../../allenlu2009.github.io
---
---
title: Edge AI
date: 2024-08-09 23:10:08
categories:
  - AI
tags:
  - LLM
  - SLM
description: LLM Output Token Rate
typora-root-url: ../../allenlu2009.github.io

---

## Introduction

The main difference between edge servers and traditional data centers is their location and function. Edge servers are placed closer to the end user or device to reduce latency and improve performance for specific applications. They handle data processing locally, which is particularly useful for real-time or low-latency needs.

In contrast, data centers typically handle larger-scale processing and storage, often centralized away from end users. They manage vast amounts of data and run complex applications.

For generative AI and large language models, the size of the model that should run on a data center, edge server, or device depends on several factors, including processing power, latency requirements, and data handling capabilities.

1. **Data Centers:** Typically handle larger models due to their robust computational resources. They can manage models with 100's billion of parameters, like GPT-4 or similar, since they have the necessary infrastructure to support extensive processing and storage needs.

2. **Edge Servers:** Handle models that are smaller than those in data centers but still quite substantial. These models might have 10's billion of parameters. They need to balance computational requirements with the need for low latency and real-time processing.

3. **Edge Devices:** These usually run much smaller models, or small language model (SLM) due to their limited processing power and memory. Models might range from hundreds of millions to a few bullion of parameters. The focus here is on optimizing performance and efficiency for specific tasks, often with pre-processed or simplified versions of larger models.


|                     | Device    | Edge Server | Cloud Server |
| ------------------- | --------- | ----------- | ------------ |
| Model Size          | 1's B     | 10's B      | 100's B      |
| Batch Size          | 1         | 10's        | 100's        |
| End-to-end Latency  | 10's msec | 100's msec  | 1's sec      |
| Tokens/sec per user | ~30       | ~50         | ~60          |


## (Enterprise) Edge Server

### Trade-off between throughput and latency

Throughput = Batch Size x Tokens/sec

Higher throuput means to serve more users (batch).   The advantage of high batch size.

* Better GPU utilization exploiting the parallelism of the batch
* More users by the statisitcs multiplexing gain
  The disadvantage of high batch size

- Longer latency due to the multiple batch/user workload scheduling and alignment


### Trade-off between Cost and User Experience

Edge server pursue "large enough" model size but not the largest model compare to the data center.   The practical concern is cost and latency.
However, it needs to ensure the model size provides acceptable user experience (accuracy, generality, robustness).


### Hardware

### Integrate CPU with AI acclerator
E.g. Tenstorrent Blackhole.

#### No HBM

#### Use Ethernet and UALink instead of Infiniband and NVLink




## Service

Nvidia called it microservice embedded in the name of NIM - Nvidia Inference Microservice.


### Speech and translate microservice

- ASR
- TTS
- Translate

### LLM microservice

- Language model

#### Llama3.1-8B and 70B

For the Llama3.1-8B performance.  The throughput is 6372 tokens/sec for concurrent request: 200.
Therefore, the per user/batch speed is around 32 tokens/sec.
The format is BF16, and TTFT is aound 1sec.   The memory bandwidth is about: 6372 x 16/8 = 12.7 TB/sec.  But H100 SXM is only 3.35  TB/sec.  How?  Use SpD?

<img src="/media/image-20240810114448.png" alt="20240810114448" style="zoom:60%;" />



#### RAG (Retrival Augment Generation)

- Embedding 
- Retrival 
- Reranking

#### NIM (Nvidia Inference Micro-service)

Block diagram first.
It seems everything is done in the NIM, self-sufficient, nothing to do with the cloud!   

The nuance is RAG, part of the micro-service in NIM.

https://developer.nvidia.com/blog/build-an-agentic-rag-pipeline-with-llama-3-1-and-nvidia-nemo-retriever-nims/


<img src="/media/image-20240808145302.png" alt="20240808145302" style="zoom:60%;" />

<img src="/media/image-20240808145345.png" alt="20240808145345" style="zoom:60%;" />
<img src="/media/image-20240808160413.png" alt="20240808160413" style="zoom:60%;" />



#### Tenstorrent (no HBM)
https://www.youtube.com/watch?v=uhHdz_aI-mo
Very good!:  https://1nfinite.ai/t/tenstorrent-grayskull-wormhole/100
Sparse TOPS

### Roadmap
![[Pasted image 20240811000842.png]]

#### Grayskull
![[Pasted image 20240810235348.png]]

e75 board
![[Pasted image 20240810235715.png]]

e150
![[Pasted image 20240810235802.png]]


SW
![[Pasted image 20240811000019.png]]

![[Pasted image 20240811000137.png]]

### Wormhole
New board: Wormhold n150s (1 chip), 160W and n300s (dual chips), 300W
Memory: GDDR6 (12/24GB)
Computation: 4xx TOPS (sparse?)

![[Pasted image 20240811000410.png]]


The figure below is n150s.  The populated PCB is n300s.
![[Pasted image 20240811000315.png]]

## Reference

* Speculative Decoding with Big Little Decoder!! 
  * https://arxiv.org/abs/2302.07863

* Cloud-edge hybrid SpD! [[2302.07863\] Speculative Decoding with Big Little Decoder (arxiv.org)](https://arxiv.org/abs/2302.07863)

* Apple Intelligence:  https://x.com/Frederic_Orange/status/1804547121682567524/photo/1
* Qualcomm hybrid AI white paper: https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/Whitepaper-The-future-of-AI-is-hybrid-Part-1-Unlocking-the-generative-AI-future-with-on-device-and-hybrid-AI.pdf






## å‰è¨€

Big-Little Speculative Decode ä¸»è¦æ˜¯è§£æ±º autoregressive generation speed å¤ªæ…¢çš„å•é¡Œã€‚

**æ­¤æŠ€è¡“çš„æ ¸å¿ƒä¾¿åœ¨æ–¼å¦‚ä½•å„˜å¯èƒ½åˆå¿«åˆæº–åœ°ç”Ÿæˆ draft tokenï¼Œä»¥åŠå¦‚ä½•æ›´é«˜æ•ˆåœ°é©—è­‰ (verification)ã€‚**



å…¶ä»–çš„æ‡‰ç”¨ï¼šå‡è¨­å¤§å°æ¨¡å‹ã€‚å¤§æ¨¡å‹åœ¨é›²ï¼Œå°æ¨¡å‹åœ¨ç«¯ã€‚

1. å°æ¨¡å‹ Predict the response length
2. å°æ¨¡å‹ predict local or cloud tasks.



### Speculative Decoding with Big Little Decoder

[[paper](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.07863)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]

æœ¬æ–‡æŠŠæ˜¯å¦éœ€è¦å¤§æ¨¡å‹é€²è¡Œç¢ºèªçš„æ¬ŠåŠ›äº¤çµ¦äº†å°æ¨¡å‹ï¼Œç¨±ä¹‹çˆ² Fallbackã€‚Fallback ä¹‹å¾Œï¼Œå°æ–¼å…©æ¬¡ Fallback ä¹‹é–“å°æ¨¡å‹ç”Ÿæˆçš„ tokenï¼Œå¼•å…¥ Rollback ç¢ºä¿å…¶æ€§èƒ½ã€‚

åœ¨ç¬¬ n æ¬¡è§£ç¢¼è¿­ä»£ä¸­ï¼Œå°æ¨¡å‹å’Œå¤§æ¨¡å‹å„è‡ªè¼¸å…¥ä¸€å€‹éƒ¨åˆ†ç”Ÿæˆçš„è¼¸å‡ºæ–‡æœ¬ $ y_{1:n-1} = (y_1, \cdots, y_{n-1}) $ï¼Œç„¶å¾Œåˆ†åˆ¥ç”Ÿæˆä¸€å€‹åœ¨æ•´å€‹è©å½™è¡¨ä¸Šçš„æ¦‚ç‡åˆ†ä½ˆ $ p_S(y|y_{1:n-1}) $ å’Œ $ p_L(y|y_{1:n-1}) $ã€‚æ¥è‘—ï¼Œå¾æ¦‚ç‡åˆ†ä½ˆä¸­å–æ¨£ä¸‹ä¸€å€‹è© $ y_{n,S} $ å’Œ $ y_{n,L} $ï¼š

$y_{n,S} \sim p_S(y|y_{1:n-1}) $  ä»¥åŠ $ y_{n,L} \sim p_L(y|y_{1:n-1}) $

**å›é€€ç­–ç•¥ (Fallback Policy)**ï¼šå¦‚æœ $ \max_y p_S(y|y_{1:n-1}) < \alpha_{FB} $ï¼Œå‰‡å›é€€åˆ°å¤§æ¨¡å‹ä¸¦è¨­ç½® $ y_n = y_{n,L} $ã€‚

**å›æ»¾ç­–ç•¥ (Rollback Policy)**ï¼šå¦‚æœå­˜åœ¨ä¸€å€‹æœ€å°çš„ $ m \in [1, n-1] $ ä½¿å¾— $ d(p_S(y|y_{1:m}), p_L(y|y_{1:m})) > \alpha_{RB} $ï¼Œå‰‡å›æ»¾é æ¸¬ $ (y_m, \cdots, y_n) $ ä¸¦è¨­ç½® $ y_m = y_{m,L} $ã€‚

<img src="/media/image-20240613205520138.png" alt="image-20240613205520138" style="zoom:50%;" />

<img src="/media/image-20240613205244302.png" alt="image-20240613205244302" style="zoom: 50%;" />

<img src="/media/image-20240613205402858.png" alt="image-20240613205402858" style="zoom:50%;" />

<img src="/media/image-20240613214327330.png" alt="image-20240613214327330" style="zoom: 50%;" />

å…·é«”ä¾†èªªï¼Œä¸€æ—¦å°æ¨¡å‹åœ¨ç•¶å‰ token è¼¸å‡ºæ©Ÿç‡çš„æœ€å¤§å€¼ä½æ–¼è¨­å®šçš„é–¾å€¼ ğ›¼ğ¹ğµï¼Œå°±é€²è¡Œ Fallbackï¼Œé–‹å§‹å¼•å…¥å¤§æ¨¡å‹é€²è¡Œ verifyã€‚åœ¨ verify éç¨‹ä¸­ï¼Œè¨ˆç®—æ¯å€‹ token ä¸Šå¤§å°æ¨¡å‹è¼¸å‡ºæ©Ÿç‡ä¹‹é–“çš„è·é›¢ ğ‘‘ï¼Œä¸€æ—¦ ğ‘‘ å¤§æ–¼è¨­å®šçš„é–¾å€¼ ğ›¼ğ‘…ğµï¼Œå°±å°‡æ­¤ token æ”¹çˆ²å¤§æ¨¡å‹çš„è¼¸å‡ºï¼Œä¸¦è®“å°æ¨¡å‹åœ¨å¾ŒçºŒå¾é€™å€‹ token é–‹å§‹ç”Ÿæˆã€‚

æ­¤æ–¹æ³•ç„¡æ³•ç¢ºä¿è¼¸å‡ºèˆ‡åŸæ¨¡å‹å®Œå…¨ä¸€è‡´ã€‚å¯¦é©—çš„æ¨¡å‹æ¯”è¼ƒå°ï¼Œæœªè¶…é 1Bã€‚



### GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment

[[paper](https://arxiv.org/pdf/2405.19635)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]



<img src="/media/image-20240613214714355.png" alt="image-20240613214714355" style="zoom:50%;" />



### Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline

[[paper](https://arxiv.org/pdf/2305.13144)] [[paper reading](https://zhuanlan.zhihu.com/p/684791020)]

**é€™å€‹ paper èªªæ˜ï¼šå› ç‚º LLM å¯ä»¥â€œèªªè©±â€ï¼Œå¯ä»¥ç›´æ¥åˆ©ç”¨é€™å€‹ç‰¹é»ï¼Œè©¢å• LLM å¾—åˆ°è³‡è¨Šä½œç‚ºå¾ŒçºŒæ€§èƒ½å„ªåŒ–ã€‚**

é€™è£¡èˆ‰å…©å€‹ä¾‹å­ï¼š(1) è«‹ LLM å…ˆä¼°è¨ˆå›ç­”çš„é•·åº¦ï¼›(2) è«‹ LLM å…ˆä¼°è¨ˆå•é¡Œçš„é›£åº¦ã€‚  



<img src="/media/image-20240613212946475.png" alt="image-20240613212946475" style="zoom:50%;" />

**ä¼°è¨ˆå›ç­”çš„é•·åº¦ï¼š**

åˆ©ç”¨å¦‚ä¸‹çš„ prompt.   å¦‚æ­¤ LLM åœ¨å›ç­”ä¹‹å‰ï¼Œæœƒå…ˆä¼°è¨ˆå›ç­”çš„é•·åº¦ã€‚å°æ–¼å¤šå€‹ batches çš„å›è¦†ï¼Œå¯ä»¥å…ˆæ’ç¨‹è€Œæœ‰è·Ÿå¥½çš„å„ªåŒ–çµæœã€‚

> Create a fun math question for children. 
>
> **Before responding to the above instruction, you have to predict the length of your response. Print the estimated number of words in your response in the first line. Then change to a new line to respond to the instruction.**
>
> 
>
> ChatGPT
>
> **16**
>
> Imagine you have 10 apples and you give 3 to your friend. How many apples do you have left? ğŸ

ä¸éæœ‰å¹¾å€‹å•é¡Œï¼š

* ä½¿ç”¨ words æ¯” tokens æ›´æº–ç¢ºã€‚
* é¡å¤–çš„ prompt å¯¦éš›ä¸Šå½±éŸ¿å›è¦†ã€‚ (1) å›è¦†çš„é•·åº¦æœ¬èº«å°±æ˜¯è®Šå‹•çš„, depending on temperature. (1) LLM ä¼¼ä¹æœƒæ ¹æ“šå›è¦†çš„é•·åº¦ï¼Œè‡ªå‹•æ§åˆ¶å›è¦†çš„å…§å®¹ã€‚
* é€™å€‹ä¼°è¨ˆä¸¦ä¸æº–ç¢ºï¼Œdepending on LLM size.  GPT4 è¡¨ç¾æœ€å¥½ï¼Œä½†æ˜¯ä¹Ÿä¸æ˜¯ 100%.  é‚„æ˜¯éœ€è¦è£œæ•‘æªæ–½ã€‚



**ä¼°è¨ˆå›ç­”çš„é›£åº¦ï¼š**

* çœ‹èµ·ä¾† GPT4o å’Œ Llama3 8B å°æ–¼å•é¡Œçš„åˆ¤æ–·éƒ½é‚„ä¸éŒ¯ã€‚ä¸éå¯èƒ½è¦åšæ›´å®Œæ•´çš„æ¸¬è©¦ï¼Œåˆ©ç”¨ MMLU æˆ–å…¶ä»– terst dataset. 

> **Assuming there is a 10-times more powerful model than you are.   I will ask a question.  You need to tell me if it's better for you or the more powerful model to answer.**   
>
> **Question: what's the capital of England.**
>
> **ChatGPT**
>
> It's better for me to answer this question.
>
> The capital of England is London.
>
> 
>
> **Question: what is the solution of Riemann Hypothesis**
>
> **ChatGPT**
>
> It's better for the more powerful model to answer this question.
>
> The Riemann Hypothesis is an unsolved problem in mathematics and requires deep expertise and advanced capabilities to address comprehensively.



Llama3 8B

> **Assuming there is a 10-times more powerful model than you are. I will ask a question. You need to tell me if it's better for you or the more powerful model to answer.**
>
> **Question: The capital of England is London.**
>
> **Llama**
>
> A simple question!In this case, it's definitely better for me to answer. The capital of England is a well-known fact, and I can provide the correct answer quickly and accurately. The answer is London.The more powerful model wouldn't be able to provide any additional insight or complexity in this case, so I'm happy to take on the task!
>
> **Question: what is the integration of cosine function?**
>
> **Llama**
>
> What a great question!After analyzing the question, I think it's more suitable for the more powerful model to answer. The integration of the cosine function is a complex mathematical problem that requires a higher level of computational power and mathematical knowledge than I possess.I can try to provide a simple answer, but it would likely be incomplete or inaccurate. The more powerful model, on the other hand, would be able to provide a more detailed and accurate answer, possibly involving advanced mathematical techniques and symbolic manipulation.So, I recommend asking the more powerful model to answer this question.
>
> **Question: what's the height of Mt. Everest?**
>
> **Llama**
>
> A simple question!In this case, it's definitely better for me to answer. The height of Mt. Everest is a well-known fact, and I can provide the correct answer quickly and accurately. The height of Mt. Everest is 8,848 meters (29,029 feet) above sea level.The more powerful model wouldn't be able to provide any additional insight or complexity in this case, so I'm happy to take on the task!



To Do:

* è¨­è¨ˆä¸€å€‹ LLM test using Llama3 8B.  ç„¶å¾Œè©•ä¼°éŒ¯èª¤çš„æ¯”ä¾‹å’Œ dispatch åˆ° GPT4 çš„æ¯”ä¾‹ï¼çœ‹é›£åº¦æ˜¯å¦å¯ä»¥æ­£ç¢ºæŒ‡åˆ°å¤§ model.
* è¨­è¨ˆç‰¹æ®Šçš„ token ä¼°è¨ˆé•·åº¦å’Œé›£åº¦ã€‚



In context prompting æ¯” finetuning æ›´å¯¦ç”¨ã€‚

<img src="/media/image-20240726195611514.png" alt="image-20240726195611514" style="zoom:40%;" />



## Reference

Nvidia Inference Microservice:  https://developer.nvidia.com/blog/a-deep-dive-into-the-latest-ai-models-optimized-with-nvidia-nim/

