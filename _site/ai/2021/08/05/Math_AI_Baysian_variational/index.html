
<!doctype html>














<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="softmax,EM,Bayesian,Variational," />





  <link rel="alternate" href="/atom.xml" title="NexT" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?v=5.1.1" />
















<meta name="description" content="">
<meta name="keywords" content="softmax, EM, Bayesian, Variational">
<meta property="og:type" content="article">
<meta property="og:title" content="Math AI - From EM to Variational Bayesian Inference">
<meta property="og:url" content="http://localhost:4000/ai/2021/08/05/Math_AI_Baysian_variational/">
<meta property="og:site_name" content="NexT">
<meta property="og:description" content="">
<meta property="og:locale" content="en">
<meta property="og:image" content="/media/16286850167880.jpg">
<meta property="og:image" content="/media/16286850351205.jpg">
<meta property="og:image" content="/media/16287874512211.jpg">
<meta property="og:image" content="/media/16285137362672.jpg">
<meta property="og:image" content="/media/16285679550223.jpg">
<meta property="og:image" content="/media/16285916007272.jpg">
<meta property="og:image" content="/media/16286002562443.jpg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Math AI - From EM to Variational Bayesian Inference">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="/media/16286850167880.jpg">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://localhost:4000/"/>





  <title>Math AI - From EM to Variational Bayesian Inference | NexT</title>
  
















</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NexT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

<div id="posts" class="posts-expand">
  
  

  

  
  
  

  <article class="post post-type- " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://localhost:4000/ai/2021/08/05/Math_AI_Baysian_variational/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Allen Lu (from John Doe)">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="assets/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NexT">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
          
          
            Math AI - From EM to Variational Bayesian Inference
          
        </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-08-05T16:29:08+08:00">
                2021-08-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/#/AI" itemprop="url" rel="index">
                    <span itemprop="name">AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          
            
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
  
  












  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<h2 id="major-reference">Major Reference</h2>
<ul>
  <li>[@poczosCllusteringEM2015]</li>
  <li>[@matasExpectationMaximization2018] good reference</li>
  <li>[@choyExpectationMaximization2017]</li>
  <li>[@tzikasVariationalApproximation2008] excellent introductory paper</li>
</ul>

<h2 id="em-algorithm">EM Algorithm</h2>
<p>EM 可以視為 MLE 的 extension to hidden state / data.</p>

<p>Let’s start with EM algorithm</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\ln p(\mathbf{x} ; \boldsymbol{\theta})&=F(q, \boldsymbol{\theta})+K L(q \| p) \\
F(q, \boldsymbol{\theta})&=\int q(\mathbf{z}) \ln \left(\frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}\right) d \mathbf{z} \\
\mathrm{KL}(q \| p)&=-\int q(\mathbf{z}) \ln \left(\frac{p(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta})}{q(\mathbf{z})}\right) d \mathbf{z}
\end{align} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right) &=\int p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{\text {OLD }}\right) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) d \mathbf{z} \nonumber\\
&=\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{0 \mathrm{LD}}\right)} \label{eqQ}
\end{align} %]]></script>

<p>此時可以用 $\eqref{eqQ}$ 定義 EM algorithm</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{E-step : Compute}\quad &p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{\mathrm{OLD}}\right) \label{eqE}\\
\text{M-step : Evaluate}\quad &\boldsymbol{\theta}^{\mathrm{NEW}}=\underset{\boldsymbol{\theta}}{\arg \max } Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right) \label{eqM}
\end{align} %]]></script>

<p>一般 $\eqref{eqQ}$ 的 joint distribution $p\left(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}\right)$ 包含完整的 data，容易計算或有 analytical solution.
大多的問題是 $\eqref{eqE}$ conditional or posterior distribution 是否容易計算，是否有 analytical solution.</p>

<h2 id="variational-em-framework">Variational EM Framework</h2>

<p>最簡單的話就是 hidden variable z = z1, z2, .., zM.  and p(z) = p(z1)…p(zM).
什麼時候會有這種 distribution product?  後面會說明。</p>

<script type="math/tex; mode=display">\begin{equation}
q(\mathbf{z})=\prod_{i=1}^{M} q_{i}\left(z_{i}\right) \label{eqFactor}
\end{equation}</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
F(q, \boldsymbol{\theta})=& \int \prod_{i} q_{i}\left[\ln p (\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})-\sum_{i} \ln q_{i}\right] d \mathbf{z}\nonumber\\
=& \int \prod_{i} q_{i} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \prod_{i} d z_{i} - \sum_{i} \int \prod_{j} q_{j} \ln q_{i} d z_{i} \nonumber\\
=& \int q_{j}\left[\int \ln p (\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \prod_{i \neq j}\left(q_{i} d z_{i}\right)\right] d z_{j} -\int q_{j} \ln q_{j} d z_{j}-\sum_{i \neq j} \int q_{i} \ln q_{i} d z_{i} \nonumber\\
=& \int q_{j} \ln \tilde{p} (\mathbf{x}, z_{j} ; \boldsymbol{\theta}) d z_{i}-\int q_{j} \ln q_{j} d z_{j} -\sum_{i \neq j} \int q_{i} \ln q_{i} d z_{i} \nonumber\\
=&-\mathrm{KL}\left(q_{j} \| \tilde{p}\right)-\sum_{i \neq j} \int q_{i} \ln q_{i} d z \label{eqVarELBO}
\end{align} %]]></script>

<p>where</p>

<script type="math/tex; mode=display">\begin{equation}
\ln \tilde{p}\left(\mathbf{x}, z_{j} ; \boldsymbol{\theta}\right)=\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{i \neq j}=\int \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \prod_{i \neq j}\left(q_{i} d z_{i}\right) \label{eqVarJ}
\end{equation}</script>

<p>$\eqref{eqVarELBO}$ 是 (variational, 因為有 KL divergence) lower bound, KL divergence 必大於 0, 負號後必小於 0.  第二項加上負號是 self-entropy 必大於 0.<br />
直觀看出讓 KL 為 0，就是 $q_j(z_j) = \tilde{p}(x, z_j; \theta)$, 似乎就是最大值 (how about the self-entropy?).
也就是 optimal distribution $q_j^* (z_j)$ 是</p>

<script type="math/tex; mode=display">\ln q_j^* \left(z_{j}\right)=\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{i \neq j} + \text{const.}</script>

<p>上面的 const 可以由 distribution normalization 得到。所以我們可以得到一組 consistency conditions $\eqref{eqVarJ2}$ for the maximum of variational lower bound subject to $\eqref{eqFactor}$.</p>

<script type="math/tex; mode=display">\begin{equation}
q_{j}^{*}\left(z_{j}\right)=\frac{\exp \left(\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{i \neq j}\right)}{\int \exp \left(\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{i \neq j}\right) d z_{j}} \quad\text{for}\,\, j=1,\cdots,M \label{eqVarJ2}
\end{equation}</script>

<p>$\eqref{eqVarJ2}$ 顯然不會有 explicit solution, 因為 $q_j$ factors 之間是相互 dependent.  A consistent solution 需要 cycling through these factors.  我們定義 Variational EM algorithm</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\text{Variational E-step : Evaluate}\quad &q^{\mathrm{NEW}}(\mathbf{z})\quad\text{using above equations}\\
\text{Variational M-step : Find}\quad &\boldsymbol{\theta}^{\mathrm{NEW}}=\underset{\boldsymbol{\theta}}{\arg \max } F\left(q^{\mathrm{NEW}}, \boldsymbol{\theta}\right) \label{eqM2}
\end{aligned} %]]></script>

<h2 id="examples">Examples</h2>

<h3 id="例一-linear-regression-filterestimate-a-noisy-signal">例一： Linear Regression (filter/estimate a noisy signal)</h3>
<p>我很喜歡這個例子。從簡單的 least-square error filter 進步到 Kalman filter.  類似的應用：deconvolution/equalization, channel estimation, speech recognition, frequency estimation, time series prediction, and
system identification.</p>

<h4 id="問題描述">問題描述</h4>
<p>考慮一個未知信號 $y(x) \in R, x \in \Omega ⊆ R^N$, i.e. $R^N \to R$. 
我們想要 predict its value $t_* = y(x_<em>)$ at an arbitrary location $x_</em> \in \Omega$.</p>

<p>我們用 vector 表示 $(t_1, \cdots, t_N)$
 using a vector t = (t1,…, tN)T of N noisy observations tn = y(xn) + εn, at locations x = (x1,…, xN)T, xn ∈ , n = 1,…, N. The additive noise εn is commonly assumed to be independent, zeromean, Gaussian distributed:</p>

<script type="math/tex; mode=display">y(\mathbf{x})=\sum_{m=1}^{M} \omega_{m} \phi_{m}(\mathbf{x})</script>

<p>注意 $y(x)$ 不是真正的 observables, 而是加上 noise 之後的 t 才是 observations.  我們的目標就是用 $\mathbf{t}$ 來 estimate $\mathbf{w}$.</p>

<script type="math/tex; mode=display">\mathbf{t}=\boldsymbol{\Phi} \mathbf{w}+\boldsymbol{\varepsilon}</script>

<p>The likelihood function</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p(\mathbf{t} ; \mathbf{w}, \beta)&=N\left(\mathbf{t} \mid \mathbf{\Phi} \mathbf{w}, \beta^{-1} \mathbf{I}\right)\\
&=(2 \pi)^{-\frac{N}{2}} \beta^{\frac{N}{2}} \exp \left(-\frac{\beta}{2}\|\mathbf{t}-\Phi \mathbf{w}\|^{2}\right)
\end{aligned} %]]></script>

<h4 id="三種解法圖式">三種解法圖式</h4>

<p>以下我們用三種 methodologies 用 $\mathbf{t}$ 來 estimate $\mathbf{w}$ (i.e. signal) and $\beta$ (i.e. noise if needed).</p>

<p><em>Method 1:</em> ML Estimation 
如果 number of parameters (w) is the same as the number of observations (t), the ML estimates are very sensitive to the model noise.  我們可以用 DAG (Directed Acyclic Graphic) 說明，如下圖 (a).  雙圓框 t 代表 observed random variable. 方框 (W, beta) 代表 parameter to be estimated.  單圓框（e.g. (b) W）代表 hidden random variable.</p>

<p><em>Method 2:</em> 假設 weight W 是 random variable with imposed prior. 我們先用 a simple Bayesian model with stationary Gaussian prior on weight, 如下圖 (b).  以這個 model 而言，我們用 EM algorithm performs Bayesian inference.  結果 robust to noise, 類似 Kalman filter?</p>

<p><img src="/media/16286850167880.jpg" alt="-w414" /></p>

<p><em>Method 3:</em> method 2 的一個缺點是假設 stationary Gaussian noise (i.e. $\beta$, a fixed value to be estimated, 無法 capture the local signal properties.  我們可以引入更複雜 spatially/temporally varying hierarchical model which is based on a non-stationary Gaussian prior for the weight, W and a hyperprior, $\beta$, 如下圖 (c).</p>

<p>這麼複雜的 DAG 顯然無法用 EM algorithm 解，必須用本文的 “Variational EM Framework” infer values of the unknowns.</p>

<p><img src="/media/16286850351205.jpg" alt="-w245" /></p>

<h4 id="method-1-ml-for-vanilla-linear-regression">Method 1, ML for Vanilla Linear Regression</h4>

<p>始於 likelihood function</p>

<script type="math/tex; mode=display">\begin{aligned}
p(\mathbf{t} ; \mathbf{w}, \beta)=(2 \pi)^{-\frac{N}{2}} \beta^{\frac{N}{2}} \exp \left(-\frac{\beta}{2}\|\mathbf{t}-\Phi \mathbf{w}\|^{2}\right)
\end{aligned}</script>

<p>假設 $\mathbf{w}, \beta$ 為 constant parameters (to be estimated).  Maximize the likelihood or log-likelihood 等價於 minimize $|\mathbf{t}-\Phi \mathbf{w}|^{2}$.  因此**maximal likelihood (ML) estimate of w 等價 least squares (LS) estimate.</p>

<script type="math/tex; mode=display">\begin{equation}
\mathbf{w}_{L S}=\underset{w}{\arg \max } p(\mathbf{t} ; \mathbf{w}, \beta)=\underset{w}{\arg \min } E_{L S}(\mathbf{w})=\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{T} \mathbf{t} \label{eqLS}
\end{equation}</script>

<p>很多情況 $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)$ 可能是 “ill-conditioned” and difficult to invert.  意味如果 observation t 包含 noise $\varepsilon$, noise 會嚴重干擾 $\mathbf{w}_{L S}$ estimation.</p>

<h5 id="例一communication-equalizationdeconvolution">例一：Communication equalization/deconvolution</h5>
<p>Assuming a lowpass channel $\Phi = 1 + 0.9 z^{-1}$.  The equalizer $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{T}$ 變成 highpass filter; zero-forcing equalizer (ZFE).  如果 noise $\varepsilon$ 是 broadband noise, high frequency noise 會被放大。</p>

<p>In the case of ML, 我們必須小心選 basis functions to ensure matrix $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)$ can be inverted and avoid “ill-condition”.  通常使用 sparse model with few basis functions.</p>

<h4 id="method-2-em-algorithm-for-bayesian-linear-regression">Method 2, EM algorithm for Bayesian Linear Regression</h4>

<p>Method 2 放寬 $w$ 從定值 fixed value 變成 distribution (random variable). Voila，這就是 Bayesian 精神！</p>

<p>A Bayesian treatment of the linear model begins by assigning a prior distribution to the weights of the model. This introduces bias in the estimation but also greatly reduces its variance, which is a major problem of the ML estimate.</p>

<p>此處我們用 common choice of independent, zero-mean, Gaussian prior distribution for the weights of the linear model:</p>

<script type="math/tex; mode=display">p(\mathbf{w} ; \alpha)=\prod_{m=1}^{M} N\left(w_{m} \mid 0, \alpha^{-1}\right)</script>

<p>當然假設 zero-mean 聽起來有點奇怪，有可能引入 bias, 但好處是有 regularization 的效果，儘量讓 $w_m$ 不要太大。</p>

<p>Bayesian inference 接下來是計算 posterior distribution of the hidden variable</p>

<script type="math/tex; mode=display">\begin{equation}
p(\mathbf{w} \mid \mathbf{t} ; \alpha, \beta)=\frac{p(\mathrm{t} \mid \mathbf{w} ; \beta) p(\mathbf{w} ; \alpha)}{p(\mathbf{t} ; \alpha, \beta)} \label{eqMAP}
\end{equation}</script>

<p>$\eqref{eqMAP}$ 分母部分進一步展開：</p>

<script type="math/tex; mode=display">p(\mathbf{t} ; \alpha, \beta)=\int p(\mathbf{t} \mid \mathbf{w} ; \beta) p(\mathbf{w} ; \alpha) d \mathbf{w}=N\left(\mathbf{t} \mid 0, \beta^{-1} \mathbf{I}+\alpha^{-1} \mathbf{\Phi} \boldsymbol{\Phi}^{T}\right)</script>

<p>$\eqref{eqMAP}$，posterior of the hidden variable，可以寫成：</p>

<script type="math/tex; mode=display">\begin{equation} 
p(\mathbf{w} \mid \mathbf{t} ; \alpha, \beta)=N(\mathbf{w} \mid \boldsymbol{\mu}, \boldsymbol{\mathbf{\Sigma}}) \label{eqPost}
\end{equation}</script>

<p>where</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\boldsymbol{\mu} &=\beta \boldsymbol{\Sigma} \Phi^{T} \mathbf{t} \label{eqMean}\\
\boldsymbol{\Sigma} &=\left(\beta \boldsymbol{\Phi}^{T} \boldsymbol{\Phi}+\alpha \mathbf{I}\right)^{-1} \label{eqVar}
\end{align} %]]></script>

<p>可以證明，$\alpha, \beta$ 可以用以下的 maximum likelihood estimate.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\left(\alpha_{\mathrm{ML}}, \beta_{\mathrm{ML}}\right)=& \underset{\alpha, \beta}{\arg \min }\left\{\log \left|\beta^{-1} \mathbf{I}+\alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{T}\right|\right. \nonumber \\
&\left.+\mathbf{t}^{T}\left(\beta^{-1} \mathbf{I}+\alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{T}\right)^{-1} \mathbf{t}\right\} \label{eqab}
\end{align} %]]></script>

<p>直接計算 $\eqref{eqab}$ 非常困難。除了 $\eqref{eqab}$ 微分非常複雜。$\alpha, \beta \ge 0$ 是一個 constrained optimization 問題。 EM algorithm 提供一個有效的方法解 $\alpha, \beta$ and infer $\mathbf{w}$</p>

<p><strong>E-step</strong> Compute the Q function</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta) &=\langle\ln p(\mathbf{t}, \mathbf{w} ; \alpha, \beta)\rangle_{p\left(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)}\right)} \\
&=\langle\ln p(\mathbf{t} \mid \mathbf{w} ; \alpha, \beta) p(\mathbf{w} ; \alpha, \beta)\rangle_{p\left(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)}\right)} \\
&=\left\langle\frac{N}{2} \ln \beta-\frac{\beta}{2}\left(\|\mathbf{t}-\boldsymbol{\Phi} \mathbf{w}\|^{2}\right)\right.\\
&\left.+\frac{M}{2} \ln \alpha-\frac{\alpha}{2}\left(\|\mathbf{w}\|^{2}\right)\right\rangle+\text { const } \\
=& \frac{N}{2} \ln \beta-\frac{\beta}{2}\left\langle\|\mathbf{t}-\boldsymbol{\Phi} \mathbf{w}\|^{2}\right\rangle+\frac{M}{2} \ln \alpha \\
&-\frac{\alpha}{2}\left(\left\langle\|\mathbf{w}\|^{2}\right\rangle\right)+\text { const. }
\end{aligned} %]]></script>

<p>三角括號是對 $p(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)})$ 的期望值。代入 $\eqref{eqPost}$ 得到</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)=& \frac{N}{2} \ln \beta-\frac{\beta}{2}\left(\left\|\mathbf{t}-\boldsymbol{\Phi} \boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Phi}^{T} \boldsymbol{\Sigma}^{(t)} \boldsymbol{\Phi}\right]\right) \\
&+\frac{M}{2} \ln \alpha-\frac{\alpha}{2}\left(\left\|\boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Sigma}^{(t)}\right]\right)+\mathrm{const}
\end{aligned} %]]></script>

<p>where $\boldsymbol{\mu}^{(t)}$ and $\boldsymbol{\Sigma}^{(t)}$ are computed using the current estimates of the parameters $\alpha^{(t)}$ and $\beta^{(t)}$ :</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\boldsymbol{\mu}^{(t)} &=\beta^{(t)} \boldsymbol{\Sigma}^{(t)} \boldsymbol{\Phi}^{T} \mathbf{t} \\
\boldsymbol{\Sigma}^{(t)} &=\left(\beta^{(t)} \mathbf{\Phi}^{T} \boldsymbol{\Phi}+\alpha^{(t)} \mathbf{I}\right)^{-1}
\end{aligned} %]]></script>

<p><strong>M-step</strong> Maximize $Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)$ with respect to $\alpha, \beta$.</p>

<script type="math/tex; mode=display">\left(\alpha^{(t+1)}, \beta^{(t+1)}\right)=\underset{(\alpha, \beta)}{\arg \max } Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)</script>

<p>結果很簡單</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\alpha^{(t+1)} &=\frac{M}{\left\|\boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Sigma}^{(t)}\right]} \label{eqa}\\
\beta^{(t+1)} &=\frac{N}{\left\|\mathbf{t}-\boldsymbol{\Phi} \boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Phi}^{T} \mathbf{\Sigma}^{(t)} \boldsymbol{\Phi}\right]} \label{eqb}
\end{align} %]]></script>

<p>$\eqref{eqa}$ 和 $\eqref{eqb}$ 同時保證 $\alpha, \beta$ 永遠為正值。</p>

<p>幾個重點：</p>
<ul>
  <li>EM algorithm 有可能收斂到 local minimum; initial condition 很重要</li>
  <li>注意 $\mathbf{w}$ 不是一個值，而是 distribution.  Inference of $\mathbf{w}$ 就是 posterior distribution $\eqref{eqPost}$.  Posterior distribution 的 mean $\eqref{eqMean}$ 稱為 Bayesian linear minimum mean squire error (LMMSE) inference for $\mathbf{w}$.</li>
</ul>

<h4 id="method-3-variational-em-based-bayesian-linear-regression">Method 3, Variational EM-based Bayesian Linear Regression</h4>

<p>因為非常複雜，可以直接參考 [].</p>

<h5 id="例二filtering">例二：filtering</h5>

<p><img src="/media/16287874512211.jpg" alt="" /></p>

<h3 id="例二-bayesian-gmm">例二： Bayesian GMM</h3>

<script type="math/tex; mode=display">p(\mathbf{x})=\sum_{j=1}^{M} \pi_{j} N\left(x ; \boldsymbol{\mu}_{j}, \mathbf{T}_{j}\right)</script>

<p>where $\boldsymbol{\pi} = { \pi_j }$ 代表 weights or mixing coefficients.  $\boldsymbol{\mu} = { \boldsymbol{\mu}<em>{j} }$ 是 means of Gaussian distribution.  $\mathbf{T} = { \mathbf{T}</em>{j} }$ 是 precision (inverse covariance) matrices.  在 Bayesian GMM 我們更常用 precision matrix.</p>

<p>Bayesian GMM 和一般 GMM 有什麼不同？ 最大的差別就是 $\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T}$ 不再是 parameters for estimation, 而是 random variables. 這有什麼好處？我們可以 impose or embedded our priors on $\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T}$, 通常是 conjugate priors (i.e. no informative priors) <sup id="fnref:prior"><a href="#fn:prior" class="footnote">1</a></sup>.</p>

<p>Bayesian GMM 的 graph model 如下。Hidden random variables 包含 $h = (\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T})$. Bayesian 的目標是找出 $p(h\mid x)$, 顯然不會有 analytic solution.</p>

<p><img src="/media/16285137362672.jpg" alt="-w237" /></p>

<p>因此我們 divide-and-conquer 利用 $\eqref{eqVarJ2}$
假設 mean-field approximation</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
q(\mathrm{h}) &= q_{Z}(\mathbf{Z}) q_{\pi}(\boldsymbol{\pi}) q_{\mu T}(\boldsymbol{\mu}, \mathbf{T}) \\
q_{Z}(\mathbf{Z}) &=\prod_{n=1}^{N} \prod_{j=1}^{M} r_{j n}^{z_{j n}} \\
q_{\pi}(\boldsymbol{\pi}) &=\operatorname{Dir}\left(\boldsymbol{\pi} \mid\left\{\lambda_{j}\right\}\right) \\
q_{\mu T}(\boldsymbol{\mu}, \mathbf{T}) &=\prod_{j=1}^{M} q_{\mu}\left(\boldsymbol{\mu}_{j} \mid \mathbf{T}_{j}\right) q_{T}\left(\mathbf{T}_{j}\right) \\
q_{\mu}\left(\boldsymbol{\mu}_{j} \mid \mathbf{T}\right) &=\prod_{j=1}^{M} N\left(\boldsymbol{\mu}_{j} ; \mathbf{m}_{j}, \beta_{j} \mathbf{T}_{j}\right) \\
q_{T}(\mathbf{T}) &=\prod_{j=1}^{M} W\left(\mathbf{T}_{j} ; \eta_{j}, \mathrm{U}_{j}\right)
\end{aligned} %]]></script>

<p>看起來還是很複雜，不過 [@tzikasVariationalApproximation2008] 的 reference [27] 有詳細的公式。可以用“簡單” iterative update procedure 得到 optimal approximation $q(h)$ to the true posterior $p(h\mid x)$, 這就是 variational E-step.  下一步就是 variation M-step, 不贅述。</p>

<p>Bayesian-GMM 比起 EM-GMM 到底有什麼好處。前面提到可以 impose priors. 如果沒有 prior information (i.e. use conjugate prior), 還有好處嗎？[@tzikasVariationalApproximation2008] 的說法是 Bayesian-GMM 不會有 singular solution, i.e. single data point Gaussian.  然而在 EM-GMM 常常會發生，如下圖 20 Gaussian components。一般 EM-GMM 解決的方法就是多跑幾次 randomize initial conditions to avoid it.</p>

<p><img src="/media/16285679550223.jpg" alt="" /></p>

<p>另一個好處是可以直接用 Bayesian GMM 決定 Gaussian component number, 而不需要用其他方法 (e.g. cross-validation)。實作如下圖。(a) 初始是 20 component Gaussians; (b), (c) model evolution; (d) 最終解只剩下 5 個 Gaussian components, 其餘 15 個 Gaussian components weight 為 0。注意收斂的過程中都沒有 singularity.</p>

<p>這聽起來比較 significant, 不過有一個 catch, 就是 Dirichlet prior 不允許 component mixing weight 為 0.  因此如果要用 Bayesian-GMM 決定 Gaussian component number, 必須 remove $\boldsymbol{\pi} = { \pi_j }$ from priors.  也就是把 $\boldsymbol{\pi} = { \pi_j }$ 視為 parameter to be estimated.</p>

<p><img src="/media/16285916007272.jpg" alt="" /></p>

<p>Bayesian GMM 的 graph model 如下。注意此時的 $\pi$ 變成方框，代表 parameter to be estimated.  Hidden random variables 包含 $h = (\mathbf{Z}, \boldsymbol{\mu}, \mathbf{T})$.</p>

<p><img src="/media/16286002562443.jpg" alt="" /></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
&q(\mathrm{h})=q_{Z}(\mathbf{Z}) q_{\mu}(\boldsymbol{\mu}) q_{T}(\mathrm{T})\\
&q_{Z}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{j=1}^{M} r_{j n}^{z_{j n}} \\
&q_{\mu}(\boldsymbol{\mu})=\prod_{j=1}^{M} N\left(\boldsymbol{\mu}_{j} \mid \mathrm{m}_{j}, \mathbf{S}_{j}\right) \\
&q_{T}(\mathbf{T})=\prod_{j=1}^{M} W\left(\mathbf{T}_{j} \mid \eta_{j}, \mathbf{U}_{j}\right)
\end{aligned} %]]></script>

<p>同樣經過一番計算 variational E-step and M-step (此處省略)，可以得到</p>

<script type="math/tex; mode=display">\pi_{j}=\frac{\sum_{n=1}^{N} r_{j n}}{\sum_{k=1}^{M} \sum_{n=1}^{N} r_{k n}}</script>

<p>在 iteration 過程中，有一些 mixing coefficients ${\pi_j}$ 收斂到 0. 定性來說，variational bound 可以視為兩項之和：第一項是 likelihood function, 第二項是 prior 造成的 penalty term to penalizes complex models.</p>

<div class="footnotes">
  <ol>
    <li id="fn:prior">
      <p>Dirichlet for $\boldsymbol{\pi}$.  Gauss-Wishart for ($\boldsymbol{\mu}, \mathbf{T})$ <a href="#fnref:prior" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            
            <a href="/tag/#/softmax" rel="tag"># softmax</a>
          
            
            <a href="/tag/#/EM" rel="tag"># EM</a>
          
            
            <a href="/tag/#/Bayesian" rel="tag"># Bayesian</a>
          
            
            <a href="/tag/#/Variational" rel="tag"># Variational</a>
          
        </div>
      

      
      
      
      
      

      
      
        <div class="post-nav" id="post-nav-id">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/08/03/English/" rel="prev" title="English">
                English <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      
      

      
    </footer>
  </article>

  <div class="post-spread">
    
  </div>
</div>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          

  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        
        
        







      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/assets/images/avatar.gif"
               alt="Allen Lu (from John Doe)" />
          <p class="site-author-name" itemprop="name">Allen Lu (from John Doe)</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">27</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
        
        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            








            
              <div class="post-toc-content">
    <ol class=nav>
      <li class="nav-item nav-level-2"> <a class="nav-link" href="#major-reference"> <span class="nav-number">1</span> <span class="nav-text">Major Reference</span> </a> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#em-algorithm"> <span class="nav-number">2</span> <span class="nav-text">EM Algorithm</span> </a> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#variational-em-framework"> <span class="nav-number">3</span> <span class="nav-text">Variational EM Framework</span> </a> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#examples"> <span class="nav-number">4</span> <span class="nav-text">Examples</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-3"> <a class="nav-link" href="#例一-linear-regression-filterestimate-a-noisy-signal"> <span class="nav-number">4.1</span> <span class="nav-text">例一： Linear Regression (filter/estimate a noisy signal)</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-4"> <a class="nav-link" href="#問題描述"> <span class="nav-number">4.1.1</span> <span class="nav-text">問題描述</span> </a> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#三種解法圖式"> <span class="nav-number">4.1.2</span> <span class="nav-text">三種解法圖式</span> </a> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#method-1-ml-for-vanilla-linear-regression"> <span class="nav-number">4.1.3</span> <span class="nav-text">Method 1, ML for Vanilla Linear Regression</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-5"> <a class="nav-link" href="#例一communication-equalizationdeconvolution"> <span class="nav-number">4.1.3.1</span> <span class="nav-text">例一：Communication equalization/deconvolution</span> </a> </li> </ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#method-2-em-algorithm-for-bayesian-linear-regression"> <span class="nav-number">4.1.4</span> <span class="nav-text">Method 2, EM algorithm for Bayesian Linear Regression</span> </a> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#method-3-variational-em-based-bayesian-linear-regression"> <span class="nav-number">4.1.5</span> <span class="nav-text">Method 3, Variational EM-based Bayesian Linear Regression</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-5"> <a class="nav-link" href="#例二filtering"> <span class="nav-number">4.1.5.1</span> <span class="nav-text">例二：filtering</span> </a> </li> </ol> </li> </ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#例二-bayesian-gmm"> <span class="nav-number">4.2</span> <span class="nav-text">例二： Bayesian GMM</span> </a> </li> </ol> </li>
    </ol>
  </div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>

        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Allen Lu (from John Doe)</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://jekyllrb.com">Jekyll</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  











  




  

    

  







  






  

  

  
  


  

  

  

</body>
</html>

