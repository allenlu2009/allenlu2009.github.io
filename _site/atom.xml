<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-09-13T21:03:41+08:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">NexT</title><author><name>Allen Lu (from John Doe)</name></author><entry><title type="html">跨平臺 Markdown Plus MathJax Blog Editing 經驗分享</title><link href="http://localhost:4000/ai/2021/09/12/Cross_Platfrom_Markdown_Math_Blog/" rel="alternate" type="text/html" title="跨平臺 Markdown Plus MathJax Blog Editing 經驗分享" /><published>2021-09-12T00:00:00+08:00</published><updated>2021-09-12T00:00:00+08:00</updated><id>http://localhost:4000/ai/2021/09/12/Cross_Platfrom_Markdown_Math_Blog</id><content type="html" xml:base="http://localhost:4000/ai/2021/09/12/Cross_Platfrom_Markdown_Math_Blog/">&lt;h2 id=&quot;跨平臺-markdownmathjax-blog-editing-分享&quot;&gt;跨平臺 Markdown+MathJax Blog Editing 分享&lt;/h2&gt;

&lt;p&gt;我常用的計算平臺包含：MacBook air/pro (Mac OS),  PC (Windows 10), and iPad air (iPad OS).&lt;/p&gt;

&lt;p&gt;常常使用的 computing platform and offline blog editors 如下：&lt;/p&gt;

&lt;h2 id=&quot;computing-platform-and-editor&quot;&gt;Computing Platform and Editor&lt;/h2&gt;

&lt;p&gt;MacBook air/pro (mobile at work)  - &lt;strong&gt;Marsedit (paid), mweb (paid, Markdown)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Windows (fixed at home) - WLW (Window Live Writer, free), &lt;strong&gt;VScode (free, Markdown), Typora (free, Markdown)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;iPad air (for portability and photo) - &lt;strong&gt;mweb (free for editing, not for creating, publishing, Markdown)&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;推薦的-markdownmathjax-editor&quot;&gt;推薦的 Markdown+MathJax Editor&lt;/h2&gt;

&lt;h3 id=&quot;爲什麽使用-markdown-editor-for-blog&quot;&gt;爲什麽使用 Markdown editor for Blog&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Math equation input: Markdown editor 大多可以插入 latex-based 數學公式 (e.g. MathJax or KaTex).  搭配 &lt;a href=&quot;https://mathpix.com/&quot;&gt;Mathpix Snip&lt;/a&gt; (大推！) 是我用過最好的 math equation generator!  Example: $i \hbar \frac{d}{d t}\mid \Psi(t)\rangle=\hat{H}\mid \Psi(t)\rangle$&lt;/li&gt;
  &lt;li&gt;主要的 blog platforms (e.g. Wordpress, 特別是 Github) 都支持 Markdown.&lt;/li&gt;
  &lt;li&gt;可携性大爲提高。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此我從 WYSIWYG editor 像是 Marsedit (Mac), WLW (PC) 改用 Markdown editors.&lt;/p&gt;

&lt;h3 id=&quot;數學公式-rendering-for-blog&quot;&gt;數學公式 rendering for Blog&lt;/h3&gt;

&lt;p&gt;最早的數學公式 rendering 來自 Latex，主打 professional and static publising 例如 thesis, paper, etc. pdf files.&lt;/p&gt;

&lt;p&gt;Latex 的龐大和 static 特性並不適合用於 dynamic rendering 的 blog.  因此有兩種變形：MathJax and KaTex.   MathJax 比較接近 Latex.  KaTex 主打快速 dynamic rendering, 有一些 equation numbering and reference 並不支持，&lt;strong&gt;因此我主要使用 MathJax.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210911234257339.png&quot; alt=&quot;image-20210911234257339&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Latex/MathJax 的輸入可以使用 Mathpix Snip capture, 非常有用！&lt;/p&gt;

&lt;h3 id=&quot;分享我用過的-markdownmath-editors&quot;&gt;分享我用過的 Markdown+Math editors&lt;/h3&gt;

&lt;h4 id=&quot;mweb-built-in-mathjax-support&quot;&gt;mweb (built-in MathJax support)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;優點:&lt;/strong&gt; (1) 支持 markdown editing, content management, figure management, and blog publishing; (2) 跨 Mac OS and iPad OS or iOS;  (3) 支持 iCloud sync between MacBook pro and iPad air.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;缺點:&lt;/strong&gt; &lt;strong&gt;(1) 沒有 Windows version.&lt;/strong&gt;  (2) mweb3.0 對於 math equation 支持比較差，有一些常用的 math symbol (e.g. Lagrangian) 不支持。不過 mweb4.0 似乎有改善。&lt;strong&gt;(3) 另外對於 Apple M1 晶片的穩定性很差！&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;typora-built-in-mathjax-support&quot;&gt;Typora (built-in MathJax support)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;優點:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;設定簡單，基本打開  (Jekyll _post) directory 就可以編輯。下圖左是 directory file list.  下圖右是 editing&amp;amp;preview together window.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;UI 非常簡潔，不支持 side-by-side markdown and rendering display. 但是使用一陣子發覺 Typora 直接 rendering output to display 很棒。&lt;/strong&gt;不會像 mweb editing 時，另一個 display 動來動去。特別在小銀幕非常適合，mweb 的 iOS 版採用一樣的做法。可惜沒有 iOS/iPad OS version&lt;/li&gt;
      &lt;li&gt;跨 Windows/Mac OS/Linux, 但沒有 iOS version.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;缺點:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;只有 Markdown editing, preview, file list, 但是沒有 blog publishing.&lt;/li&gt;
      &lt;li&gt;Another big disadvantage: math equation number 支持不好 \label{} 常常有問題。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210912100220717.png&quot; alt=&quot;image-20210912100220717&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;設定 root-path for image directory (下面兩步都要做！)
    &lt;ul&gt;
      &lt;li&gt;在本文加上： typora-root-url: ../../allenlu2009.github.io&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/media/image-20210814233107185.png&quot; alt=&quot;image-20210814233107185&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Typora: Format: Image: Use Image Root Path: set to the above directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;設定 copy and paste image path:  Typora: File : Preference: Image :  設定 copy and paste image directory.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210912100913945.png&quot; alt=&quot;image-20210912100913945&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;vscode--math-preview-extension&quot;&gt;VSCode + Math Preview Extension&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;優點:&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;VSCode 整合 (via extension) git version control and Github pull/push, 可能對 Github posting 比較容易&lt;/li&gt;
      &lt;li&gt;VSCode 有很多的 extensions, 例如 jekyll 可以直接 preview post to github blog.  或是 mathjax 以及其他 preview 的功能。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;缺點:&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;VSCode 有 built-in markdown preview!  但完全不支持 Math rendering!!&lt;/strong&gt; 需要 install plug-in.  另外 preview image 非常多坑！&lt;/li&gt;
      &lt;li&gt;除非是 coding 達人，不然不推用 VSCode 做 math blog!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VSCode extension 有兩個 extensions  &lt;strong&gt;(1) Markdown+Math:  only support KaTex;  (2) Markdown Preview Enhaced: Default KaTex, Optional MathJax&lt;/strong&gt;, changed in setting.
&lt;img src=&quot;/media/image-20210912001358757.png&quot; alt=&quot;image-20210912001358757&quot; style=&quot;zoom:40%;&quot; /&gt; and &lt;img src=&quot;/media/image-20210912001314540.png&quot; alt=&quot;image-20210912001314540&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;兩者各有缺點，所以兩個都用。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Markdown+Math (KaTex only): 下圖左是 Markdown, 下圖右是 Markdown+Math preview.  首先這個 markdown 支持 dark mode; 再來 math 只支持 KaTex, 所以 \label{} 以及 cross reference equation 不支持。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210912085129732.png&quot; alt=&quot;image-20210912085129732&quot; style=&quot;zoom:90%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Markdown Preview Enhanced (Default KaTex, change to MathJax in setting): 下圖左是 Markdown, 下圖右是 Markdown Preview Enhanced (Setting: KaTex -&amp;gt; MathJax).   看起來還不錯。但只要左邊編輯 markdown, 右邊數學公式就出現亂碼！所以只能用於最後確認效果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210912081052675.png&quot; alt=&quot;image-20210912081052675&quot; style=&quot;zoom:100%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image preview 一堆問題！同樣分成 built-in preview 和 Markdown Preview Enhaced.  總結來説都很爛，但是 Markdown Preview Enhanced (差) 比 built-in preview (爛) 好。
    &lt;ul&gt;
      &lt;li&gt;image reference 格式:  built-in preview 只接受標準 markdown 格式： &lt;img src=&quot;...jpg&quot; alt=&quot;text&quot; /&gt; ；preview enhanced 支持標準格式以及 html 格式 : &amp;lt;img src =“…”&amp;gt;”&lt;/li&gt;
      &lt;li&gt;remote image:  由於 secuity concern, built-in preview 只接受 https;  preview enhanced 支持 http or https.  e.g.
&lt;img src=&quot;https://ww1.sinaimg.cn/mw690/81b78497jw1emfgwkasznj21hc0u0qb7.jpg&quot; alt=&quot;https&quot; /&gt;
&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/81b78497jw1emfgwkasznj21hc0u0qb7.jpg&quot; alt=&quot;http&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;local file system image:  這是最糟糕的部分！只支持 local directory 為 root 的絕對 path!!!  也就是説，無法另外設定 root path, 也不能用 ../media/ 往上 path (因爲 root 沒有更上面的 directory).  我最後只能在 _post directory 之下做一個 symbolic link : PS&amp;gt;  new-item -itemtype symboliclink -path ./  -name media -value ../media&lt;/li&gt;
      &lt;li&gt;完全不建議一般人用 VSCode 作 markdown blog!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cloud-platform&quot;&gt;Cloud Platform&lt;/h2&gt;

&lt;p&gt;跨平臺不只是 computing platforms, Windows/Mac OS/iOS, 更重要是要有 cloud platform 同步到一個 database!  從不同的 computing platforms 要很容易讀寫這個 cloud platform.  基本上只有幾個常見 cloud platforms 能達到這個要求：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google Drive&lt;/li&gt;
  &lt;li&gt;Microsoft Onedrive&lt;/li&gt;
  &lt;li&gt;Apple iCloud&lt;/li&gt;
  &lt;li&gt;Dropbox&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我選擇使用 iCloud, 原因很簡單。因爲 mweb 只支持 iCloud and Dropbox.   Dropbox 的 free quota 只有 2GB.  iCloud 的 free quota 5GB.   本文是先用 MacBook Pro 的 mweb create and start the article.  再使用 PC Typora 纂寫大部分内容。&lt;/p&gt;

&lt;p&gt;最後再切回 iPad Air 使用 MWeb 繼續，並且拍一張照片結束。這是目前我比較滿意的寫作方式。
最後再再切回 MacBook Pro to publish, have fun!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16101804667280/16102111356622.jpg&quot; alt=&quot;text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210911234257339.png&quot; alt=&quot;test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210911234257339.png&quot; alt=&quot;image-20210911234257339&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="ML" /><category term="EM" /><category term="Bayesian" /><category term="MAP" /><summary type="html">跨平臺 Markdown+MathJax Blog Editing 分享</summary></entry><entry><title type="html">Math AI - Variational Autoencoder Vs. Variational EM Algorithm</title><link href="http://localhost:4000/ai/2021/08/19/Variational-Autoencoder/" rel="alternate" type="text/html" title="Math AI - Variational Autoencoder Vs. Variational EM Algorithm" /><published>2021-08-19T07:10:08+08:00</published><updated>2021-08-19T07:10:08+08:00</updated><id>http://localhost:4000/ai/2021/08/19/Variational%20Autoencoder</id><content type="html" xml:base="http://localhost:4000/ai/2021/08/19/Variational-Autoencoder/">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;h2 id=&quot;main-reference&quot;&gt;Main Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;[@kingmaIntroductionVariational2019] : excellent reference&lt;/li&gt;
  &lt;li&gt;[@escuderoVariationalAutoEncoders2020]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;重點-outline&quot;&gt;重點 outline&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;VAE 第一個 innovation (encoder+decoder): 使用 encoder neural network ($\phi$) 和 decoder neural network ($\theta$) 架構。&lt;strong&gt;從 autoencoder 的延伸&lt;/strong&gt;似乎很直觀。但從 deterministic 延伸到 probabilistic 有點魔幻寫實，需要更嚴謹的數學框架。&lt;/li&gt;
  &lt;li&gt;VAE 第二個 innovation (DLVM):  引入 hidden (random) variable $\mathbf{z}$, 從 $\mathbf{z} \to \text{neural network}\,(\theta) \to \mathbf{x}.$  &lt;strong&gt;Hidden variable $\mathbf{z}$ 源自 (variational) EM + DAG;  再用 (deterministic) neural network of $\theta$ for parameter optimization.  這就是 DLVM (Deep Learning Variable Model) 的精神。&lt;/strong&gt;  根據 (variational) EM:
    &lt;ul&gt;
      &lt;li&gt;E-step: 找到 $q(\mathbf{z}) \approx p_{\theta}(\mathbf{z} \mid \mathbf{x})$, &lt;strong&gt;也就是 posterior&lt;/strong&gt;, &lt;strong&gt;但我們知道在 DLVM posterior 是 intractable，必須用近似&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;M-step: optimize $\theta$ based on posterior:  $\underset{\boldsymbol{\theta}}{\operatorname{argmax}} E_{q(\mathbf{z})} \ln p_{\theta}(\mathbf{x}, \mathbf{z})$,  &lt;strong&gt;其中的 joint distribution 是 tractable, 但是 $q(\mathbf{z})$ intractable&lt;/strong&gt;, 所以是卡在 posterior intractable 這個問題！&lt;/li&gt;
      &lt;li&gt;Iterate E-step and M-step in (variational EM); 在 DLVM 就變成 SGD optimization!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;VAE 第三個 innovation 就是為了解決2.的 posterior 問題  $q(\mathbf{z}) \to q_{\phi}(\mathbf{z}\mid x)$:  用另一個 (tractable) decoder neural network $\phi$, 來近似 (intractable) posterior $q_{\phi}(\mathbf{z}\mid x) \approx p(\mathbf{z}\mid x)$&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;因此 VAE 和 DLVM (or variational EM) 的差別在於 VAE 多了 decoder neural network $\phi$ ，所以三者的數學框架非常相似！&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;VAE 的 training loss 包含 reconstruction loss (源自 encoder+decoder) + 上面的 M-step loss (源自 variational EM)&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Maximum likelihood optimization ~ minimum cross-entropy loss (not in this case)  ~ M-step loss (in this case)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;同樣的方法應該可以用在很多 DLVM 應用中。如果有 intractable posterior, 就用 (encoder) neural network 近似。但問題是要有方法 train 這個 encoder.  VAE 很巧妙的同時 train encoder + decoder 是用原始的 image and generative image.   需要再檢驗。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;下圖顯示 ML, EM, DLVM, VAE 的演進關係；DLVM 和 VAE echo 1-4.  雙圓框代表 observed random variable, 單圓框代表 hidden random variable.  單方框代表 (fixed and to be estimated) parameter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210830230538496.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其他的重點：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;如何用 deterministic neural network 表示 probabilistic bayesian inference?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如何用 deterministic neural network 表示 probabilistic VAE encoder and decoder?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如何把 intractable posterior 用 tractable neural network encoder 近似?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-autoencoder-again&quot;&gt;Variational Autoencoder, Again&lt;/h2&gt;

&lt;p&gt;第 N 次討論 VAE (variational autoencoder).  之前從 AE (autoencoder) 出發，有一些手感。但用 deterministic autoencoder 延伸想像力到 probabilistic VAE 還是隔了一層~~紗~~山。有點像二十世紀初把古典力學加上一點量子想像 ($E = h\nu$) 得到氫原子的量子光譜。雖然結果對了，但只能用在特定的情況。&lt;/p&gt;

&lt;p&gt;或是從 “variational inference” 的出發, 掉入一堆數學中沒有抓到重點。&lt;/p&gt;

&lt;p&gt;我們這次從 gaph+variational inference 出發。引入 neural network 變成 deep learning variable model (DLVM)。再引入 encoder neural network for posterior.  另外我們會比較 variational EM 和 VAE 增加理解。&lt;/p&gt;

&lt;h2 id=&quot;ml-estimation-和-bayesian-inference-到底有什麼差別&quot;&gt;ML estimation 和 Bayesian inference 到底有什麼差別？&lt;/h2&gt;

&lt;p&gt;簡單說 ML estimation 把 unknown/hidden 視為 a &lt;strong&gt;“fixed parameter”&lt;/strong&gt; (上圖左上).  Bayesian inference 把 unknown/hidden 視為 &lt;strong&gt;“distribution”&lt;/strong&gt; described by a random variable (上圖左下).&lt;/p&gt;

&lt;p&gt;有時候我們也把 $p(x;\theta)$ 寫成 conditional distribution 形式 $p(x\mid\theta).$​  嚴格來說並不對。不過可以視為 Bayesian 詮釋的擴展。&lt;/p&gt;

&lt;p&gt;ML estimation 做法是微分上式，解 $\theta$ parameter.&lt;/p&gt;

&lt;p&gt;Bayesian 的觀念是: (1) $\theta$ 視為 hidden random variable; (2) 引入 hidden random variable $\mathbf{z}$ with $\theta$ as a parameter.&lt;/p&gt;

&lt;p&gt;我們假設 (1), 利用 Bayes formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta | x) = \frac{p(x | \theta) p(\theta)}{p(x)}&lt;/script&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z | x; \theta ) = \frac{p(x | z; \theta) p(z; \theta)}{p(x)}&lt;/script&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\theta}(z | x) = \frac{p_{\theta}(x | z) p_{\theta}(z)}{p(x)}&lt;/script&gt;

&lt;p&gt;&lt;u&gt;上式的術語和解讀&lt;/u&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Random variable $x$ :  post (事後) observations, (post) evidence. $p(x)$ 稱為 evidence distribution or marginal likelihood.&lt;/li&gt;
  &lt;li&gt;Random variable $\mathbf{z}$ : 相對於 $x$, $\mathbf{z}$ 是 prior (事前, 先驗) 並且是 hidden variable (i.e. not evidence).  擴展我們在 maximum likelihood 的定義，從 parameter 變成 random variable.  $p(z)$​​ &lt;strong&gt;稱為 prior distribution.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;注意 prior 是 distribution&lt;/strong&gt;,  不會出現在 ML, 因為 $z$​ 在 ML 是 parameter.  只有在 Bayesian 才有 prior (distribution)!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conditional distribution $p(x\mid z)$ :  likelihood (或然率)。擴展我們在 maximum likelihood 的定義，從 parameter dependent distribution or function 變成 conditional distribution.&lt;/li&gt;
  &lt;li&gt;Conditional distribution $p(z\mid x)$ ： &lt;strong&gt;posterior, 事後機率。就是我們想要求解的東西。&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;注意 posterior 是 conditional distribution&lt;/strong&gt;.  有人會以為 $p(z)$ 是 prior distribution, $p(x)$​ 是 posterior distribution. Wrong!&lt;/li&gt;
      &lt;li&gt;Posterior 不會出現在 ML, 只有在 Bayesian 才會討論 posterior (distribution)!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;簡言之：Posterior&lt;/strong&gt; $\propto$ &lt;strong&gt;Likelihood x Prior&lt;/strong&gt; $\to p(z \mid x) \propto {p(x \mid z) \times p(z)}$
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;一般我們忽略 $p(x)$ ，因為它和要 estimate 的 $z$​​ distribution (or parameter) 無關，視為常數忽略。&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;很好記: 事後 = 事前 x 喜歡 (likelihood).  如果很喜歡，才會有事後。如果不喜歡，事後不理 (0分)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Prior 和 posterior (事前/先驗，事後) 都是 Bayesian 才有的說法。 ML (or Frequentist) 不會有 prior and posterior 說法。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;以通信為例，$z$ 是 transmitted signal (unknown),  $x$ 是 received signal,  $x = z + n$,  是 transmitted signal 加 noise.  如果只根據 $p(\text{received signal}\mid\text{transmitted signal}) = p(x\mid z)$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayesian-inference-for-vae-思路&quot;&gt;Bayesian Inference for VAE 思路&lt;/h2&gt;

&lt;p&gt;我們的問題比較類似 (2), 引入一個 hidden variable, z, with parameter $\theta$.  這和 EM algorithm 的想法完全一樣。藉著引入 hidden variable to account for some incomplete information (參考 EM article of incomplete data).&lt;/p&gt;

&lt;p&gt;一般 Bayesian inference 是求 posterior $p(z\mid x; \theta)$, or maximize the likelihood $p(x \mid z; \theta)$.   我們待會談到 VAE，卻是要找 $p(x)$, i.e. marginal likelihood.  數學上是 $p(x) = \int_{z} p(x, z; \theta) dz = \int_{z} p(x \mid z; \theta)p(z) dz $; where $\theta$ 是 parameter, not a random variable.&lt;/p&gt;

&lt;p&gt;另一個表示式 $p(x)= \int_{z} p(z \mid x)p(x) dx$  顯然不行，因為 $p(x)$ 就是我們要找的 unknown.&lt;/p&gt;

&lt;p&gt;所以我們現在缺 likelihood $p(x\mid z)$ and prior $p(z)$.  $p(z)$ 不是問題，基本就是假設。會隨著 more evidence x 而被取代。我們在 VAE 一般用 N(0, 1).  理論上可以用其他的 distribution, but why bother.  現在問題就是如何求 posterior $p(x\mid z)$.  結論就是用 VAE 來 train 一個 $p(x\mid z)$.&lt;/p&gt;

&lt;h2 id=&quot;deterministic-neural-network-vs-probabilistic-bayesian-inference-how&quot;&gt;Deterministic Neural Network Vs. Probabilistic Bayesian Inference, How?&lt;/h2&gt;

&lt;p&gt;對於 random variable 如 $x$ or $z$,  總有兩個截然不同的面向：(1) (deterministic) distribution function, $p(x), p(z)$; 以及 (2) random sample $\mathbf{x} = {x_1, x_2, \cdots, x_k}$, $\mathbf{z} = {z_1, z_2, \cdots, z_k}.$    一般常用 deterministic function maps random samples $z_i = f(x_i)$ from $x$ space to $z$ space.   因此  $p(x)$ distribution 可以轉換成 $p(z)$ distribution.  實務上我們常常用 function 把一個 distribution 轉成另一個 distribution, 例如 uniform distribution to normal distribution.   Neural network 其實就是一個比較複雜的 (deterministic) function.  這部分沒有問題。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;問題是 Bayesian 需要 conditional distribution.&lt;/strong&gt;   如果 $z = f(x)$ 是一個 deterministic neural network (or any deterministic function).  在這種情況下，conditional probability $p(z\mid x)$ 在 given $x$ 時,  $z$ 卻是一個定值 ，無法變成 distribution (or a delta distribution)?  因為每一個 $x$ 只對應一個 $z$, 沒有所謂 distribution.&lt;/p&gt;

&lt;p&gt;因此如何讓 deterministic neural network 用於 Bayesian inference?  有以下幾種可能性：&lt;/p&gt;

&lt;h4 id=&quot;example-1two-neural-networks-from-a-hidden-random-variable-to-create-conditional-distribution--only-for-demonstration-not-use-here&quot;&gt;Example 1：Two neural networks from a hidden random variable to create conditional distribution.  Only for demonstration, not use here&lt;/h4&gt;

&lt;p&gt;Deterministic functions 可以產生 conditional probability.  如下例&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_probability_distribution&quot;&gt;https://en.wikipedia.org/wiki/Conditional_probability_distribution&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Consider the roll of a fair &lt;a href=&quot;https://en.wikipedia.org/wiki/Dice&quot;&gt;die&lt;/a&gt; and let {\displaystyle X=1}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/889527b2a786390a016fc3ef7cd8eee77e86b6f4&quot; alt=&quot;{\displaystyle X=1}&quot; /&gt; if the number is even (i.e. 2, 4, or 6) and {\displaystyle X=0}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/d519e9e94f279ea82581dfa70a2444e896e2d860&quot; alt=&quot;{\displaystyle X=0}&quot; /&gt; otherwise. Furthermore, let {\displaystyle Y=1}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/867ae2de7c84119e258e68ca484e01e03b00bd73&quot; alt=&quot;Y=1&quot; /&gt; if the number is prime (i.e. 2, 3, or 5) and {\displaystyle Y=0}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/56cd853e6606465d2259975da9d0a0bb08f612af&quot; alt=&quot;Y=0&quot; /&gt; otherwise.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210821221517777.png&quot; alt=&quot;image-20210821221517777&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then the unconditional probability that {\displaystyle X=1}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/889527b2a786390a016fc3ef7cd8eee77e86b6f4&quot; alt=&quot;{\displaystyle X=1}&quot; /&gt; is 3/6 = 1/2 (since there are six possible rolls of the die, of which three are even), whereas the probability that {\displaystyle X=1}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/889527b2a786390a016fc3ef7cd8eee77e86b6f4&quot; alt=&quot;{\displaystyle X=1}&quot; /&gt; conditional on {\displaystyle Y=1}&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/867ae2de7c84119e258e68ca484e01e03b00bd73&quot; alt=&quot;Y=1&quot; /&gt; is 1/3 (since there are three possible prime number rolls—2, 3, and 5—of which one is even).&lt;/p&gt;

&lt;p&gt;$X = f_1(Z)$ and $Y=f_2(Z)$   $Z$ 是 die 的 output random variable $1,2,\cdots,6$ 雖然 $f_1$ and $f_2$  都是 deterministic function, 但是 $P(Y\mid X)$ 的確是 distribution, 因為我們不知道 $X=1$ 到底對應 $Z=?$&lt;/p&gt;

&lt;p&gt;所以如果我們有一個 $Z$ random variable, 以及不同的 neural network $X = f_1(Z)$ and $Y = f_2(Z)$.   Then $p(Y\mid X)$  可以是一個 distribution 而非單一 value.&lt;/p&gt;

&lt;h4 id=&quot;example-2--given-input-經過-deterministic-nn-轉成-probabilistic-conditional-distribution&quot;&gt;Example 2:  Given Input 經過 Deterministic NN 轉成 Probabilistic Conditional Distribution&lt;/h4&gt;

&lt;p&gt;[@kingmaIntroductionVariational2019]&lt;/p&gt;

&lt;p&gt;一般的 differentiable feed-forward neural networks are a particularly flexible and computationally scalable type of &lt;strong&gt;function approximator.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A particularly interesting application is probabilistic models&lt;/strong&gt;, i.e. the use of neural networks for probability density functions (PDFs) or probability mass functions (PMFs) in probabilistic models (how?). Probabilistic models based on neural networks are computationally scalable since they allow for stochastic gradient-based optimization.&lt;/p&gt;

&lt;p&gt;We will denote a deep NN as a vector function:  NeuraNet(.).  In case of neural entwork based image classifcation, for example, nerual networks parameterize a categorical distrbution $p_{\theta}(y\mid \mathbf{x})$ over a class label $y$,  conditioned on an image $\mathbf{x}$.  ??? y is a single label or distribution?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbf{p} &amp;=\operatorname{NeuralNet}_{\boldsymbol{\theta}}(\mathbf{x}) \\
p_{\boldsymbol{\theta}}(y \mid \mathbf{x}) &amp;=\text { Categorical }(y ; \mathbf{p})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the last operation of NeuralNet(.) is typical a softmax() function! such that $\Sigma_i p_i = 1$&lt;/p&gt;

&lt;p&gt;這是很有趣的觀點。 $\mathbf{x}$ and $\mathbf{p}$ 都是 deterministic, 甚至 softmax function 都是 deterministic.  但我們賦予最後的 $y$ probabilistic distribution 涵義！基本上 NN 分類網路都是如此 (e.g. VGG, ResNet, MobileNet)。&lt;/p&gt;

&lt;p&gt;例如 $\mathbf{x}$ 可能是一張狗照片， $\mathbf{p}$ 是 feature extraction of $\mathbf{x}$.  兩者都是 deterministic.   但最後 categorical function 直接把 $\mathbf{p}$  賦予多值的 (deterministic) distribution, 例如狗的機率 $p_1 = 0.8,$ 貓的機率  $p_2 = 0.15,$ 其他的機率  $p_3 = 0.05.$    這和我們一般想像的機率性 outcome,   同一個 $\mathbf{p}$ 有時 output 狗，有時 output 貓不同。&lt;/p&gt;

&lt;p&gt;數學上這只是 vector to vector conversion,   $\mathbf{p}$ 是 high dimension feature vector (e.g. 1024x1), $\mathbf{y} = [y_1, y_2, \cdots]$ 是 low dimension output vector (e.g. 3x1 or 10x1) summing to 1.  重點是這個 low dimension vector $\mathbf{y}$ 就是 conditional distribution!  &lt;strong&gt;也就是一個 sample $\mathbf{x}$ 就可以 output 一個 conditional distribution, 而不需要很多 $\mathbf{x}$ samples 產生 conditional distribution!&lt;/strong&gt;   這很像量子力學中一個電子就可以產生 wave distribution, 有點違反直覺。&lt;/p&gt;

&lt;p&gt;這似乎是把一個 random sample 轉換成一個 (deterministic) conditional distribution 的方式。不過是否是 general method, TBC.&lt;/p&gt;

&lt;p&gt;另外這裡的 $\theta$ 就是 neural network 的 weights, determinstic parameters to be optimzed.&lt;/p&gt;

&lt;h3 id=&quot;neural-network-and-dag-directed-acyclic-graph&quot;&gt;Neural Network and DAG (Directed Acyclic Graph)&lt;/h3&gt;

&lt;p&gt;我們 focus on directed probabilistic graphical models  (PGM) or Bayesian networks.
&lt;script type=&quot;math/tex&quot;&gt;p_{\boldsymbol{\theta}}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{M}\right)=\prod_{j=1}^{M} p_{\boldsymbol{\theta}}\left(\mathbf{x}_{j} \mid P a\left(\mathbf{x}_{j}\right)\right)&lt;/script&gt;
where Pa(xj) is the set of parent variables of node j in the directed graph.&lt;/p&gt;

&lt;p&gt;Traditionally, each conditional probability distribution xx is parameterized as a lookup table or a linear model.&lt;/p&gt;

&lt;p&gt;A more flexible way to parameterize such conditional distributions is with neural networks.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\boldsymbol{\eta} &amp;=\operatorname{NeuralNet}(P a(\mathbf{x})) \\
p_{\boldsymbol{\theta}}(\mathbf{x} \mid P a(\mathbf{x})) &amp;=p_{\boldsymbol{\theta}}(\mathbf{x} \mid \boldsymbol{\eta})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;同樣這裡的 $\theta$ 就是 neural network 的 weights, determinstic parameters to be optimzed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;重要！我們用 $\theta$ 代表這個 neural network.  這個 $\theta$ neural network 的方向是從 hidden variable $Pa(\mathbf{x})$ 到 observations $\mathbf{x}$.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;deep-learning-latent-variable-model-dlvm-tractable-and-intractable&quot;&gt;Deep (Learning) Latent Variable Model (DLVM) Tractable and Intractable&lt;/h4&gt;

&lt;p&gt;以下我們用 hand-waving 方法說明幾個&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) d \mathbf{z}&lt;/script&gt;

&lt;p&gt;The above equation is the marginal likelihood or the model evidence, when taken as a function of $\theta$&lt;/p&gt;

&lt;p&gt;$\theta$ 代表這個 neural network.  這個 $\theta$ neural network 的方向是從 hidden variable $\mathbf{z}$ 到 observations $\mathbf{x}$.&lt;/p&gt;

&lt;p&gt;$p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})$ : joint distribution is tractable because it includes both evidence and latent&lt;/p&gt;

&lt;p&gt;$p_{\boldsymbol{\theta}}(\mathbf{x})$ : marginal likelihood is intractable in DLVM; 因此上式的積分也是 intractable&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})=p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})&lt;/script&gt;

&lt;p&gt;$p_{\boldsymbol{\theta}}(\mathbf{z})$ and $p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$ : prior and likelihood 一般 tractable because the joint distribution is tractable.  一般 prior 和 likelihood 是 tractable.&lt;/p&gt;

&lt;p&gt;$p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})$: posterior is intractable in DLVM because marginal likelihood is intractable&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})=\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{x})}&lt;/script&gt;

&lt;p&gt;In summary&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Joint distribution, prior, likelihood 通常是 tractable, 甚至有 analytic solution.&lt;/li&gt;
  &lt;li&gt;Marginal likelihood, posterior 通常是 intractable, 需要解但只有 approximate solution.
    &lt;ul&gt;
      &lt;li&gt;Posterior $p(z\mid x)$ =&amp;gt; discriminative problem!   given high dimension x to get a low dimension z, or $\theta$&lt;/li&gt;
      &lt;li&gt;Marginal likelihood p(x) =&amp;gt; generative problem!  generate a high dimension x; or sometimes given a low dimension z to generate dimensional x (conditional generative model)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下是一個例子。&lt;/p&gt;

&lt;h4 id=&quot;example-3-multivariate-bernoulli-data-3-產生-conditional-distribution-的方法和-2-一樣&quot;&gt;Example 3: Multivariate Bernoulli data (3 產生 conditional distribution 的方法和 2 一樣)&lt;/h4&gt;

&lt;p&gt;一個簡單的例子說明 hand-waving 的 assertion for the DLVM.&lt;/p&gt;

&lt;p&gt;Prior $p(z)$ 是簡單的 normal distribution.   Neural network 把 random sample $z$ 轉換成 $\mathbf{p}$, 再來 $\mathbf{p}$ 直接變成 Bernoulli distribution!  就像例三的 softmax 一樣。&lt;/p&gt;

&lt;p&gt;Likelihood $\log p(x\mid z)$ 因此也是簡單的 cross-entropy, i.e. maximum likelihood ~ minimum cross-entropy loss&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(\mathbf{z}) &amp;=\mathcal{N}(\mathbf{z} ; 0, \mathbf{I}) \\
\mathbf{p} &amp;=\text { DecoderNeuralNet }_{\boldsymbol{\theta}}(\mathbf{z}) \\
\log p(\mathbf{x} \mid \mathbf{z}) &amp;=\sum_{j=1}^{D} \log p\left(x_{j} \mid \mathbf{z}\right)=\sum_{j=1}^{D} \log \operatorname{Bernoulli}\left(x_{j} ; p_{j}\right) \\
&amp;=\sum_{j=1}^{D} x_{j} \log p_{j}+\left(1-x_{j}\right) \log \left(1-p_{j}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\forall p_j \in \mathbf{p}: 0 \le p_j \le 1$&lt;/p&gt;

&lt;p&gt;Joint distribution  $p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})=p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$​ 就是把兩者乘積。雖然看起來 messy, 還夾著 neural network, 但理論上 straightforward, 甚至可以寫出 analytical form.&lt;/p&gt;

&lt;p&gt;但反過來:  posterior $p(z\mid x)$,  marginal likelihood $p(x)$  即使在這麼簡單的 network, 都是難啃的骨頭！&lt;/p&gt;

&lt;h2 id=&quot;vae-and-dlvm&quot;&gt;VAE and DLVM&lt;/h2&gt;

&lt;p&gt;前面提到  基本就是把 intractable posterior inference and learning problem.&lt;/p&gt;

&lt;p&gt;Marginal likelihood, posterior 通常是 intractable, 需要解但只有 approximate solution.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Posterior $p(z\mid x)$ =&amp;gt; discriminative problem!   given high dimension x to get a low dimension z&lt;/li&gt;
  &lt;li&gt;Marginal likelihood p(x) =&amp;gt; generative problem!  generate a high dimension x; or sometimes given a low dimension z to generate dimensional x (conditional generative model)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先 target posterior $p_{\theta}(\mathbf{z}\mid \mathbf{x})$ :  注意，此處 $\theta$ 代表的 neural network (weights) from $\mathbf{z}$ to $\mathbf{x}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;引入 encoder neural network&lt;/strong&gt; $q_{\phi}(\mathbf{z}\mid x)$：注意，此處 $\phi$ 代表 neural network from $\mathbf{x}$ to $\mathbf{z}$.&lt;/p&gt;

&lt;p&gt;我們希望 optimize the variational parameter $\phi$ such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \approx p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})&lt;/script&gt;

&lt;p&gt;就是讓 (tractable) encoder 近似 (intractable) posterior.&lt;/p&gt;

&lt;p&gt;現在問題是：這個 neural network 長得怎麼樣？以及如何把 deterministic neural network 轉換成 probabilistic distribution?&lt;/p&gt;

&lt;h4 id=&quot;example-4given-input-經過-deterministic-nn-轉成-parameters-of-a-random-variable-to-create-conditional-distribution-eg-vae&quot;&gt;Example 4：Given Input 經過 Deterministic NN 轉成 Parameters of A Random Variable to Create Conditional Distribution (e.g. VAE)&lt;/h4&gt;

&lt;p&gt;Example 2 and 3 NN 產生 conditional distribution 的方式只能用在 discrete distribution.   對於 continuous distribution, NN 無法產生無限長的 distribution!  例如 VAE 使用 Normal distribution 如下：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
(\boldsymbol{\mu}, \log \boldsymbol{\sigma}) &amp;=\text { EncoderNeuralNet }_{\boldsymbol{\phi}}(\mathbf{x}) \\
q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) &amp;=\mathcal{N}(\mathbf{z} ; \boldsymbol{\mu}, \operatorname{diag}(\boldsymbol{\sigma}))
\end{aligned} %]]&gt;&lt;/script&gt;
Neural network 產生 $\mu, \log \sigma$ for normal distribution.  雖然這解決 deterministic to probabilistic 問題。但聽起來還是有點魔幻寫實方式把 deterministic to probabilistic.  這是 VAE 的實際做法。&lt;/p&gt;

&lt;p&gt;雖然的確產生 conditional distribtuion, 但似乎比直接產生 distribution 更不直觀！例如為什麼是 $\log \sigma$, 不是 $\sigma$ 或 $1/\sigma$ ? 另外只產生 $\mu, \log \sigma$ 兩個 parameters, 是否太簡化？  比起 softmax distribution 可能包含 10-100 parameters.&lt;/p&gt;

&lt;p&gt;Before we can answer this question, let me quote below and move on to algorithm.&lt;/p&gt;

&lt;p&gt;Typically, we use a single encoder neural network to perform posterior inference over all of the datapoints in our dataset. This can be contrasted to more traditional variational inference methods where the variational parameters are not shared, but instead separately and iteratively optimized per datapoint. The strategy used in VAEs of sharing variational parameters across datapoints is also called amortized variational inference (Gershman and Goodman, 2014). With amortized inference we can avoid a per-datapoint optimization loop, and leverage the efficiency of SGD.&lt;/p&gt;

&lt;h4 id=&quot;example-5-decoder--how-to-explain-pxmid-z-的-conditional-distribution&quot;&gt;Example 5: Decoder:  How to explain $p(x\mid z)$ 的 conditional distribution?&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&quot;&gt;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Let’s now make the assumption that p(z) is a standard Gaussian distribution and that p(x&lt;/td&gt;
      &lt;td&gt;z) is a Gaussian distribution whose mean is defined by a deterministic function f of the variable of z and whose covariance matrix has the form of a positive constant c that multiplies the identity matrix I. The function f is assumed to belong to a family of functions denoted F that is left unspecified for the moment and that will be chosen later. Thus, we have (不是很 make sense!)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}(\boldsymbol{f(z)}) &amp;=\text { EncoderNeuralNet }_{\boldsymbol{\theta}}(\mathbf{z}) \\p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z}) &amp;=\mathcal{N}(\mathbf{x} ; \boldsymbol{f(z)}, c)\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;p(z) \equiv \mathcal{N}(0, I) \\
&amp;p(x \mid z) \equiv \mathcal{N}(f(z), c I) \quad f \in F \quad c&gt;0
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;似乎只能 heuristically 解釋，沒有很深的 math fondation.&lt;/p&gt;

&lt;h2 id=&quot;比較-variational-em-and-vae-algorithm&quot;&gt;比較 (Variational) EM and VAE Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;recap-variational-em-algorithm&quot;&gt;Recap (Variational) EM algorithm&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; (ML) Estimate $\theta$ of $\arg \max_{\theta} \ln p(x;\theta)$  from posterior $p(z\mid x; \theta)$.&lt;/p&gt;

&lt;p&gt;Step 1: 為了 estimate $\theta$ 引入 hidden random variable $z$, log marginal likelihood (negative):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(\mathbf{x} \mid \boldsymbol{\theta}) &amp;= \mathcal{L}(q, \boldsymbol{\theta}) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta}) ) \\
&amp;= \underbrace{\sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})}{q(\mathbf{z})}}_{\text{ELBO}} + \underbrace{D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta}) )}_{\text{Gap of posterior}} \\
&amp;= \underbrace{\sum_{\mathbf{z}} q(\mathbf{z}) \ln p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta}) + \sum_{\mathbf{z}} -q(\mathbf{z}) \ln {q(\mathbf{z})}}_{\text{ELBO}}+ \underbrace{D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta}) )}_{\text{Gap of posterior}} \\
&amp;= \underbrace{E_{q(z)} \ln p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta}) + H(q)}_{\text{ELBO}} + \underbrace{D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta}) )}_{\text{Gap of posterior}} \\
&amp;= \underbrace{Q(q | \theta) + H(q)}_{\text{ELBO}} + \underbrace{D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta}) )}_{\text{Gap of posterior}} \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;第一項 (negative) 加第二項 (self-entropy of q, positive) 稱為 ELBO. 第三項稱為 gap (positive).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log Marginal Likelihood = ELBO + KL Gap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Or another formulation (same as above but better notation to compare with DLVM or VAE)&lt;/p&gt;

&lt;p&gt;Let’s start with EM algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(\mathbf{x} ; \boldsymbol{\theta})&amp;=\mathcal{L}(q, \boldsymbol{\theta})+K L(q \| p) \\
\mathcal{L}(q, \boldsymbol{\theta}) &amp;= \int q(\mathbf{z}) \ln \left(\frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}\right) d \mathbf{z} \\
\mathrm{KL}(q \| p)&amp;= \int q(\mathbf{z}) \ln \left(\frac{p(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta})}{q(\mathbf{z})}\right) d \mathbf{z}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Step 2: 假設 posterior $p(z\mid x)$ 有 analytic soluiton, e.g. GMM 的 posterior 是 softmax funtion.&lt;/p&gt;

&lt;p&gt;We let $q(z) = p(z \mid x )$  and define the  $Q$ function (log joint distribution integration over posterior)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right) &amp;=\int p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{\text {OLD }}\right) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) d \mathbf{z} \nonumber\\
&amp;=\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{0 \mathrm{LD}}\right)} \\
&amp;=E_{z\sim p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{0 \mathrm{LD}}\right)} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Log Marginal Likelihood = ELBO + KL Gap&lt;/strong&gt;
&lt;strong&gt;ELBO = Q function (negative value) + self-entropy (postive value)&lt;/strong&gt;
&lt;strong&gt;Q Function = log joint distribution (tractable) expectation over (approx.) posterior&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;此時可以用定義 EM algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{E-step, Minimize KL Gap : Compute}\quad &amp;p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{\mathrm{OLD}}\right)\,\text{and}\,Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right)\\
\text{M-step, Maximize ELBO : Evaluate}\quad &amp;\boldsymbol{\theta}^{\mathrm{NEW}}=\underset{\boldsymbol{\theta}}{\arg \max } Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;一般 joint distribution $p\left(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}\right)$ 包含完整的 data，容易計算或有 analytical solution.
大多的問題是 conditional or posterior distribution 是否有 analytical solution.&lt;/p&gt;

&lt;h3 id=&quot;vae&quot;&gt;VAE&lt;/h3&gt;

&lt;p&gt;主要參考 [@kingmaIntroductionVariational2019].&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210901154112484.png&quot; alt=&quot;image-20210901154112484&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal A:&lt;/strong&gt; get the marginal likelihood:  $\ln_{\theta} p(x)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal B:&lt;/strong&gt; get the $\theta$ (and decoder $\phi$) is to $\arg \max_{\theta} \ln p_{\theta}(x)$&lt;/p&gt;

&lt;p&gt;Step 1: same as above (引入 hidden random variable $z$ and decoder NN $\theta$)&lt;/p&gt;

&lt;p&gt;Step 2: 因為 posterior intractable, 引入另一個 encoder neural network ($\phi$) which is tractable&lt;/p&gt;

&lt;h3 id=&quot;em-algorithm-和-vae-的差別&quot;&gt;EM algorithm 和 VAE 的差別&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;EM posterior is tractable (Q funciton);  VAE posterior is intractable (沒有 analytical form). 我們用另一個 (tractable) neural network $\phi$ 去近似 (intractable) posterior.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\log p_{\boldsymbol{\theta}}(\mathbf{x}) &amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x})\right] \\
&amp;=\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right] \\
&amp;=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \frac{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right] \\
&amp;=\underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=\mathcal{L}_{\theta,\phi}{(\boldsymbol{x}})}+\underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{q_{\boldsymbol{x}}(\mathbf{z} \mid \mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}_{=D_{K L}\left(q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \| p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\right)}
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;把所有 EM 的 $q(z)$  變成 $q_{\phi}(z\mid x)$.    兩者完全一致&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Log Marginal Likelihood = ELBO + KL Gap.&lt;/strong&gt;  兩者完全一致&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;第一項是 ELBO, $\mathcal{L}&lt;em&gt;{\theta,\phi}{(\boldsymbol{x}})$, 第二項是 KL divergence gap, $D&lt;/em&gt;{K L} \ge 0$.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;KL divergence 決定近似的 NN 和 true posterior 距離多遠。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;KL divergence gap 也決定 ELBO bound 的 tightness.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EM Training 方法：（&lt;strong&gt;假設 posterior is tractable&lt;/strong&gt;）&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;E-step: &lt;strong&gt;update posterior&lt;/strong&gt; ( tractable $q=p(z\mid x)$ ) to &lt;strong&gt;minimize KL gap&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;M-step: &lt;strong&gt;update parameter&lt;/strong&gt; $\theta$ to &lt;strong&gt;maximize ELBO/Marginal likelihood&lt;/strong&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;E-step and M-step Iterative update 永遠會增加 ELBO, 但這不一定是好事！很有可能會卡在 local maximum, 需要多個 initial condition to avoid some local maximum.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VAE 的 posterior is intractable, 但巧妙的利用 encoder ($\phi$) + decoder ($\theta$) structure.  可以用原來的 image 為 golden 做 self-supervise learning.  使用 SGD 於多張 images to back-propagation &lt;strong&gt;同時 update&lt;/strong&gt; $\theta, \phi$  (&lt;strong&gt;這和 EM 不同，一石二鳥&lt;/strong&gt;)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Log Marginal Likelihood = ELBO + KL Gap  $\to$  ELBO = Log Marginal Likelihood - KL Gap&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Update $\theta$ and $\phi$  to &lt;strong&gt;maximize ELBO implies maximize the marginal likelihood&lt;/strong&gt;,  equivalent to M-step in EM.&lt;/li&gt;
      &lt;li&gt;NN $\phi$  近似 posterior ($q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) \approx p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})$), &lt;strong&gt;update $\phi$ implies to minimize KL gap&lt;/strong&gt;, equivalent to E-step in EM.&lt;/li&gt;
      &lt;li&gt;VAE 使用 SGD with mini-batch training iteratively,  並不保證 ELBO 永遠會增加 (or loss function 永遠變小)，但可以 leverage neural network trainging 的經驗，似乎收斂性還不錯，雖然無法證明 global 收斂性, 但不至於卡在太差的 local minimum.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210901180808893.png&quot; alt=&quot;image-20210901180808893&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VAE 和 AE neural network 不同，中間還卡了一個 random variable $z$!  如何 back-propagation 穿過 $z$? Reparameterization Trick!&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;question-maximize-elbo-等價-minimize-gap-between-posterior-and-q&quot;&gt;Question: Maximize ELBO 等價 Minimize GAP between posterior and q?&lt;/h5&gt;

&lt;p&gt;在 EM 這是兩件事：E-step: update posterior q = .. to minimize the gap between ;   M-step: update $\theta$  to maximize ELBO or the simplified version Q function (joint distribution over posterior distribution, remove self-entropy from ELBO)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log Marginal Likelihood = ELBO + KL Gap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ELBO = Q function (negative value) + self-entropy (postive value)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q Function = log joint distribution (tractable) expectation over (approx.) posterior&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在 VAE 似乎是同一件事，let’s take a look of minimize KL gap between posterior and approx. q.&lt;/p&gt;

&lt;p&gt;此處 $g^&lt;em&gt;= \mu$ and $h^&lt;/em&gt; = \log \sigma$,  $g^&lt;em&gt;$ and $h^&lt;/em&gt;$ 其實就是 $\phi$&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\left(g^{*}, h^{*}\right) &amp;=\underset{(g, h) \in G \times H}{\arg \min } K L\left(q_{x}(z), p(z \mid x)\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \min }\left(\mathbb{E}_{z \sim q_{x}}\left(\log q_{x}(z)\right)-\mathbb{E}_{z \sim q_{x}}\left(\log \frac{p(x \mid z) p(z)}{p(x)}\right)\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \min }\left(\mathbb{E}_{z \sim q_{x}}\left(\log q_{x}(z)\right)-\mathbb{E}_{z \sim q_{z}}(\log p(z))-\mathbb{E}_{z \sim q_{x}}(\log p(x \mid z))+\mathbb{E}_{z \sim q_{x}}(\log p(x))\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \max }\left(\mathbb{E}_{z \sim q_{x}}(\log p(x \mid z))-K L\left(q_{x}(z), p(z)\right)\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \max }\left(\mathbb{E}_{z \sim q_{x}}\left(-\frac{\|x-f(z)\|^{2}}{2 c}\right)-K L\left(q_{x}(z), p(z)\right)\right)
\end{align} %]]&gt;&lt;/script&gt;
這個結果好像跟下面 maximize ELBO 的結論一樣？？&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;結論一： 從 joint pdf 出發 (ELBO)&lt;/li&gt;
  &lt;li&gt;結論二：從 conditional pdf 出發 (posterior)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;vae-的-loss-function&quot;&gt;VAE 的 Loss Function&lt;/h3&gt;

&lt;p&gt;標準 bayesian formulated VAE 的 loss function for a specific $x_i$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_{i}(\theta, \phi)=-E_{z \sim q_{\phi}\left(z | x_{i}\right)}\left[\log p_{\theta}(x_{i} | z)\right]+K L\left(q_{\phi}(z | x_{i}) \|\,p(z)\right)&lt;/script&gt;

&lt;p&gt;數學等價上面的 ELBO x (-1)：&lt;/p&gt;

&lt;p&gt;$\underbrace{\mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}&lt;em&gt;{=\mathcal{L}&lt;/em&gt;{\theta,\phi}{(\boldsymbol{x}})}$&lt;/p&gt;

&lt;p&gt;$= {\mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}$&lt;/p&gt;

&lt;p&gt;$= {\mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z}) p(z)}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})p(z)}\right]\right]} = {\mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{p(z)}\right]\right]} + {\mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[\frac{ p(z)}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\right]\right]}$&lt;/p&gt;

&lt;p&gt;$ = {\mathbb{E}&lt;em&gt;{q&lt;/em&gt;{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}\left[\log \left[{p_{\boldsymbol{\theta}}(\mathbf{x}\mid \mathbf{z})}\right]\right]} - K L  { \left[{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) | { p(z)}}\right]}$&lt;/p&gt;

&lt;h4 id=&quot;normal-distribution-assumption&quot;&gt;Normal Distribution Assumption&lt;/h4&gt;

&lt;h5 id=&quot;假設-pz-px--z-為-normal-distribution-vae-的-elbo-可以近似為&quot;&gt;假設 p(z)， p(x | z) 為 Normal distribution, VAE 的 ELBO 可以近似為&lt;/h5&gt;

&lt;p&gt;參考 &lt;a href=&quot;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&quot;&gt;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{z \sim q_{\phi}(z\mid x)}\left(-\frac{\|x-f(z)\|^{2}}{2 c}\right)-K L\left(q_{\phi}(z\mid x)\| p(z)\right)&lt;/script&gt;
第二項假設 prior p(z) and posterior q(z|x) 為 normal distribution, 有 close form.&lt;/p&gt;

&lt;p&gt;ELBO x (-1) 變成 VAE loss function.  此時拆解和解釋和 EM 有些不同。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;EM ELBO 留下 Q function of joint distribution，discard self-entropy independent of parameter.  因為我們目標是&lt;/strong&gt; $\arg \max_{\theta} Q$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;VAE ELBO loss 第一項則是 reconstruction loss; 第二項代表 regularization.  兩者是互相 balance, 而不是 minimize gap!&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;如果 input/output loss 很小，代表 variance 接近 0。 此時 regularization loss 變大，這是 overfit case like conventional autoencoder, not good.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;如果 regularization 很小，代表 variance 接近 1。此時 reconstruction loss 變大。 encoding or decoding 就不好。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Log Marginal Likelihood = ELBO + KL Gap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ELBO (negative value) = Q function (negative value) + self-entropy (postive value).&lt;/strong&gt; (for EM)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;-1 x ELBO = Loss (positive value) = reconstruction loss (positive value) + regularization loss (positive value).&lt;/strong&gt;  (for VAE)&lt;/p&gt;

&lt;p&gt;Very important:  maximize ELBO = minimize gap between posterior and q!!! (by xxx)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\left(g^{*}, h^{*}\right) &amp;=\underset{(g, h) \in G \times H}{\arg \min } K L\left(q_{x}(z), p(z \mid x)\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \min }\left(\mathbb{E}_{z \sim q_{x}}\left(\log q_{x}(z)\right)-\mathbb{E}_{z \sim q_{x}}\left(\log \frac{p(x \mid z) p(z)}{p(x)}\right)\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \min }\left(\mathbb{E}_{z \sim q_{x}}\left(\log q_{x}(z)\right)-\mathbb{E}_{z \sim q_{z}}(\log p(z))-\mathbb{E}_{z \sim q_{x}}(\log p(x \mid z))+\mathbb{E}_{z \sim q_{x}}(\log p(x))\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \max }\left(\mathbb{E}_{z \sim q_{x}}(\log p(x \mid z))-K L\left(q_{x}(z), p(z)\right)\right) \\
&amp;=\underset{(g, h) \in G \times H}{\arg \max }\left(\mathbb{E}_{z \sim q_{x}}\left(-\frac{\|x-f(z)\|^{2}}{2 c}\right)-K L\left(q_{x}(z), p(z)\right)\right)
\end{align} %]]&gt;&lt;/script&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="ML" /><category term="VAE" /><category term="Autoencoder" /><category term="Variational" /><category term="EM" /><summary type="html"></summary></entry><entry><title type="html">Math ML - Maximum Likelihood Vs. Bayesian</title><link href="http://localhost:4000/ai/2021/08/17/Math_ML_Bayesian/" rel="alternate" type="text/html" title="Math ML - Maximum Likelihood Vs. Bayesian" /><published>2021-08-17T00:00:00+08:00</published><updated>2021-08-17T00:00:00+08:00</updated><id>http://localhost:4000/ai/2021/08/17/Math_ML_Bayesian</id><content type="html" xml:base="http://localhost:4000/ai/2021/08/17/Math_ML_Bayesian/">&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;[@poczosCllusteringEM2015]&lt;/li&gt;
  &lt;li&gt;[@matasExpectationMaximization2018] good reference&lt;/li&gt;
  &lt;li&gt;[@choyExpectationMaximization2017]&lt;/li&gt;
  &lt;li&gt;[@tzikasVariationalApproximation2008] excellent introductory paper&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;maximum-likelihood-estimation-vs-bayesian-inference&quot;&gt;Maximum Likelihood Estimation Vs. Bayesian Inference&lt;/h2&gt;

&lt;p&gt;ML estimation 和 Bayesian inference 到底有什麼差別？簡單說 ML estimation 把 unknown/hidden 視為 a &lt;strong&gt;“fixed parameter”&lt;/strong&gt;.  Bayesian inference 把 unknown/hidden 視為 &lt;strong&gt;“distribution”&lt;/strong&gt; described by a random variable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bernoulli distribution&lt;/em&gt;：投擲硬幣正面的機率 $\theta$, 反面的機率 $1-\theta$. 連續投擲的正面/反面的次數分別是 x/(n-x).  Likelihood function, 其實就是 probability distribution  為&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x; \theta) = p(x ; \theta) = \theta^{x}(1-\theta)^{n-x}&lt;/script&gt;

&lt;p&gt;有時候我們也把 $p(x;\theta)$ 寫成 conditional distribution 形式 $p(x\mid\theta).$​  嚴格來說並不對。不過可以視為 Bayesian 詮釋的擴展。&lt;/p&gt;

&lt;p&gt;ML estimation 做法是微分上式，解 $\theta$ parameter.&lt;/p&gt;

&lt;p&gt;Bayesian 的觀念是: (1) $\theta$ 視為 hidden random variable; (2) 引入 hidden random variable $z$ with $\theta$ as a parameter.&lt;/p&gt;

&lt;p&gt;我們假設 (1), 利用 Bayes formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta | x) = \frac{p(x | \theta) p(\theta)}{p(x)}&lt;/script&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z | x; \theta ) = \frac{p(x | z; \theta) p(z; \theta)}{p(x)}&lt;/script&gt;

&lt;p&gt;&lt;u&gt;上式的術語和解讀&lt;/u&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Random variable $x$ :  post (事後) observations, (post) evidence. $p(x)$ 稱為 evidence distribution or marginal likelihood.&lt;/li&gt;
  &lt;li&gt;Random variable $\theta$ : 相對於 $x$, $\theta$ 是 prior (事前, 先驗) 並且是 hidden variable (i.e. not evidence).  擴展我們在 maximum likelihood 的定義，從 parameter 變成 random variable.  &lt;strong&gt;$p(\theta)$​​ 稱為 prior distribution.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;注意 prior 是 distribution&lt;/strong&gt;,  不會出現在 ML, 因為 $\theta$​ 在 ML 是 parameter.  只有在 Bayesian 才有 prior (distribution)!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conditional distribution $p(x\mid\theta)$ :  likelihood (或然率)。擴展我們在 maximum likelihood 的定義，從 parameter dependent distribution or function 變成 conditional distribution.&lt;/li&gt;
  &lt;li&gt;Conditional distribution $p(\theta\mid x)$ ： &lt;strong&gt;posterior, 事後機率。就是我們想要求解的東西。&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;注意 posterior 是 conditional distribution&lt;/strong&gt;.  有人會以為 $p(\theta)$ 是 prior distribution, $p(x)$​ 是 posterior distribution. Wrong!&lt;/li&gt;
      &lt;li&gt;Posterior 不會出現在 ML, 因為 $\theta$​ 在 ML 是 parameter.  只有在 Bayesian 才會討論 posterior (distribution)!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;簡言之：Posterior&lt;/strong&gt; $\propto$ &lt;strong&gt;Likelihood x Prior&lt;/strong&gt; $\to p(\theta \mid x) \propto {p(x \mid \theta) \times p(\theta)}$
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;一般我們忽略 $p(x)$ ，因為它和要 estimate 的 $\theta$​​ distribution (or parameter) 無關，視為常數忽略。&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;很好記: 事後 = 事前 x 喜歡 (likelihood).  如果很喜歡，才會有事後。如果不喜歡，事後不理 (0分)&lt;/li&gt;
      &lt;li&gt;Prior 和 posterior (事前/先驗，事後) 都是 Bayesian 才有的說法。 ML (or Frequentist) 不會有 prior and posterior 說法。&lt;/li&gt;
      &lt;li&gt;以通信為例，$z$ 是 transmitted signal (unknown),  $x$ 是 received signal,  $x = z + n$,  是 transmitted signal 加 noise.  如果只根據 $p(\text{received signal}\mid\text{transmitted signal}) = p(x\mid z)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;事前事後哪一個重要&quot;&gt;事前、事後，哪一個重要？&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;是否注意到一件很矛盾的事？要估計 posterior (事後),&lt;/strong&gt;  $p(\theta\mid x)$​​, &lt;strong&gt;必須要有 prior (事前),&lt;/strong&gt; $p(\theta)$​.&lt;/p&gt;

&lt;p&gt;那如果都已經有 $p(\theta)$​ 的 distribution, 就可以直接 estimate $\theta$​ 的特性 (e.g. mean, variance), 還需要 posterior 嗎？&lt;/p&gt;

&lt;p&gt;有兩個 answers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Bayesian 相信 evidence!  Prior 只是沒有 evidence 的一種猜測。不可靠的 prior 在更多的 evidence 後會轉變成更可靠的 posterior!&lt;/li&gt;
  &lt;li&gt;大多數情況，我們並不關心 prior 的 distribution, 而是關心 likelihood or posterior distribution!
    &lt;ul&gt;
      &lt;li&gt;在 ML estimation, 我們只關心 &lt;strong&gt;the specific $\theta$  (not distribution) to maximize the likelihood.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;在 ML extension to EM algorithm, 我們我們只關心 &lt;strong&gt;the specific $\theta$  to maximize $Q$ function&lt;/strong&gt; &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
      &lt;li&gt;在窮人的 Bayesian inference, MAP (Maximum A Posteriori) estimation,  我們只關心 &lt;strong&gt;posterior distribution 的 maximum.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;在 Bayesian inference, 同樣我們關心的是 &lt;strong&gt;posterior distribution&lt;/strong&gt; (例如 EAP - Expected A Posteriori), 而非 prior.&lt;/li&gt;
      &lt;li&gt;以實際應用：一般通信使用 $p(x\mid z)$, i.e. maximum likelihood; 或者 $p(z\mid x)$, i.e. MAP, to decode each bit information!   通常我們不需要 $p(z)$ ，除了偶爾在 MAP 會用到。一般我們假設 $p(z)$ by default, e.g. uniform distribution in communication.&lt;/li&gt;
      &lt;li&gt;在 ML 應用，Dirichlet, Gaussian, or W-Gaussian prior distribution 通常用於 default setting.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;以 Bayesian 而言，posterior (事後) 遠比 prior (事前) 重要！&lt;/strong&gt;  &lt;strong&gt;甚至  Posterior &amp;gt; Likelihood &amp;gt; Prior&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;所以針對 prior, 只要是合理的假設 (猜測)，一般都可以接受。因為 more evidence, $x$, 所得出的 posterior 會把 prior 的影響消除！&lt;/p&gt;

&lt;h2 id=&quot;真的-prior-information-先驗-怎麼辦&quot;&gt;真的 Prior Information (先驗) 怎麼辦?&lt;/h2&gt;

&lt;p&gt;Bayesian prior 只是一個 initial condition.  隨著 evidence 越多，posterior 逐漸 overtake prior.&lt;/p&gt;

&lt;p&gt;但如果有真的 prior information 如何處理，例如物理定律或者一些 rule (e.g. 左括號一定對應一個右括號)？&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Bayesian prior 的定義就是一個假設，並非是 hard rule.  不像哲學的先驗有拔高的地位。Bayesian 期待 rule 會從 evidence 學到。&lt;/li&gt;
  &lt;li&gt;如果 rule 無法反應在 evidence, 可能要考慮其他的 AI 方法，e.g. rule-based AI, or mixture model.&lt;/li&gt;
  &lt;li&gt;如果 rule 有反應在 evidence, 但 Bayesian 學不好。可以考慮 embedded the rule, e.g. rule violation penalty in the cost function during training, post-processing for hard rule, etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ml-em-map-and-bayesian-inference-difference&quot;&gt;ML, EM, MAP, and Bayesian Inference Difference&lt;/h2&gt;

&lt;p&gt;這幾種都是常見的 parameter estimator, 差別為何？&lt;/p&gt;

&lt;h4 id=&quot;ml-maximum-likelihood-estimator&quot;&gt;ML (Maximum likelihood) Estimator&lt;/h4&gt;

&lt;p&gt;$\theta_{MLE} = \arg_{\theta} \max  p(x\mid\theta)$   還是強調一下此處 $\theta$ 是 parameter, 不是 conditional distribution 中的 random variable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; (1) consistency, converges in probability to its true value; (2) almost unbiased; (3) 2nd order efficiency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; (1) point estimator, sensitive to assumption of distribution and parameter.&lt;/p&gt;

&lt;p&gt;另一個 ML twist 可能更常見：maximum log-likelihood estimator (MLL).  基本和 ML 等價。&lt;/p&gt;

&lt;p&gt;$\theta_{MLLE} = \arg_{\theta} \max  \log p(x\mid\theta)$&lt;/p&gt;

&lt;p&gt;Maximization of the log-likelihood criterion is equivalent to minimization of a Kullback Leibler divergence between the data and model distributions.&lt;/p&gt;

&lt;h4 id=&quot;em-estimator-extension-of-ml-for-hidden-data&quot;&gt;EM Estimator (Extension of ML for Hidden Data)&lt;/h4&gt;

&lt;p&gt;$\boldsymbol{\theta}^{(t+1)}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} Q(\boldsymbol{\theta}^{t+1} \mid \boldsymbol{\theta}^{t})$ &lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;    iteratively get the ML estimation of parameter&lt;/p&gt;

&lt;p&gt;Q function 包含 posterior of hidden variable $z$,  已經半步 bayesian!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; (1) point estimator, sensitive to assumption of distribution and parameter.&lt;/p&gt;

&lt;h4 id=&quot;map-maximum-a-posteriori-estimator&quot;&gt;MAP (Maximum A Posteriori) Estimator&lt;/h4&gt;

&lt;p&gt;$\theta_{MAP} =\arg_{\theta} \max p(\theta\mid x) = \arg_{\theta} \max p(x\mid\theta) p(\theta)$   此處 $\theta$ 是 random variable.&lt;/p&gt;

&lt;p&gt;窮人的 bayesian: 利用 posterior, 但只取 maximum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;  unknown is a distribution instead of a fixed parameter, better for the non-stationary circumstance&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;  (1) still point estimator, still sensitive to assumption?  (2) biased?&lt;/p&gt;

&lt;h4 id=&quot;bayesian-inference&quot;&gt;Bayesian Inference&lt;/h4&gt;

&lt;p&gt;Bayesian inference 的精神就是 posterior distribution.  至於從 posterior 再找 maximum (MAP), 或是平均 (EAP)&lt;/p&gt;

&lt;p&gt;$\theta_{EAP} =E[\theta\mid x]$, 或是 marginal distribution,  或是再進一步做 parameter estimation (e.g. EM) or variational inference, 都屬於 bayesian inference.  此處先不討論。&lt;/p&gt;

&lt;h3 id=&quot;bayesian-inference-and-directed-acyclic-graph-dag&quot;&gt;Bayesian Inference and Directed Acyclic Graph (DAG)&lt;/h3&gt;

&lt;p&gt;Bayesian inference 最有威力的部分是結合 DAG.  不然只是把簡單的問題複雜化。&lt;/p&gt;

&lt;p&gt;在 DAG model 中，可以一路用 conditional probablility back trace 到 root.  TBD&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/BNspA.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;$Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}) = \langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{0 \mathrm{LD}}\right)}$​ &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="ML" /><category term="EM" /><category term="Bayesian" /><category term="MAP" /><summary type="html">Reference [@poczosCllusteringEM2015] [@matasExpectationMaximization2018] good reference [@choyExpectationMaximization2017] [@tzikasVariationalApproximation2008] excellent introductory paper</summary></entry><entry><title type="html">Math AI - From EM to Variational Bayesian Inference</title><link href="http://localhost:4000/ai/2021/08/16/Math_AI_Baysian_variational/" rel="alternate" type="text/html" title="Math AI - From EM to Variational Bayesian Inference" /><published>2021-08-16T07:10:08+08:00</published><updated>2021-08-16T07:10:08+08:00</updated><id>http://localhost:4000/ai/2021/08/16/Math_AI_Baysian_variational</id><content type="html" xml:base="http://localhost:4000/ai/2021/08/16/Math_AI_Baysian_variational/">&lt;script id=&quot;MathJax-script&quot; async=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
//MathJax.Hub.Config({
//  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
//});
&lt;/script&gt;

&lt;h2 id=&quot;main-reference&quot;&gt;Main Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;[@matasExpectationMaximization2018] : good reference&lt;/li&gt;
  &lt;li&gt;[@tzikasVariationalApproximation2008] : excellent introductory paper&lt;/li&gt;
  &lt;li&gt;[@wikiVariationalBayesian2021]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;em-algorithm&quot;&gt;EM Algorithm&lt;/h2&gt;

&lt;p&gt;EM 可以視為 MLE 的 extension to hidden state / data.&lt;/p&gt;

&lt;p&gt;Let’s start with EM algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(\mathbf{x} ; \boldsymbol{\theta})&amp;=F(q, \boldsymbol{\theta})+K L(q \| p) \\
F(q, \boldsymbol{\theta})&amp;=\int q(\mathbf{z}) \ln \left(\frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}\right) d \mathbf{z} \\
\mathrm{KL}(q \| p)&amp;= \int q(\mathbf{z}) \ln \left(\frac{p(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta})}{q(\mathbf{z})}\right) d \mathbf{z}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right) &amp;=\int p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{\text {OLD }}\right) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) d \mathbf{z} \nonumber\\
&amp;=\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{0 \mathrm{LD}}\right)} \label{eqQ}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;此時可以用 $\eqref{eqQ}$ 定義 EM algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{E-step : Compute}\quad &amp;p\left(\mathbf{z} \mid \mathbf{x} ; \boldsymbol{\theta}^{\mathrm{OLD}}\right) \label{eqE}\\
\text{M-step : Evaluate}\quad &amp;\boldsymbol{\theta}^{\mathrm{NEW}}=\underset{\boldsymbol{\theta}}{\arg \max } Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\mathrm{OLD}}\right) \label{eqM}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;一般 $\eqref{eqQ}$ 的 joint distribution $p\left(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}\right)$ 包含完整的 data，容易計算或有 analytical solution.
大多的問題是 $\eqref{eqE}$ conditional or posterior distribution 是否容易計算，是否有 analytical solution.&lt;/p&gt;

&lt;h2 id=&quot;variational-em-or-variational-bayesian-framework&quot;&gt;Variational EM or Variational Bayesian Framework&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q&amp;amp;A 這裡的思路和前文 variation EM minimize KL gap 似乎不同？&lt;/strong&gt;&lt;br /&gt;
A: 這裏定義的 variational EM 比較是一般的定義。思路還是 maximize ELBO (or F free energy function), 但採取 divide-and-conquer 方法。可以和 graph model 結合。前文定義比較有問題。&lt;/p&gt;

&lt;p&gt;最簡單的話就是 hidden variable $\mathbf{z} = [z_1, z_2,\cdots,z_M]$  and $p(\mathbf{z}) = p(z_1)\cdots p(z_M)$.
什麼時候會有這種 distribution product?  主要是來自 graph model, 後面會說明。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
q(\mathbf{z})=\prod_{i=1}^{M} q_{i}\left(z_{i}\right) \label{eqFactor}
\end{equation}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
F(q, \boldsymbol{\theta})=&amp; \int \prod_{i} q_{i}\left[\ln p (\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})-\sum_{i} \ln q_{i}\right] d \mathbf{z}\nonumber\\
=&amp; \int \prod_{i} q_{i} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \prod_{i} d z_{i} - \sum_{i} \int \prod_{j} q_{j} \ln q_{i} d z_{i} \nonumber\\
=&amp; \int q_{j}\left[\int \ln p (\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \prod_{i \neq j}\left(q_{i} d z_{i}\right)\right] d z_{j} -\int q_{j} \ln q_{j} d z_{j}-\sum_{i \neq j} \int q_{i} \ln q_{i} d z_{i} \nonumber\\
=&amp; \int q_{j} \ln \tilde{p} (\mathbf{x}, z_{j} ; \boldsymbol{\theta}) d z_{i}-\int q_{j} \ln q_{j} d z_{j} -\sum_{i \neq j} \int q_{i} \ln q_{i} d z_{i} \nonumber\\
=&amp;-\mathrm{KL}\left(q_{j} \| \tilde{p}\right)-\sum_{i \neq j} \int q_{i} \ln q_{i} d z \label{eqVarELBO}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\ln \tilde{p}\left(\mathbf{x}, z_{j} ; \boldsymbol{\theta}\right)=\langle\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})\rangle_{i \neq j} =E_{i \neq j} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) =\int \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \prod_{i \neq j}\left(q_{i} d z_{i}\right) \label{eqVarJ}
\end{equation}&lt;/script&gt;

&lt;p&gt;$\eqref{eqVarELBO}$ 是 (variational, 因為有 KL divergence) lower bound, KL divergence 必大於 0, 負號後必小於 0.  第二項加上負號是 self-entropy 必大於 0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q&amp;amp;A: 這裏 KL divergence between $q_j(z_j)$ and “joint distribution” $\tilde{p}(\mathbf{x}, z_{j} ; \boldsymbol{\theta})$, 似乎抵觸前文說的 KL divergence between $q(\mathbf{z})$ and joint distribution $p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})$ dimension 不對的問題&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
F(q, \boldsymbol{\theta})= \mathcal{L}(q, \boldsymbol{\theta})&amp;=\int_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})} d\mathbf{z}  \\
&amp;\ne - D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z}, \mathbf{x}; \boldsymbol{\theta}) )
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; $\eqref{eqVarJ}$ 的 “joint distribution” $\tilde{p}$ 不是真的 joint distribution. 重點 $\tilde{p}$ 是不是一個 distribution: (1) $\tilde{p} \ge 0$, and (2) $\int \tilde{p}(\mathbf{x}, z_j; \boldsymbol{\theta})\, dz_j = 1$ for any $\mathbf{x}$.  From $\eqref{eqVarJ}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{p}(\mathbf{x}, z_j; \boldsymbol{\theta}) = \exp(E_{i \neq j} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})) \ge 0&lt;/script&gt;

&lt;p&gt;滿足 (1).  我們主要檢查 (2), how to prove? TBD&lt;/p&gt;

&lt;p&gt;直觀看出讓 KL 為 0，就是 $q_j(z_j) = \tilde{p}(x, z_j; \theta)$, 似乎就是最大值 (how about the self-entropy?).
也就是 optimal distribution $q_j^* (z_j)$ 是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ln q_j^* \left(z_{j}\right)= E_{i \neq j} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) + \text{const.}&lt;/script&gt;

&lt;p&gt;上面的 const 可以由 distribution normalization 得到。所以我們可以得到一組 consistency conditions $\eqref{eqVarJ2}$ for the maximum of variational lower bound subject to $\eqref{eqFactor}$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
q_{j}^{*}\left(z_{j}\right)=\frac{\exp E_{i \neq j} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{\int \exp E_{i \neq j}\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) d z_{j}} \quad\text{for}\,\, j=1,\cdots,M \label{eqVarJ2}
\end{equation}&lt;/script&gt;

&lt;p&gt;$\eqref{eqVarJ2}$ 顯然不會有 explicit solution, 因為 $q_j$ factors 之間是相互 dependent.  A consistent solution 需要 cycling through these factors.  我們定義 Variational EM algorithm&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\text{Variational E-step : Evaluate}\quad &amp;q^{\mathrm{NEW}}(\mathbf{z})\quad\text{using above equations}\\
\text{Variational M-step : Find}\quad &amp;\boldsymbol{\theta}^{\mathrm{NEW}}=\underset{\boldsymbol{\theta}}{\arg \max } F\left(q^{\mathrm{NEW}}, \boldsymbol{\theta}\right) \label{eqM2}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;

&lt;h3 id=&quot;例一-linear-regression-filterestimate-a-noisy-signal&quot;&gt;例一： Linear Regression (filter/estimate a noisy signal)&lt;/h3&gt;

&lt;p&gt;我很喜歡這個例子。從簡單的 least-square error filter 進步到 Kalman filter.  類似的應用：deconvolution/equalization, channel estimation, speech recognition, frequency estimation, time series prediction, and
system identification.&lt;/p&gt;

&lt;h4 id=&quot;問題描述&quot;&gt;問題描述&lt;/h4&gt;

&lt;p&gt;考慮一個未知信號 $y(x) \in R, x \in \Omega ⊆ R^N$, i.e. $R^N \to R$.
我們想要 predict its value $t_* = y(x_&lt;em&gt;)$ at an arbitrary location $x_&lt;/em&gt; \in \Omega$.&lt;/p&gt;

&lt;p&gt;我們用 vector 表示 $(t_1, \cdots, t_N)$
 using a vector t = (t1,…, tN)T of N noisy observations tn = y(xn) + εn, at locations x = (x1,…, xN)T, xn ∈ , n = 1,…, N. The additive noise εn is commonly assumed to be independent, zero mean, Gaussian distributed:
&lt;script type=&quot;math/tex&quot;&gt;y(\mathbf{x})=\sum_{m=1}^{M} \omega_{m} \phi_{m}(\mathbf{x})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;注意 $y(x)$ 不是真正的 observables, 而是加上 noise 之後的 t 才是 observations.  我們的目標就是用 $\mathbf{t}$ 來 estimate $\mathbf{w}$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{t}=\boldsymbol{\Phi} \mathbf{w}+\boldsymbol{\varepsilon}&lt;/script&gt;

&lt;p&gt;The likelihood function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
p(\mathbf{t} ; \mathbf{w}, \beta)&amp;=N\left(\mathbf{t} \mid \mathbf{\Phi} \mathbf{w}, \beta^{-1} \mathbf{I}\right)\\
&amp;=(2 \pi)^{-\frac{N}{2}} \beta^{\frac{N}{2}} \exp \left(-\frac{\beta}{2}\|\mathbf{t}-\Phi \mathbf{w}\|^{2}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;三種解法圖式&quot;&gt;三種解法圖式&lt;/h4&gt;

&lt;p&gt;以下我們用三種 methodologies 用 $\mathbf{t}$ 來 estimate $\mathbf{w}$ (i.e. signal) and $\beta$ (i.e. noise if needed).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Method 1:&lt;/em&gt; ML Estimation
如果 number of parameters (w) is the same as the number of observations (t), the ML estimates are very sensitive to the model noise.  我們可以用 DAG (Directed Acyclic Graphic) 說明，如下圖 (a).  雙圓框 t 代表 observed random variable. 方框 (W, beta) 代表 parameter to be estimated.  單圓框（e.g. (b) W）代表 hidden random variable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Method 2:&lt;/em&gt; 假設 weight W 是 random variable with imposed prior. 我們先用 a simple Bayesian model with stationary Gaussian prior on weight, 如下圖 (b).  以這個 model 而言，我們用 EM algorithm performs Bayesian inference.  結果 robust to noise, 類似 Kalman filter?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16286850167880.jpg&quot; width=&quot;414&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Method 3:&lt;/em&gt; method 2 的一個缺點是假設 stationary Gaussian noise (i.e. $\beta$, a fixed value to be estimated, 無法 capture the local signal properties.  我們可以引入更複雜 spatially/temporally varying hierarchical model which is based on a non-stationary Gaussian prior for the weight, W and a hyperprior, $\beta$, 如下圖 (c).&lt;/p&gt;

&lt;p&gt;這麼複雜的 DAG 顯然無法用 EM algorithm 解，必須用本文的 “Variational EM Framework” infer values of the unknowns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16286850351205.jpg&quot; width=&quot;245&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;method-1-ml-for-vanilla-linear-regression&quot;&gt;Method 1, ML for Vanilla Linear Regression&lt;/h4&gt;

&lt;p&gt;始於 likelihood function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
p(\mathbf{t} ; \mathbf{w}, \beta)=(2 \pi)^{-\frac{N}{2}} \beta^{\frac{N}{2}} \exp \left(-\frac{\beta}{2}\|\mathbf{t}-\Phi \mathbf{w}\|^{2}\right)
\end{aligned}&lt;/script&gt;

&lt;p&gt;假設 $\mathbf{w}, \beta$ 為 constant parameters (to be estimated).  Maximize the likelihood or log-likelihood 等價於 minimize $|\mathbf{t}-\Phi \mathbf{w}|^{2}$.  因此**maximal likelihood (ML) estimate of w 等價 least squares (LS) estimate.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathbf{w}_{L S}=\underset{w}{\arg \max } p(\mathbf{t} ; \mathbf{w}, \beta)=\underset{w}{\arg \min } E_{L S}(\mathbf{w})=\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{T} \mathbf{t} \label{eqLS}
\end{equation}&lt;/script&gt;

&lt;p&gt;很多情況 $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)$ 可能是 “ill-conditioned” and difficult to invert.  意味如果 observation t 包含 noise $\varepsilon$, noise 會嚴重干擾 $\mathbf{w}_{L S}$ estimation.&lt;/p&gt;

&lt;h5 id=&quot;例-1acommunication-equalizationdeconvolution&quot;&gt;例 1A：Communication equalization/deconvolution&lt;/h5&gt;

&lt;p&gt;Assuming a lowpass channel $\Phi = 1 + 0.9 z^{-1}$.  The equalizer $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{T}$ 變成 highpass filter; zero-forcing equalizer (ZFE).  如果 noise $\varepsilon$ 是 broadband noise, high frequency noise 會被放大。&lt;/p&gt;

&lt;p&gt;In the case of ML, 我們必須小心選 basis functions to ensure matrix $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)$ can be inverted and avoid “ill-condition”.  通常使用 sparse model with few basis functions.&lt;/p&gt;

&lt;h4 id=&quot;method-2-em-algorithm-for-bayesian-linear-regression&quot;&gt;Method 2, EM algorithm for Bayesian Linear Regression&lt;/h4&gt;

&lt;p&gt;Method 2 放寬 $w$ 從定值 fixed value 變成 distribution (random variable). Voila，這就是 Bayesian 精神！&lt;/p&gt;

&lt;p&gt;A Bayesian treatment of the linear model begins by assigning a prior distribution to the weights of the model. This introduces bias in the estimation but also greatly reduces its variance, which is a major problem of the ML estimate.&lt;/p&gt;

&lt;p&gt;此處我們用 common choice of independent, zero-mean, Gaussian prior distribution for the weights of the linear model:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{w} ; \alpha)=\prod_{m=1}^{M} N\left(w_{m} \mid 0, \alpha^{-1}\right)&lt;/script&gt;

&lt;p&gt;當然假設 zero-mean 聽起來有點奇怪，有可能引入 bias, 但好處是有 regularization 的效果，儘量讓 $w_m$ 不要太大。&lt;/p&gt;

&lt;p&gt;Bayesian inference 接下來是計算 posterior distribution of the hidden variable&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
p(\mathbf{w} \mid \mathbf{t} ; \alpha, \beta)=\frac{p(\mathrm{t} \mid \mathbf{w} ; \beta) p(\mathbf{w} ; \alpha)}{p(\mathbf{t} ; \alpha, \beta)} \label{eqMAP}
\end{equation}&lt;/script&gt;

&lt;p&gt;$\eqref{eqMAP}$ 分母部分進一步展開：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{t} ; \alpha, \beta)=\int p(\mathbf{t} \mid \mathbf{w} ; \beta) p(\mathbf{w} ; \alpha) d \mathbf{w}=N\left(\mathbf{t} \mid 0, \beta^{-1} \mathbf{I}+\alpha^{-1} \mathbf{\Phi} \boldsymbol{\Phi}^{T}\right)&lt;/script&gt;

&lt;p&gt;$\eqref{eqMAP}$，posterior of the hidden variable，可以寫成：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
p(\mathbf{w} \mid \mathbf{t} ; \alpha, \beta)=N(\mathbf{w} \mid \boldsymbol{\mu}, \boldsymbol{\mathbf{\Sigma}}) \label{eqPost}
\end{equation}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\boldsymbol{\mu} &amp;=\beta \boldsymbol{\Sigma} \Phi^{T} \mathbf{t} \label{eqMean}\\
\boldsymbol{\Sigma} &amp;=\left(\beta \boldsymbol{\Phi}^{T} \boldsymbol{\Phi}+\alpha \mathbf{I}\right)^{-1} \label{eqVar}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;可以證明，$\alpha, \beta$ 可以用以下的 maximum likelihood estimate.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\left(\alpha_{\mathrm{ML}}, \beta_{\mathrm{ML}}\right)=&amp; \underset{\alpha, \beta}{\arg \min }\left\{\log \left|\beta^{-1} \mathbf{I}+\alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{T}\right|\right. \nonumber \\
&amp;\left.+\mathbf{t}^{T}\left(\beta^{-1} \mathbf{I}+\alpha^{-1} \boldsymbol{\Phi} \boldsymbol{\Phi}^{T}\right)^{-1} \mathbf{t}\right\} \label{eqab}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;直接計算 $\eqref{eqab}$ 非常困難。除了 $\eqref{eqab}$ 微分非常複雜。$\alpha, \beta \ge 0$ 是一個 constrained optimization 問題。 EM algorithm 提供一個有效的方法解 $\alpha, \beta$ and infer $\mathbf{w}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E-step&lt;/strong&gt; Compute the Q function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta) &amp;=\langle\ln p(\mathbf{t}, \mathbf{w} ; \alpha, \beta)\rangle_{p\left(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)}\right)} \\
&amp;=\langle\ln p(\mathbf{t} \mid \mathbf{w} ; \alpha, \beta) p(\mathbf{w} ; \alpha, \beta)\rangle_{p\left(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)}\right)} \\
&amp;=\left\langle\frac{N}{2} \ln \beta-\frac{\beta}{2}\left(\|\mathbf{t}-\boldsymbol{\Phi} \mathbf{w}\|^{2}\right)\right.\\
&amp;\left.+\frac{M}{2} \ln \alpha-\frac{\alpha}{2}\left(\|\mathbf{w}\|^{2}\right)\right\rangle+\text { const } \\
=&amp; \frac{N}{2} \ln \beta-\frac{\beta}{2}\left\langle\|\mathbf{t}-\boldsymbol{\Phi} \mathbf{w}\|^{2}\right\rangle+\frac{M}{2} \ln \alpha \\
&amp;-\frac{\alpha}{2}\left(\left\langle\|\mathbf{w}\|^{2}\right\rangle\right)+\text { const. }
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;三角括號是對 $p(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)})$ 的期望值。代入 $\eqref{eqPost}$ 得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)=&amp; \frac{N}{2} \ln \beta-\frac{\beta}{2}\left(\left\|\mathbf{t}-\boldsymbol{\Phi} \boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Phi}^{T} \boldsymbol{\Sigma}^{(t)} \boldsymbol{\Phi}\right]\right) \\
&amp;+\frac{M}{2} \ln \alpha-\frac{\alpha}{2}\left(\left\|\boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Sigma}^{(t)}\right]\right)+\mathrm{const}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\boldsymbol{\mu}^{(t)}$ and $\boldsymbol{\Sigma}^{(t)}$ are computed using the current estimates of the parameters $\alpha^{(t)}$ and $\beta^{(t)}$ :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\boldsymbol{\mu}^{(t)} &amp;=\beta^{(t)} \boldsymbol{\Sigma}^{(t)} \boldsymbol{\Phi}^{T} \mathbf{t} \\
\boldsymbol{\Sigma}^{(t)} &amp;=\left(\beta^{(t)} \mathbf{\Phi}^{T} \boldsymbol{\Phi}+\alpha^{(t)} \mathbf{I}\right)^{-1}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;M-step&lt;/strong&gt; Maximize $Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)$ with respect to $\alpha, \beta$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(\alpha^{(t+1)}, \beta^{(t+1)}\right)=\underset{(\alpha, \beta)}{\arg \max } Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)&lt;/script&gt;

&lt;p&gt;結果很簡單&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\alpha^{(t+1)} &amp;=\frac{M}{\left\|\boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Sigma}^{(t)}\right]} \label{eqa}\\
\beta^{(t+1)} &amp;=\frac{N}{\left\|\mathbf{t}-\boldsymbol{\Phi} \boldsymbol{\mu}^{(t)}\right\|^{2}+\operatorname{tr}\left[\boldsymbol{\Phi}^{T} \mathbf{\Sigma}^{(t)} \boldsymbol{\Phi}\right]} \label{eqb}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\eqref{eqa}$ 和 $\eqref{eqb}$ 同時保證 $\alpha, \beta$ 永遠為正值。&lt;/p&gt;

&lt;p&gt;幾個重點：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;EM algorithm 有可能收斂到 local minimum; initial condition 很重要&lt;/li&gt;
  &lt;li&gt;注意 $\mathbf{w}$ 不是一個值，而是 distribution.  Inference of $\mathbf{w}$ 就是 posterior distribution $\eqref{eqPost}$.  Posterior distribution 的 mean $\eqref{eqMean}$ 稱為 Bayesian linear minimum mean squire error (LMMSE) inference for $\mathbf{w}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;method-3-variational-em-based-bayesian-linear-regression&quot;&gt;Method 3, Variational EM-based Bayesian Linear Regression&lt;/h4&gt;

&lt;p&gt;因為非常複雜，可以直接參考 [@tzikasVariationalApproximation2008].&lt;/p&gt;

&lt;h5 id=&quot;例-1bnoisy-signal-estimation&quot;&gt;例 1B：Noisy Signal Estimation&lt;/h5&gt;

&lt;p&gt;如下圖，Original signal 是虛線。實際的 observations ‘x’ 是 N = 50 samples 包含 signal + Gaussian noise ($\sigma^2 = 4 \times 10^{-2}$), 大約 SNR = 6.6dB.&lt;/p&gt;

&lt;p&gt;這裡的 basis functions 使用 Gaussian kernels&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_{i}(\mathbf{x})=K\left(\mathbf{x}, \mathbf{x}_{i}\right)=\exp \left(-\frac{1}{2 \sigma_{\phi}^{2}}\left\|\mathbf{x}-\mathbf{x}_{i}\right\|^{2}\right)&lt;/script&gt;

&lt;p&gt;接下來用上述三個方法 (1) ML estimation; (2) EM-based Bayesian inference, and (3) variational EM-based Bayesian inference.&lt;/p&gt;

&lt;p&gt;(1) ML 基本上完全 follow noisy input, 所以最糟。這也符合期待，因為沒有任何 constraint on the weight. 所以所有的 weights 和 Gaussian kernel 都用來 fit noisy observations.  也就是說 N=50 samples/observations 對應 50 個 Gaussian kernel functions.  這可以從下圖的綠線看出。&lt;/p&gt;

&lt;p&gt;(2) Weights are constrained by prior, 此處 prior 假設 zero-mean Gaussian, which regularise the weight to be minimum; otherwise it will incur penalty.&lt;/p&gt;

&lt;p&gt;(3) 我們可以通過 $a, b$ 選取控制 non-zero weights, 類似 supporting vectors in SVM.  我們稱為 relevance vectors (RV). 此例只有 5 個  non-zero RV.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16287874512211.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;例二-bayesian-gmm&quot;&gt;例二： Bayesian GMM&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{x})=\sum_{j=1}^{M} \pi_{j} N\left(x ; \boldsymbol{\mu}_{j}, \mathbf{T}_{j}\right)&lt;/script&gt;

&lt;p&gt;where $\boldsymbol{\pi} =  \{ \pi_j \}$ 代表 weights or mixing coefficients.&lt;br /&gt;
$\boldsymbol{\mu} =  \{ \mu_{j} \} $ 是 means of Gaussian distribution.
$\mathbf{T} = \{ \mathbf{T}_{j} \}$ 是 precision (inverse covariance) matrices.   在 Bayesian GMM 我們更常用 precision matrix.&lt;/p&gt;

&lt;p&gt;Bayesian GMM 和一般 GMM 有什麼不同？ 最大的差別就是 $\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T}$ 不再是 parameters for estimation, 而是 random variables. 這有什麼好處？我們可以 impose or embedded our priors on $\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T}$, 通常是 conjugate priors (i.e. no informative priors) &lt;sup id=&quot;fnref:prior&quot;&gt;&lt;a href=&quot;#fn:prior&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Bayesian GMM 的 graph model 如下。Hidden random variables 包含 $h = (\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T})$. Bayesian 的目標是找出 $p(h\mid x)$, 顯然不會有 analytic solution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16285137362672.jpg&quot; width=&quot;237&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因此我們 divide-and-conquer 利用 $\eqref{eqVarJ2}$
假設 mean-field approximation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
q(\mathrm{h}) &amp;= q_{Z}(\mathbf{Z}) q_{\pi}(\boldsymbol{\pi}) q_{\mu T}(\boldsymbol{\mu}, \mathbf{T}) \\
q_{Z}(\mathbf{Z}) &amp;=\prod_{n=1}^{N} \prod_{j=1}^{M} r_{j n}^{z_{j n}} \\
q_{\pi}(\boldsymbol{\pi}) &amp;=\operatorname{Dir}\left(\boldsymbol{\pi} \mid\left\{\lambda_{j}\right\}\right) \\
q_{\mu T}(\boldsymbol{\mu}, \mathbf{T}) &amp;=\prod_{j=1}^{M} q_{\mu}\left(\boldsymbol{\mu}_{j} \mid \mathbf{T}_{j}\right) q_{T}\left(\mathbf{T}_{j}\right) \\
q_{\mu}\left(\boldsymbol{\mu}_{j} \mid \mathbf{T}\right) &amp;=\prod_{j=1}^{M} N\left(\boldsymbol{\mu}_{j} ; \mathbf{m}_{j}, \beta_{j} \mathbf{T}_{j}\right) \\
q_{T}(\mathbf{T}) &amp;=\prod_{j=1}^{M} W\left(\mathbf{T}_{j} ; \eta_{j}, \mathrm{U}_{j}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;看起來還是很複雜，不過 [@tzikasVariationalApproximation2008] 的 reference [27] 有詳細的公式。可以用“簡單” iterative update procedure 得到 optimal approximation $q(h)$ to the true posterior $p(h\mid x)$, 這就是 variational E-step.  下一步就是 variation M-step, 不贅述。&lt;/p&gt;

&lt;p&gt;Bayesian-GMM 比起 EM-GMM 到底有什麼好處。前面提到可以 impose priors. 如果沒有 prior information (i.e. use conjugate prior), 還有好處嗎？[@tzikasVariationalApproximation2008] 的說法是 Bayesian-GMM 不會有 singular solution, i.e. single data point Gaussian.  然而在 EM-GMM 常常會發生，如下圖 20 Gaussian components。一般 EM-GMM 解決的方法就是多跑幾次 randomize initial conditions to avoid it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16285679550223.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另一個好處是可以直接用 Bayesian GMM 決定 Gaussian component number, 而不需要用其他方法 (e.g. cross-validation)。實作如下圖。(a) 初始是 20 component Gaussians; (b), (c) model evolution; (d) 最終解只剩下 5 個 Gaussian components, 其餘 15 個 Gaussian components weight 為 0。注意收斂的過程中都沒有 singularity.&lt;/p&gt;

&lt;p&gt;這聽起來比較 significant, 不過有一個 catch, 就是 Dirichlet prior 不允許 component mixing weight 為 0.  因此如果要用 Bayesian-GMM 決定 Gaussian component number, 必須 remove $\boldsymbol{\pi} = \{ \pi_j \}$ from priors.  也就是把 $\boldsymbol{\pi} = \{ \pi_j \}$ 視為 parameter to be estimated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16285916007272.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bayesian GMM 的 graph model 如下。注意此時的 $\pi$ 變成方框，代表 parameter to be estimated.  Hidden random variables 包含 $h = (\mathbf{Z}, \boldsymbol{\mu}, \mathbf{T})$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16286002562443.jpg&quot; width=&quot;237&quot; /&gt;&lt;/p&gt;

&lt;p&gt;根據新的 DAG, 我們可以分解如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;q(\mathrm{h})=q_{Z}(\mathbf{Z}) q_{\mu}(\boldsymbol{\mu}) q_{T}(\mathrm{T})\\
&amp;q_{Z}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{j=1}^{M} r_{j n}^{z_{j n}} \\
&amp;q_{\mu}(\boldsymbol{\mu})=\prod_{j=1}^{M} N\left(\boldsymbol{\mu}_{j} \mid \mathrm{m}_{j}, \mathbf{S}_{j}\right) \\
&amp;q_{T}(\mathbf{T})=\prod_{j=1}^{M} W\left(\mathbf{T}_{j} \mid \eta_{j}, \mathbf{U}_{j}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;同樣經過一番計算 variational E-step and M-step (此處省略)，可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_{j}=\frac{\sum_{n=1}^{N} r_{j n}}{\sum_{k=1}^{M} \sum_{n=1}^{N} r_{k n}}&lt;/script&gt;

&lt;p&gt;在 iteration 過程中，有一些 mixing coefficients $\{\pi_j\}$ 收斂到 0. 定性來說，variational bound 可以視為兩項之和：第一項是 likelihood function, 第二項是 prior 造成的 penalty term to penalizes complex models.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;Choy, Chris. 2017. “Expectation Maximization and Variational Inference (Part 1).” February 26, 2017.&lt;br /&gt;
&lt;a href=&quot;https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/&quot;&gt;https://chrischoy.github.io/research/Expectation-Maximization-and-Variational-Inference/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Matas, J., and O. Drbohlav. 2018. “Expectation Maximization Algorithm.” December 1, 2018.&lt;br /&gt;
&lt;a href=&quot;https://cw.fel.cvut.cz/old/_media/courses/a4b33rpz/pr_11_em_2017.pdf&quot;&gt;https://cw.fel.cvut.cz/old/_media/courses/a4b33rpz/pr_11_em_2017.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Poczos, Barnabas. 2015. “Cllustering and EM.” 2015.&lt;br /&gt;
&lt;a href=&quot;https://www.cs.cmu.edu/~epxing/Class/10715/lectures/EM.pdf&quot;&gt;https://www.cs.cmu.edu/~epxing/Class/10715/lectures/EM.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Tzikas, Dimitris G., Aristidis C. Likas, and Nikolaos P. Galatsanos. 2008. “The Variational Approximation for Bayesian Inference.”   &lt;em&gt;IEEE Signal Processing Magazine&lt;/em&gt; 25 (6): 131–46.&lt;br /&gt;
&lt;a href=&quot;https://doi.org/10.1109/MSP.2008.929620&quot;&gt;https://doi.org/10.1109/MSP.2008.929620&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:prior&quot;&gt;
      &lt;p&gt;Dirichlet for $\boldsymbol{\pi}$.  Gauss-Wishart for ($\boldsymbol{\mu}, \mathbf{T})$ &lt;a href=&quot;#fnref:prior&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="ML" /><category term="EM" /><category term="Bayesian" /><summary type="html"></summary></entry><entry><title type="html">English</title><link href="http://localhost:4000/2021/08/03/English/" rel="alternate" type="text/html" title="English" /><published>2021-08-03T00:00:00+08:00</published><updated>2021-08-03T00:00:00+08:00</updated><id>http://localhost:4000/2021/08/03/English</id><content type="html" xml:base="http://localhost:4000/2021/08/03/English/">&lt;h1 id=&quot;語音語調和節奏&quot;&gt;語音語調和節奏&lt;/h1&gt;

&lt;p&gt;語音：pronunciation (word)
語調：intonation (sentence)
節奏：rhymes: biggest problem!!! for Chinese&lt;/p&gt;

&lt;p&gt;Isochrone:  Chinese word is unit time!! syllable-timed
stress-timed language: english
mora-timed language: Japanese&lt;/p&gt;

&lt;p&gt;https://www.youtube.com/watch?v=VMDhdaMkeBU&lt;/p&gt;</content><author><name>Allen Lu (from John Doe)</name></author><summary type="html">語音語調和節奏</summary></entry><entry><title type="html">Math AI - ML Estimation To EM Algorithm For Hidden Data</title><link href="http://localhost:4000/ai/2021/06/30/MLE_to_EM/" rel="alternate" type="text/html" title="Math AI - ML Estimation To EM Algorithm For Hidden Data" /><published>2021-06-30T16:29:08+08:00</published><updated>2021-06-30T16:29:08+08:00</updated><id>http://localhost:4000/ai/2021/06/30/MLE_to_EM</id><content type="html" xml:base="http://localhost:4000/ai/2021/06/30/MLE_to_EM/">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&lt;/script&gt;

&lt;h2 id=&quot;main-reference&quot;&gt;Main Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;[@poczosCllusteringEM2015]&lt;/li&gt;
  &lt;li&gt;[@matasExpectationMaximization2018] good reference&lt;/li&gt;
  &lt;li&gt;[@choyExpectationMaximization2017]&lt;/li&gt;
  &lt;li&gt;[@tzikasVariationalApproximation2008] excellent introductory paper&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;maximum-likelihood-estimation-mle-和應用&quot;&gt;Maximum Likelihood Estimation (MLE) 和應用&lt;/h2&gt;

&lt;p&gt;Maximum likelihood estimation (MLE) 最大概似估計是一種估計模型參數的方法。適用時機在於手邊有模型，但是模型參數有無限多種，透過真實觀察到的樣本資訊，想辦法導出最有可能產生這些樣本結果的模型參數，也就是挑選使其概似性(Likelihood)最高的一組模型參數，這系列找參數的過程稱為最大概似估計法。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bernoulli distribution&lt;/em&gt;：投擲硬幣正面的機率 $\theta$, 反面的機率 $1-\theta$. 連續投擲的正面/反面的次數分別是 H/T.  Likelihood function 為&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\theta, H, T)=\theta^{H}(1-\theta)^{T}&lt;/script&gt;

&lt;p&gt;MLE 在無限個 $\theta$ 中，找到一個使概似性最大的 $\theta$, i.e. $\widehat{\theta}_{\mathrm{MLE}} =\arg \max _{\theta} {\theta^{H}(1-\theta)^{T}}$&lt;/p&gt;

&lt;p&gt;只要 likelihood function 一次微分，可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{\theta}_{M L E}=\frac{H}{T+H}&lt;/script&gt;

&lt;p&gt;就是平均值，推導出來的模型參數符合直覺。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Normal distribution&lt;/em&gt;： 假設 mean unknown, variance known, 我們可以用 maximum log-likelihood function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{\mu}{\operatorname{argmax}} f\left(x_{1}, \ldots, x_{n}\right) \Rightarrow \underset{\mu}{\operatorname{argmax}} \log f\left(x_{1}, \ldots, x_{n}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;\frac{\mathrm{d}}{d \mu}\left(\sum_{i=1}^{n}-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}\right)=\sum_{\mathrm{i}=1}^{\mathrm{n}} \frac{\left(x_{i}-\hat{\mu}\right)}{\sigma^{2}}=\sum_{i=1}^{n} x_{i}-n \hat{\mathrm{u}}=0 \\
&amp;\hat{\mu}=\overline{\mathrm{X}}=\frac{\sum_{i=1}^{n} x_{i}}{n}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;微分的結果告訴我們，樣本的平均值，其實就是母體平均值 $\mu$ 最好的估計！又是一個相當符合直覺的答案，似乎 MLE 只是用來驗證直覺的工具。&lt;/p&gt;

&lt;p&gt;這是一個錯覺，常見的 distribution (e.g. Bernoulli, normal distribution) 都是 exponential families.  可以證明 maximum log-likelihood functions of exponential families 都是 concave function, 沒有 local minimum. 非常容易用數值方法找到最佳解，而且大多有 analytical solution.&lt;/p&gt;

&lt;p&gt;但只要 distribution function 更複雜一點，例如兩個 normal/Gaussian distribution weighted sum to 1, MLE 就非常難解。稱為 Gaussian mixture model (GMM) with 2 groups, GMM(2).&lt;/p&gt;

&lt;p&gt;另一種情況：MLE 雖然直接明瞭，但現實常常會遇到 missing data 或是 hidden data/state (state 也視為 data). 此時就需要 Expectation Maximization (EM) algorithm.&lt;/p&gt;

&lt;p&gt;例如 GMM(2) 可以視為有一個 hidden state $z$ with binary value, $p(x) = p(x\mid z=0) p(z=0) + p(x\mid z=1) p(z=1)$. $p(x\mid z=0)$ 和 $p(x\mid z=1)$ 分別是不同 normal distributions.&lt;/p&gt;

&lt;p&gt;以下先 Q&amp;amp;A maximum likelihood estimation (MLE) vs. expectation maximization (EM) 兩種算法。其實是視 EM 為 MLE 的推廣。 接著用四個簡單例子 (toy example) 說明 MLE 如何推廣到 EM.&lt;/p&gt;

&lt;h2 id=&quot;qa-of-mle-versus-em&quot;&gt;Q&amp;amp;A of MLE Versus EM&lt;/h2&gt;

&lt;p&gt;Q: Why EM is a special case of MLE?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the problem can be formulated as MLE parameter estimation of incomplete/hidden data.  Then EM algorithm 的 E-step is guessing incomplete/hidden data; M-step 就對應 MLE parameter estimation with modification (見本文後段)。&lt;/li&gt;
  &lt;li&gt;EM M-Step is essentially a MLE parameter estimation with modification.&lt;/li&gt;
  &lt;li&gt;EM can be seen as an iterative MLE.  EM may converge at local minimum during iteration.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Q: How EM can be used for to parameter estimation and incomplete/hidden data estimation?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For Bayesian, 兩者可以視為同一類。Unknown parameters 亦可以視為 missing data with distribution.  此時 EM algorithm 相當于 2D &lt;strong&gt;coordinate descent&lt;/strong&gt; (energy) optimization [@wikiCoordinateDescent2021], different from &lt;strong&gt;gradient descent&lt;/strong&gt;.  EM 的 E-step 對應 (conditional) distribution coordinate descent; M-step 對應 parameter coordinate descent.&lt;/li&gt;
  &lt;li&gt;For Frequentist (古典統計), E-step is guessing incomplete/hidden data; M-step 就對應 MLE parameter estimation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;toy-example-matasexpectationmaximization2018&quot;&gt;Toy Example [@matasExpectationMaximization2018]&lt;/h2&gt;

&lt;h3 id=&quot;前提摘要&quot;&gt;前提摘要&lt;/h3&gt;
&lt;p&gt;一個簡單例子觀察 temperature and amount of snow (溫度和雪量, both are binary input) 的 joint probability depending on two “scalar factors” $a$ and $b$ as $p(t, s | a, b)$&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_0$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_1$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$t_0$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$a$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$5a$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$t_1$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$3b$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$b$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;注意 $a$ and $b$ are parameters, 不是 conditional probability.
另外因為機率和為 1 做為一個 constraint: $6a + 4b = 1$&lt;/p&gt;

&lt;h3 id=&quot;例一-mle&quot;&gt;例一: MLE&lt;/h3&gt;
&lt;p&gt;一個 ski-center 觀察 $N$ 天的溫度和雪量得到以下的統計，$N_{ij} \in \mathbf{I}$, 如何估計 $a$ and $b$?&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_0$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_1$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$t_0$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$N_{00}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$N_{01}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$t_1$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$N_{10}$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$N_{11}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Likelihood function (就是 joint pdf of $N$ repeat experiments)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\mathcal{T} \mid a, b)= C a^{N_{00}}(5 a)^{N_{01}}(3 b)^{N_{10}}(b)^{N_{11}}&lt;/script&gt;

&lt;p&gt;where $C = (\Sigma N_{ij})! / \Pi (N_{ij}!)$ 是 MLE 無關的常數&lt;/p&gt;

&lt;p&gt;問題改成 maximum log-likelihood with constraint and $C’ = \ln C$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(a, b, \lambda) = C' + N_{00} \ln a+N_{01} \ln 5 a+N_{10} \ln 3 b+N_{11} \ln b+\lambda(6 a+4 b-1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
\frac{\partial L}{\partial a}=N_{00} \frac{1}{a}+N_{01} \frac{1}{a}+6 \lambda=0 \\
\frac{\partial L}{\partial b}=N_{10} \frac{1}{b}+N_{11} \frac{1}{b}+4 \lambda=0 \\
\frac{\partial L}{\partial \lambda}=6 a+4 b - 1 = 0
\end{gathered}&lt;/script&gt;

&lt;p&gt;上述方程式的解為
&lt;script type=&quot;math/tex&quot;&gt;a=\frac{N_{00}+N_{01}}{6 N} \quad b=\frac{N_{10}+N_{11}}{4 N} \quad \lambda = -(N_{00}+N_{01}+N_{10}+N_{11})=-N&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;結果很直觀。其實就是利用大數法則： $a\cdot N \sim N_{00}; 5a\cdot N\sim N_{01}; 3b\cdot N\sim N_{10}; b\cdot N\sim N_{11}$
再來大數法則 (a+5a)N~N00+N01; (3b+b)N~N10+N11 =&amp;gt; a = .. ; b = …&lt;/p&gt;

&lt;h3 id=&quot;例二-incompletehidden-data&quot;&gt;例二 incomplete/hidden Data&lt;/h3&gt;
&lt;p&gt;假設我們無法觀察到完整的”溫度和雪量“；而是“溫度或雪量”，有時“溫度”，有時“雪量”，但不是同時。對應的不是 joint pdf, 而是 marginal pdf 如下：
&lt;img src=&quot;/media/16247543929429/16265417789274.jpg&quot; alt=&quot;-w451&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;觀察如下：
&lt;img src=&quot;/media/16247543929429/16265418866309.jpg&quot; alt=&quot;-w274&quot; style=&quot;zoom:33%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Lagrangian (log-likelihood with constraint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(a, b, \lambda)=T_{0} \ln 6 a+T_{1} \ln 4 b+S_{0} \ln (a+3 b)+S_{1} \ln (5 a+b)+\lambda(6 a+4 b-1)&lt;/script&gt;

&lt;p&gt;此時的方程式比起之前複雜的多，不一定有 close-form solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
\frac{\partial L}{\partial a}=\frac{T_{0}}{a}+\frac{S_{0}}{a+3 b}+\frac{5 S_{1}}{5 a+b}+6 \lambda=0 \\
\frac{\partial L}{\partial b}=\frac{T_{1}}{b}+\frac{3 S_{0}}{a+3 b}+\frac{S_{1}}{5 a+b}+4 \lambda=0 \\
6 a+4 b=1
\end{gathered}&lt;/script&gt;

&lt;p&gt;如果用大數法則：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$6a \cdot(T_0+T_1) \sim T0; \, 4b\cdot(T_0+T_1) \sim T_1$&lt;/li&gt;
  &lt;li&gt;$(a+3b) \cdot (S_0+S_1)\sim S_0; \, (5a+b)\cdot(S_0+S_1) \sim S_1$ 
注意不論 1. or 2. 都滿足 $6a+4b = 1$ constraint, 可以用來估計 $a$ and $b$.
問題是我們要用那一組 $(a, b)$?  單獨用一組都會損失一些 information, 應該要 combine 1 and 2 的 information, how?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;思路一&lt;/strong&gt; 平均 (a, b) from 1 and 2.  但這不是好的策略，因為平均 (a,b) 不一定滿足 constraint. 在這個 case 因為 linear constraint, 所以平均 (a,b) 仍然滿足 constraint.  但對於更複雜 constraint, 平均並非好的方法。&lt;/p&gt;

&lt;p&gt;更重要的是平均並無法代表 maximum likelihood in the above equation.  我們的目標是 maximum likelihood, 平均 (a, b) 完全無法保證會得到更好的 likelihood value!&lt;/p&gt;

&lt;p&gt;或者把 (a,b) from 1 or 2 代入上述 likelihood function 取大值。顯然這也不是最好的策略。因為一半的資訊被捨棄了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;思路二&lt;/strong&gt; 比較好的方法是想辦法用迭代法解微分後的 Lagrange multiplier 聯立方程式。 (a, b) from 1. or 2. 只作為 initial solution, 想辦法從聯立方程式找出 iterative formula.  這似乎是對的方向，問題是 Lagrange multiplier optimization 是解聯立(level 1)微分方程式。不一定有 close form as in this example.  同時也無法保證收斂。另外如何找出 iterative formula 似乎是 case-by-case, 沒有一致的方式。
&lt;strong&gt;=&amp;gt; iterative solution is one of the key, but NOT on Lagrange multiplier (level 1)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;思路三&lt;/strong&gt; 既然是 missing data, 我們是否可以假設 $(a, b) \to$  fill missing data $\to$ update $(a, b) \to$  update missing data $\cdots$ 具體做法 
$N_{00} = T_0 \cdot \frac{1}{6} + S_0 \cdot \frac{a}{a+3b}$
$N_{01} = T_0 \cdot \frac{5}{6} + S_1 \cdot \frac{5a}{5a+b}$
$N_{10} = T_1 \cdot \frac{3}{4} + S_0 \cdot \frac{3b}{a+3b}$
$N_{11} = T_1 \cdot \frac{1}{4} + S_1 \cdot \frac{b}{5a+b}$&lt;/p&gt;

&lt;p&gt;有了 $N_{00},N_{01},N_{10},N_{11}$ 可以重新估計 $(a, b)$ using joint pdf&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a'=\frac{N_{00}+N_{01}}{6 N} \quad b'=\frac{N_{10}+N_{11}}{4 N}&lt;/script&gt;

&lt;p&gt;Q: 如何證明這個方法是最佳或是對應 complete data MLE or incomplete/hidden data MLE? 甚至會收斂？&lt;/p&gt;

&lt;h4 id=&quot;em-algorithm-邏輯&quot;&gt;EM algorithm 邏輯&lt;/h4&gt;

&lt;h3 id=&quot;前提摘要-1&quot;&gt;前提摘要&lt;/h3&gt;
&lt;h3 id=&quot;gmm-特例estimate-means-of-two-gaussian-distributions-known-variance-and-ratio-unknown-means&quot;&gt;GMM 特例：Estimate Means of Two Gaussian Distributions (known variance and ratio; unknown means)&lt;/h3&gt;

&lt;p&gt;We measure lengths of vehicles. The observation space is two-dimensional, with $x$ capturing vehicle type (binary) and $y$ capturing length (Gaussian).&lt;/p&gt;

&lt;p&gt;$p(x, y)$  $x\in$ {car, truck},  $y \in \mathbb{R}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\text {car}, y)=\pi_{\mathrm{c}} \mathcal{N}\left(y \mid \mu_{\mathrm{c}}, \sigma_{\mathrm{c}}=1\right)=\kappa_{\mathrm{c}} \exp \left\{-\frac{1}{2}\left(y-\mu_{\mathrm{c}}\right)^{2}\right\},\left(\kappa_{\mathrm{c}}=\frac{\pi_{\mathrm{c}}}{\sqrt{2 \pi}}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\text {truck,} y)=\pi_{\mathrm{t}} \mathcal{N}\left(y \mid \mu_{\mathrm{t}}, \sigma_{\mathrm{t}}=2\right)=\kappa_{\mathrm{t}} \exp \left\{-\frac{1}{8}\left(y-\mu_{\mathrm{t}}\right)^{2}\right\},\left(\kappa_{\mathrm{t}}=\frac{\pi_{\mathrm{t}}}{\sqrt{8 \pi}}\right)&lt;/script&gt;

&lt;p&gt;where $\pi_c + \pi_t = 1$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16247543929429/16266210341198.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;例三-complete-data-easy-case&quot;&gt;例三 Complete Data (Easy case)&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;T=\{\underbrace{\left(\operatorname{car}, y_{1}^{(c)}\right),\left(\operatorname{car}, y_{2}^{(c)}\right), \ldots,\left(\operatorname{car}, y_{C}^{(c)}\right)}_{C \text { car observations }}, \underbrace{\left(\text {truck}, y_{1}^{(\mathrm{t})}\right),\left(\text {truck}, y_{2}^{(\mathrm{t})}\right), \ldots,\left(\text {truck}, y_{T}^{(\mathrm{t})}\right)}_{T \text { truck observations }}\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Log-likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell(\mathcal{T})=\sum_{i=1}^{N} \ln p\left(x_{i}, y_{i} \mid \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)=C \ln \kappa_{\mathrm{c}}-\frac{1}{2} \sum_{i=1}^{C}\left(y_{i}^{(c)}-\mu_{\mathrm{c}}\right)^{2}+T \ln \kappa_{\mathrm{t}}-\frac{1}{8} \sum_{i=1}^{T}\left(y_{i}^{(\mathrm{t})}-\mu_{\mathrm{t}}\right)^{2}&lt;/script&gt;

&lt;p&gt;很容易用 MLE 估計 $\mu_1, \mu_2$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \ell(\mathcal{T})}{\partial \mu_{\mathrm{c}}}=\sum_{i=1}^{C}\left(y_{i}^{(\mathrm{c})}-\mu_{\mathrm{c}}\right)=0 \quad \Rightarrow \quad \mu_{\mathrm{c}}=\frac{1}{C} \sum_{i=1}^{C} y_{i}^{(c)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \ell(\mathcal{T})}{\partial \mu_{\mathrm{t}}}=\frac{1}{4} \sum_{i=1}^{T}\left(y_{i}^{(\mathrm{t})}-\mu_{\mathrm{t}}\right)=0 \quad \Rightarrow \quad \mu_{\mathrm{t}}=\frac{1}{T} \sum_{i=1}^{T} y_{i}^{(\mathrm{t})}&lt;/script&gt;

&lt;p&gt;直觀上很容易理解。如果 observations 已經分組，求 mean 只要做 sample 的平均即可。&lt;/p&gt;

&lt;p&gt;以這個例子，ratio $\pi_c, \pi_t$ 不論已知或未知，都不影響結果。&lt;/p&gt;

&lt;h3 id=&quot;例四-incompletehidden-data&quot;&gt;例四 incomplete/hidden Data&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{T}=\{\left(\operatorname{car}, y_{1}^{(c)}\right), \ldots,\left(\operatorname{car}, y_{C}^{(c)}\right),\left(\text {truck}, y_{1}^{(\mathrm{t})}\right), \ldots,\left(\text {truck}, y_{T}^{(\mathrm{t})}\right), \underbrace{\left(\bullet, y_{1}^{\bullet}\right), \ldots,\left(\bullet, y_{M}^{\bullet}\right)}_{\begin{array}{l}
\text { data with uknown } \\
\text { vehicle type }
\end{array}}\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p\left(y^{\bullet}\right)=p\left(\text {car}, y^{\bullet}\right)+p\left(\text {truck}, y^{\bullet}\right)&lt;/script&gt;

&lt;p&gt;Log-likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell(\mathcal{T})=\sum_{i=1}^{N} \ln p\left(x_{i}, y_{i} \mid \mu_{c}, \mu_{\mathrm{t}}\right)=\overbrace{C \ln \kappa_{\mathrm{c}}-\frac{1}{2} \sum_{i=1}^{C}\left(y_{i}^{(c)}-\mu_{\mathrm{c}}\right)^{2}+T \ln \kappa_{\mathrm{t}}-\frac{1}{8} \sum_{i=1}^{T}\left(y_{i}^{(\mathrm{t})}-\mu_{\mathrm{t}}\right)^{2}}^{\text {same term as before }} \\
+\sum_{i=1}^{M} \ln \left(\kappa_{\mathrm{c}} \exp \left\{-\frac{1}{2}\left(y_{i}^{\bullet}-\mu_{\mathrm{c}}\right)^{2}\right\}+\kappa_{\mathrm{t}} \exp \left\{-\frac{1}{8}\left(y_{i}^{\bullet}-\mu_{\mathrm{t}}\right)^{2}\right\}\right)&lt;/script&gt;

&lt;p&gt;不用微分也知道非常難解 MLE. 我們必須用另外的方法，就是 EM 算法。
不過我們還是微分一下，得到更多的 insights.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
0=\frac{\partial \ell(\mathcal{T})}{\partial \mu_{\mathrm{c}}} &amp;=\sum_{i=1}^{C}\left(y_{\mathrm{c}}^{(\mathrm{c})}-\mu_{\mathrm{c}}\right) \\
&amp;+ \sum_{i=1}^{M} \overbrace{\frac{\kappa_{\mathrm{c}} \exp \left\{-\frac{1}{2}\left(y_{i}^{\bullet}-\mu_{\mathrm{c}}\right)^{2}\right\}}{\kappa_{\mathrm{c}} \exp \left\{-\frac{1}{2}\left(y_{i}^{\bullet}-\mu_{\mathrm{c}}\right)^{2}\right\}+\kappa_{\mathrm{t}} \exp \left\{-\frac{1}{8}\left(y_{i}^{\bullet}-\mu_{\mathrm{t}}\right)^{2}\right\}}}^{p\left(\operatorname{car} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)}\left(y_{i}^{\bullet}-\mu_{\mathrm{c}}\right)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0=4 \frac{\partial \ell(\mathcal{T})}{\partial \mu_{\mathrm{t}}}=\sum_{i=1}^{T}\left(y_{i}^{(\mathrm{t})}-\mu_{\mathrm{t}}\right)+\sum_{i=1}^{M} p\left(\text {truck} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)\left(y_{i}^{\bullet}-\mu_{\mathrm{t}}\right)&lt;/script&gt;

&lt;p&gt;上兩式非常有物理意義。基本是 easy case 的延伸：已知分類的平均值，加上未知分類的機率平均值。一個簡單的方法是只取前面已知的部分平均，不過這不是最佳，因為丟失部分的資訊。&lt;/p&gt;

&lt;h4 id=&quot;missing-values-em-approach&quot;&gt;Missing Values, EM Approach&lt;/h4&gt;
&lt;p&gt;重新 summarize optimality conditions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{C}\left(y_{i}^{(c)}-\mu_{c}\right)+\sum_{i=1}^{M} p\left(\operatorname{car} \mid y_{i}^{\bullet}, \mu_{c}, \mu_{\mathrm{t}}\right)\left(y_{i}^{\bullet}-\mu_{\mathrm{c}}\right)=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{T}\left(y_{i}^{(\mathrm{t})}-\mu_{\mathrm{t}}\right)+\sum_{i=1}^{M} p\left(\text {truck } \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)\left(y_{i}^{\bullet}-\mu_{\mathrm{t}}\right)=0&lt;/script&gt;

&lt;p&gt;如果 $p(\text {truck} \mid y_{i}^{\bullet}, \mu_c, \mu_t)$ 和 $p(\text {car} \mid y_{i}^{\bullet}, \mu_c, \mu_t)$ 已知，上式非常容易解 $\mu_c$ and $\mu_t$。實際這是一個雞生蛋、蛋生雞的問題，因為這兩個機率又和 $\mu_c$ and $\mu_t$ 相關。&lt;/p&gt;

&lt;p&gt;EM algorithm 剛好用來打破這個迴圈。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Let $z_i \,(i=1, 2, \cdots, M), z_i \in \text{{car, truck}}$ denote the &lt;strong&gt;missing data&lt;/strong&gt;.  Define $q\left(z_{i}\right)=p\left(z_{i} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)$&lt;/li&gt;
  &lt;li&gt;上述 optimality equations 可以得到&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{\mathrm{c}}=\frac{\sum_{i=1}^{C} y_{i}^{(\mathrm{c})}+\sum_{i=1}^{M} q\left(z_{i}=\mathrm{car}\right) y_{i}^{\bullet}}{C+\sum_{i=1}^{M} q\left(z_{i}=\mathrm{car}\right)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{\mathrm{t}}=\frac{\sum_{i=1}^{T} y_{i}^{(\mathrm{t})}+\sum_{i=1}^{M} q\left(z_{i}=\text { truck }\right) y_{i}^{\bullet}}{T+\sum_{i=1}^{M} q\left(z_{i}=\text { truck }\right)}&lt;/script&gt;

&lt;p&gt;EM Algorithm 可以用以下四步驟表示&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize $\mu_c$, $\mu_t$&lt;/li&gt;
  &lt;li&gt;Compute $q\left(z_{i}\right)=p\left(z_{i} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)$ for $i = 1, 2, \cdots, M$&lt;/li&gt;
  &lt;li&gt;Recompute $\mu_c$, $\mu_t$ according to the above equations.&lt;/li&gt;
  &lt;li&gt;If termination condition is met, finish.  Otherwise, goto 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上述步驟 2 稱為 Expectation (E) Step, 步驟 3 稱為 Maximization (M) Step.  統稱為 EM algorithm.&lt;/p&gt;

&lt;p&gt;Q. Why Step 2 稱為 Expectation? not clear.  Maximization 比較容易理解，因為 optimality condition 就是 maximization (微分為 0).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In summary&lt;/strong&gt;, EM algorithm 的一個關鍵點是：讓 incomplete/hidden data 變成 complete (Expectation?).  有了完整的 data, 就容易用 MLE 找到 maximal likelihood estimation ($\mu_c$ and $\mu_t$ in this case).&lt;/p&gt;

&lt;h2 id=&quot;clustering-soft-assignment-vs-hard-assignment-k-means&quot;&gt;Clustering: Soft Assignment Vs. Hard Assignment (K-means)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/media/16270144925547/16270374215686.jpg&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;em-algorithm-derivation&quot;&gt;EM Algorithm Derivation&lt;/h2&gt;

&lt;p&gt;EM algorithm 如果只是 heuristic algorithm, 可能有用度大幅縮減。以下討論 EM 數學上的 formulation.  先定義 terminologies&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{x}$: observed random variables (下圖雙圓框)&lt;/li&gt;
  &lt;li&gt;$\mathbf{z}$: hidden random variables (下圖單圓框)&lt;/li&gt;
  &lt;li&gt;$\mathbf{\theta}$: fixed model parameters to be estimated (下圖單方框)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210905175447897.png&quot; alt=&quot;image-20210905175447897&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;目標：Find $\theta^*$ to maximize likelihood or marginal likelihood 如下 $\eqref{eqMLE}$. 此處 $\theta$ 是一個 fixed parameter, 不是一個 random variable.  所以我們用 $p(x; \theta)$ notation, 而避免用 $p(x \mid \theta)$ notation. 不過有時候引用其他文章還是難以完全避免，可以從上下文判斷。&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\boldsymbol{\theta}^{*}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ell(\boldsymbol{\theta})=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ln p(\mathbf{x} ; \boldsymbol{\theta}) \label{eqMLE}
\end{align}&lt;/script&gt;​&lt;/p&gt;

&lt;p&gt;思路：假設解下列完整 data 很容易解 (例如例一和例三)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \label{eqMLE2}
\end{align}&lt;/script&gt;

&lt;p&gt;我們的想法是把 $\eqref{eqMLE}$ 先變形成上式 $\eqref{eqMLE2}$，再想辦法優化&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\ln p(\mathbf{x} ; \boldsymbol{\theta}) &amp;=\ln \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) \nonumber \\
&amp;=\ln \sum_{\mathbf{z}} q(\mathbf{z}) \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}  \label{eqMLE3}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;這裡引入看似任意 probability distribution $q(\mathbf{z})$ with $\sum_{\mathbf{z}} q(\mathbf{z})=1$. 後面會說明如何選 $q(\mathbf{z})$.&lt;/p&gt;

&lt;h3 id=&quot;log-likelihood-with-hidden-variable-lower-bound&quot;&gt;Log-Likelihood with Hidden Variable Lower Bound&lt;/h3&gt;

&lt;p&gt;上式 $\eqref{eqMLE3}$ 利用 Jensen’s inequality 可以導出 $\geq \sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}$&lt;/p&gt;

&lt;p&gt;我們定義 $\ln p(\mathbf{x} ; \boldsymbol{\theta})$ 的 lower bound or ELBO (Evidence Lower BOund) 為 $\mathcal{L}(q, \boldsymbol{\theta})$, for any distribution $q(\mathbf{z})$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\mathcal{L}(q, \boldsymbol{\theta})=\sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})} \label{eqELBO}
\end{align}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;這已經非常接近思路！我們的思路修正成把有 hidden data 的 MLE 變成用完整 data 的 MLE 做為 lower bound.  再通過 $q(\mathbf{z})$ 提高 lower bound 逼近原來的目標。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Maximizing $\mathcal{L}(q, \boldsymbol{\theta})$ by choosing $q(\mathbf{z})$ 就可以 push the log likelihood $\ln p(\mathbf{x} ; \boldsymbol{\theta})$ upwards.&lt;/p&gt;

&lt;p&gt;反過來我們可以計算和 lower bound 之間的 gap.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\ln p(\mathbf{x}, \boldsymbol{\theta})-\mathcal{L}(q; \boldsymbol{\theta}) &amp;=\ln p(\mathbf{x} ; \boldsymbol{\theta})-\sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})} \nonumber\\
&amp;=\ln p(\mathbf{x} ; \boldsymbol{\theta})-\sum_{\mathbf{z}} q(\mathbf{z})\{\ln \underbrace{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}_{p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) p(\mathbf{x} ; \boldsymbol{\theta})}-\ln q(\mathbf{z})\} \nonumber\\
&amp;=\ln p(\mathbf{x} ; \boldsymbol{\theta})-\sum_{\mathbf{z}} q(\mathbf{z})\{\ln p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})+\ln p(\mathbf{x} ; \boldsymbol{\theta})-\ln q(\mathbf{z})\} \nonumber\\
&amp;=\ln p(\mathbf{x} ; \boldsymbol{\theta})-\underbrace{\sum_{\mathbf{z}} q(\mathbf{z})}_{1} \ln p(\mathbf{x} ; \boldsymbol{\theta})-\sum_{\mathbf{z}} q(\mathbf{z})\{\ln p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})-\ln q(\mathbf{z})\} \nonumber\\
&amp;=-\sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})}{q(\mathbf{z})} \label{eqGAP} \\
&amp;= D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \ge 0 \label{eqKL}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;這個 gap $\eqref{eqKL}$ 深具物理意義，就是 KL divergence between $q(\mathbf{z})$ and posterior  $p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})$, 也就是兩者之間的距離，永遠大於 0. 這也和 Jensen Inequality 的結論一致！&lt;/p&gt;

&lt;p&gt;以下是關鍵：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;如果能找到 $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})$ 的 analytical solution，就可以讓 gap 變成 0.  Lower bound $\eqref{eqELBO}$ 就是我們要 maximize 目標，voila!
    &lt;ul&gt;
      &lt;li&gt;例如例四 GMM 的 $p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})$ 就是 softmax function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;即使 $q(\mathbf{z})$ 有 analytical solution, e.g. softmax, 不代表容易解 maximum 以及對應的 parameter.  EM algorithm 就是用來處理這個問題，見下文。&lt;/li&gt;
  &lt;li&gt;假如 $q(\mathbf{z})$ 非常複雜沒有 analytical solution，還有另外方法：variational approximation; 稱為 Bayesian inference；或是用一個 neural network approximate posterior；稱為 variational autoencoder (VAE). 本文不討論，下文再討論。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;em-algorithm-push-the-lower-bound-upwards&quot;&gt;EM Algorithm Push the Lower Bound Upwards&lt;/h3&gt;
&lt;p&gt;Log likelihood function 可以分為兩個部分： ELBO + KL Gap of posterior&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\ln p(\mathbf{x} ; \boldsymbol{\theta})=\mathcal{L}(q, \boldsymbol{\theta})+ D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) )
\end{equation}\label{eqSUM}&lt;/script&gt;

&lt;p&gt;從 Jensen’s inequality 得到 $\mathcal{L}(q; \boldsymbol{\theta})$ 是 lower bound.  從 KL divergence $\ge$ 0 再度驗證。&lt;/p&gt;

&lt;p&gt;如果 $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})$, the bound is tight.&lt;/p&gt;

&lt;p&gt;接下來看兩個極端的 examples.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Trivial Case:&lt;/strong&gt;&lt;/em&gt;  Hidden variable $\mathbf{z}$ does NOT provide any information of $\mathbf{x}$&lt;/p&gt;

&lt;p&gt;如果 $\mathbf{x}$ 和 $\mathbf{z}$ 完全無關，$p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) = p(\mathbf{z} ; \boldsymbol{\theta})$.  We can make $q(\mathbf{z}) = p(\mathbf{z} ; \boldsymbol{\theta})$
such that $D_{\mathrm{KL}}(q(\mathbf{z}) | p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})) = 0$, 也就是 gap = 0. Lower bound 就變成原來的 log-likelihood function, trivial case.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}(q, \boldsymbol{\theta}) &amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}\\ 
&amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x} ; \boldsymbol{\theta}) p(\mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})} \\
&amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln p(\mathbf{x} ; \boldsymbol{\theta})\\
&amp;= \ln p(\mathbf{x} ; \boldsymbol{\theta})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Case 2:&lt;/strong&gt;&lt;/em&gt; 如果  $p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})$ 有 analytical solution, let $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}(q, \boldsymbol{\theta}) &amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})}\\ 
&amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) p(\mathbf{x} ; \boldsymbol{\theta})}{q(\mathbf{z})} \\
&amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln p(\mathbf{x} ; \boldsymbol{\theta})\\
&amp;= \ln p(\mathbf{x} ; \boldsymbol{\theta})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;其實這就是 EM algorithm 的精髓&lt;/p&gt;

&lt;h2 id=&quot;em-具體步驟&quot;&gt;EM 具體步驟&lt;/h2&gt;

&lt;p&gt;Recap EM algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Gap 可以視為從 observables 推論出 unobservables, i.e. incomplete/hidden data, &lt;strong&gt;對應 EM algorithm 的 E-Step.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Lower bound 其實可以視為 MLE of complete data， &lt;strong&gt;對應 EM algorithm 的 M-Step.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recap lower bound $\eqref{eqELBO}$ 包含兩個部分：(i) $q(\mathbf{z})$ distribution and (ii) log-likelihood of complete data, $\ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})$.&lt;/p&gt;

&lt;p&gt;這兩個部分剛好對應 EM algorithm 的 E-step (i) and M-step (ii).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initialize $\boldsymbol{\theta}=\boldsymbol{\theta}^{(0)}$&lt;/li&gt;
  &lt;li&gt;E-step (Expectation):&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
q^{(t+1)}=\underset{q}{\operatorname{argmax}} \mathcal{L}\left(q, \boldsymbol{\theta}^{(t)}\right) \label{eqEstep}
\end{align}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;M-step (Maximization):&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\boldsymbol{\theta}^{(t+1)}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \mathcal{L}\left(q^{(t+1)}, \boldsymbol{\theta}\right) \label{eqMstep}
\end{align}&lt;/script&gt;

&lt;h3 id=&quot;m-step-qt1-is-fixed&quot;&gt;M-step: $q^{(t+1)}$ is fixed&lt;/h3&gt;
&lt;p&gt;我們先看 M-step $\eqref{eqMstep}$​​, 因為這和 MLE estimate $\theta$​​ 非常相似。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}\left(q^{(t+1)}, \boldsymbol{\theta}\right) &amp;=\sum_{\mathbf{z}} q^{(t+1)}(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q^{(t+1)}(\mathbf{z})} \\
&amp;=\sum_{\mathbf{z}} q^{(t+1)}(\mathbf{z}) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})-\underbrace{\sum_{\mathbf{z}} q^{(t+1)}(\mathbf{z}) \ln q^{(t+1)}(\mathbf{z})}_{\text {const. }}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\boldsymbol{\theta}^{(t+1)}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \sum_{\mathbf{z}} q^{(t+1)}(\mathbf{z}) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}^{(t)}) \label{eqMstep2}
\end{align}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;注意 M-Step 和完整 data 的 MLE 思路如下非常接近，只加了對 $q(\mathbf{z})$ 的 weighted sum.&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;上式微分等於 0 就可以解 $\theta^{t+1}$。上面例四以及例二就是很好的例子。&lt;/p&gt;

&lt;p&gt;另一個常見的寫法&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\boldsymbol{\theta}^{(t+1)}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} E_{q(z)} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}^{(t)}) \label{eqMstep3}
\end{align}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;注意 M-Step 是 maximize lower bound, 並不等於 maximize 不完整 data 的 MLE，因為還差了一個 gap function (i.e. KL divergence).  E-Step 的目標才是縮小 gap function, which is also $\boldsymbol{\theta}$ dependent.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;e-step-boldsymbolthetat-is-fixed&quot;&gt;E-step: $\boldsymbol{\theta}^{(t)}$ is fixed&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q^{(t+1)}=\underset{q}{\operatorname{argmax}} \mathcal{L}\left(q, \boldsymbol{\theta}^{(t)}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}\left(q, \boldsymbol{\theta}^{(t)}\right)=\underbrace{\ln p\left(\mathbf{x} ; \boldsymbol{\theta}^{(t)}\right)}_{\text {const. }}-D_{\mathrm{KL}}(q \| p)&lt;/script&gt;

&lt;p&gt;以上 KL divergence 大於等於 0，所以 maximize lower bound 就要讓 要選擇 $q(z)$ 儘量縮小 gap  (i.e. KL divergence) 到 0.  Gap 等於 0 的條件就是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
q^{(t+1)}(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}^{(t)}) \label{eqEstep2}
\end{align}&lt;/script&gt;

&lt;p&gt;同樣 E-Step 深具物理意義，就是猜 incomplete/hidden data distribution based on 已知的 observables 和 iterative $\theta$.&lt;/p&gt;

&lt;p&gt;例如例四 E-Step 就是計算 $q\left(z_{i}\right)=p\left(z_{i} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)$ for $i = 1, 2, \cdots, M$.  結果是 softmax function.&lt;/p&gt;

&lt;h4 id=&quot;conditional-vs-joint-distribution&quot;&gt;Conditional Vs. Joint Distribution&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我們可以把 conditional distribution 改成 joint distribution 如下。兩者都可以用來解 E-Step.&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}^{(t)}) = p(\mathbf{z}, \mathbf{x} ; \boldsymbol{\theta}^{(t)}) / p(\mathbf{x} ; \boldsymbol{\theta}^{(t)})&lt;/script&gt;

&lt;h3 id=&quot;em-精髓-結合-e-step-and-m-step&quot;&gt;EM 精髓: 結合 E-Step and M-Step&lt;/h3&gt;

&lt;p&gt;如果 E-Step $\eqref{eqEstep2}$ 有 analytic solution, 可以代入 M-Step $\eqref{eqMstep2}$ 得到有名的 $Q$ function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Q(\theta^{t+1} | \theta^{t}) &amp;=  \sum_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}^{(t)}) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}^{t+1}) \nonumber \\
&amp;= \int d \mathbf{z} \, p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}^{(t)}) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}^{t+1}) \\
&amp;= E_{z\sim p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}^{(t)})} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}^{(t+1)}) 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;New EM algorithm with fixed $\boldsymbol{\theta}^{t}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\boldsymbol{\theta}^{(t+1)}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} Q(\boldsymbol{\theta}^{t+1} | \boldsymbol{\theta}^{t}) \label{eqQ}
\end{align}&lt;/script&gt;

&lt;h3 id=&quot;qa&quot;&gt;Q&amp;amp;A&lt;/h3&gt;

&lt;p&gt;From $\eqref{eqELBO}$, 我們可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathcal{L}(q, \boldsymbol{\theta})&amp;=\sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})} \nonumber \\ 
&amp;= - D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z}, \mathbf{x}; \boldsymbol{\theta}) ) \label{eqELBOKL}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;代入 $\eqref{eqKL}$, 我們可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\ln p(\mathbf{x}; \boldsymbol{\theta}) &amp;= \mathcal{L}(q; \boldsymbol{\theta}) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \nonumber \\
&amp;= - D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z}, \mathbf{x}; \boldsymbol{\theta}) ) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \nonumber
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;也就是 &lt;strong&gt;ELBO =&lt;/strong&gt; $\mathcal{L}(q; \boldsymbol{\theta}) = - D_{\mathrm{KL}}(q(\mathbf{z}) | p(\mathbf{z}, \mathbf{x}; \boldsymbol{\theta}) )$. 我們可以反過來驗證&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\ln p(\mathbf{x}; \boldsymbol{\theta}) &amp;= \sum_z q(z) \ln p(\mathbf{x}; \boldsymbol{\theta}) \nonumber \\
&amp;= \sum_z q(z) \ln \frac{p(z, x; \theta)}{q(z)}  \frac{q(z)}{p(z \mid x; \theta)} \nonumber \\
&amp;= \sum_z q(z) \ln \frac{p(z, x; \theta)}{q(z)} + \sum q(z) \ln \frac{q(z)}{p(z \mid x; \theta)} \nonumber \\
&amp;= - D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z}, \mathbf{x}; \boldsymbol{\theta}) ) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \label{eqKL3} 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\eqref{eqKL3}$ 不免讓人浮想翩翩。 KL divergence 一定為大於等於 0.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果要 maximize (marginal) likelihood $\ln p(x; \theta)$, 好像正確的做法是讓 $\eqref{eqKL3}$ maximize 第一個 KL divergence 為 0； 第二個 KL divergence 越大越好？
    &lt;ul&gt;
      &lt;li&gt;e.g. let $q(z) = p(z, x; \theta) \to \ln p(x; \theta) = 0 + D_{K L}(p(z, x; \theta) | p(z \mid x; \theta) \ge 0$&lt;/li&gt;
      &lt;li&gt;但我們知道 $\ln p(x;\theta) &amp;lt; 0$, 如何解釋這個矛盾？&lt;/li&gt;
      &lt;li&gt;一個是 $\eqref{eqELBOKL}$ 寫成 KL divergence 有問題。因為 KL divergence 是兩個同樣 dimension distribution 的距離 measurement.  $\eqref{eqELBOKL}$ 的 joint distribution $(z, x)$ 的 dimension 大於 $q(z)$，寫成 KL divergence 無意義，也沒有距離的觀念。除非把 joint distribution marginalized 成 $p(z)$, i.e. prior, 才能和 $q(z)$ 做 KL divergence. 或者 with a fixed $x=c$, $\int p(z, x=c; \theta) = 1$ 才能滿足 distribution 的定義。&lt;/li&gt;
      &lt;li&gt;但是 conditional distribution $p(z\mid x)$, i.e. posterior 和 $q(z)$ 則是同樣的 dimension, KL divergence 有意義。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;實務上，我們的做法完全不同，甚至相反。正確的表示式 from $\eqref{eqKL}$ 得到：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\ln p(\mathbf{x}; \boldsymbol{\theta}) &amp;= \sum_z q(z) \ln \frac{p(z, x; \theta)}{q(z)} + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta})) 
\end{align} %]]&gt;&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;我們 maximize 第一項 lower bound (ELBO), 以及 minimize 第二項 KL divergence 為 0&lt;/li&gt;
      &lt;li&gt;e.g. $q(z) = p(z \mid x; \theta) \to \ln p(x; \theta) = E_{q(z)} \ln p(z, x; \theta) + H(q) + 0$&lt;/li&gt;
      &lt;li&gt;重點是 find $\theta^* = \arg \max_{\theta} E_{p(z\mid x; \theta)} \ln p(z, x; \theta)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;free-energy-interpretation-poczoscllusteringem2015&quot;&gt;Free Energy Interpretation [@poczosCllusteringEM2015]&lt;/h2&gt;
&lt;p&gt;搞 machine learning 很多是物理學家 (e.g. Max Welling), 習慣用物理觀念套用於 machine learning.  常見的例子是 training 的 &lt;em&gt;momentum&lt;/em&gt; method.  另一個是 &lt;em&gt;energy/entropy&lt;/em&gt; loss function.  此處我們看的是類似 energy loss function.&lt;/p&gt;

&lt;p&gt;我們從 gap 開始&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ln p(\mathbf{x} ; \boldsymbol{\theta})-\mathcal{L}(q, \boldsymbol{\theta}) = D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \ge 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(\mathbf{x} ; \boldsymbol{\theta}) &amp;= \mathcal{L}(q, \boldsymbol{\theta}) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \\
&amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln \frac{p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta})}{q(\mathbf{z})} + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \\
&amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) + \sum_{\mathbf{z}} -q(\mathbf{z}) \ln {q(\mathbf{z})}+ D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \\
&amp;= E_{q(z)} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}) + H(q) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}) ) \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where H(q) is the entropy of q,  第一項是負的，第二項和第三項是正的。
我們用一個例子來驗證
q = {0 or 1} with 50% chance, =&amp;gt; 
H(q) = 1 (bit) or ln (?) &amp;gt; 0
Eq(z) ln p(o, z) = -(0.5 (o-u1)^2 + 0.5 (o-u2)^2 ) / sqrt(2pi) &amp;lt; 0&lt;/p&gt;

&lt;p&gt;此處我們 switch to [@poczosCllusteringEM2015] notation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Observed data: $D = {x_1, \cdots, x_n}$&lt;/li&gt;
  &lt;li&gt;Unobserved/hidden variable: $z = {z_1, \cdots, z_n}$&lt;/li&gt;
  &lt;li&gt;Parameter: $\theta = [\mu_1, \cdots, \mu_K, \pi_1, \cdots, \pi_K, \Sigma_1, \cdots, \Sigma_K]$&lt;/li&gt;
  &lt;li&gt;Goal: $\boldsymbol{\theta}^{*}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ln p(D \mid \theta)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重寫上式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(D ; \boldsymbol{\theta}^t) &amp;= \sum_{\mathbf{z}} q(\mathbf{z}) \ln p(D, \mathbf{z} ; \boldsymbol{\theta}^t) + \sum_{\mathbf{z}} -q(\mathbf{z}) \ln {q(\mathbf{z})}+ D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid D; \boldsymbol{\theta}^t) ) \\
&amp;= E_{q(z)} \ln p(D, \mathbf{z} ; \boldsymbol{\theta}) + H(q) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid D; \boldsymbol{\theta}^t) ) \\
&amp;= F_{\theta^t} (q(\cdot), D) + D_{\mathrm{KL}}(q(\mathbf{z}) \| p(\mathbf{z} \mid D; \boldsymbol{\theta}) )
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;$F_{\theta^t} (q(\cdot), D)$ 稱為 free energy (也就是 ELBO), 包含 joint distribution expectation 和 self-entropy.&lt;/p&gt;

&lt;p&gt;如果 $p(z\mid x; \theta)$ is analytically available (e.g. GMM, this is just a softmax!).  The E-step 基本就是代入 $p(z\mid x; \theta)$ 到  LBO becomes a Q(theta, theta^old) function + H(q)&lt;/p&gt;

&lt;p&gt;The EM algorithm can be summzied as argmax Q!!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;E-step:  代入 $p(z\mid D)$ 到 free-energy (ELBO) update Q function (忽略 self-entropy)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;Q\left(\theta \mid \theta^{t}\right)=\int d y P\left(y \mid D, \theta^{t}\right) \log P(y, D \mid \theta)&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M-step; argmax Q&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{t+1}=\arg \max _{\theta} Q\left(\theta \mid \theta^{t}\right)&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It can be proved&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;log likelihood is always increasing! i.e. $\ln P(D\mid \theta^t) \le \ln P(D\mid \theta^{t+1})$  這是 EM 的重要特徵！&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/media/16270144925547/16274030539044.jpg&quot; alt=&quot;-w400&quot; style=&quot;zoom: 33%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/media/16270144925547/16274031504070.jpg&quot; alt=&quot;-w408&quot; style=&quot;zoom:33%;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use multiple, randomized initialization in practice to avoid strucking at local minima.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variational-expectation-maximization&quot;&gt;Variational Expectation Maximization&lt;/h2&gt;
&lt;p&gt;EM algorithm 一個問題是對於複雜的問題沒有 analytical from $p(z\mid x)$, then (1) variational EM; or (2) use neural network such as variational autoencoder (VAE).&lt;/p&gt;

&lt;p&gt;Variational EM 的重點是不用 Q function, 因為沒有 $p(z\mid x)$.  重點變成 minimize KL gap function for E-step.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Variational E-step:  Fix $\theta^t$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;q^{t}(\cdot)=\arg \max _{q(\cdot)} F_{\theta^{t}}(q(\cdot), D)=\underset{q(\cdot)}{\arg \min } K L\left(q(y) \| P\left(y \mid D, \theta^{t}\right)\right)&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;但並不保證會找到 best max/min  $q(y) = p(y \mid D, \theta^t)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Variational M-step; Fix $q^t$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{t+1}=\arg \max _{\theta} F_{\theta}\left(q^{t}(\cdot), D\right)&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variational EM 並不保證 marginal likelihood 每次都遞增！&lt;/li&gt;
  &lt;li&gt;關鍵問題是如何找到 $q(z)$, 下文會討論。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h4 id=&quot;例二的-conditional-vs-joint-distribution-解法&quot;&gt;例二的 Conditional Vs. Joint Distribution 解法&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我們之前的 E-Step 是猜 joint distribution, $p(t, s | a, b)$.&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_0$&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;$s_1$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$t_0$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5a&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$t_1$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3b&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;b&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;如果用上述的 conditional distribution 可以細膩的看每一個 data.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;對於所有 $(\bullet, s_0)$ 
&lt;script type=&quot;math/tex&quot;&gt;q(t \mid s_0, a, b)=\left\{\begin{array}{l}
q\left(t_{0}\right)=p\left(t_{0} \mid s_{0}, a, b\right)=\frac{a}{a+3 b} \\
q\left(t_{1}\right)=p\left(t_{1} \mid s_{0}, a, b\right)=\frac{3 b}{a+3 b}
\end{array}\right.&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;對於所有 $(\bullet, s_1)$ 
&lt;script type=&quot;math/tex&quot;&gt;q(t \mid s_1, a, b)=\left\{\begin{array}{l}
q\left(t_{0}\right)=p\left(t_{0} \mid s_{1}, a, b\right)=\frac{5a}{5 a+ b} \\
q\left(t_{1}\right)=p\left(t_{1} \mid s_{1}, a, b\right)=\frac{b}{5 a+ b}
\end{array}\right.&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;對於所有 $(t_0, \bullet)$ 
&lt;script type=&quot;math/tex&quot;&gt;q(s \mid t_0, a, b)=\left\{\begin{array}{l}
q\left(s_{0}\right)=p\left(s_{0} \mid t_{0}, a, b\right)=\frac{1}{6} \\
q\left(s_{1}\right)=p\left(s_{1} \mid t_{0}, a, b\right)=\frac{5}{6}
\end{array}\right.&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;對於所有 $(t_1, \bullet)$ 
&lt;script type=&quot;math/tex&quot;&gt;q(s \mid t_1, a, b)=\left\{\begin{array}{l}
q\left(s_{0}\right)=p\left(s_{0} \mid t_{1}, a, b\right)=\frac{3}{4} \\
q\left(s_{1}\right)=p\left(s_{1} \mid t_{1}, a, b\right)=\frac{1}{4}
\end{array}\right.&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;再來是例二的 M-Step&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;最後再把所有 dataset 的 weighted sum $(t_i, s_j)$ 統計出來，例如
$S_0$ 個 $(\bullet, s_0) \to \frac{a}{a+3b}S_0$ 個 $(t_0, s_0)$ 和 $\frac{3b}{a+3b}S_0$ 個 $(t_1, s_0)$
$S_1$ 個 $(\bullet, s_1) \to \frac{5a}{5a+b}S_1$ 個 $(t_0, s_1)$ 和 $\frac{b}{5a+b}S_1$ 個 $(t_1, s_1)$
$T_0$ 個 $(t_0, \bullet) \to \frac{1}{6}T_0$ 個 $(t_0, s_0)$ 和 $\frac{5}{6}T_0$ 個 $(t_0, s_1)$
$T_1$ 個 $(t_1, \bullet) \to \frac{3}{4}T_1$ 個 $(t_1, s_0)$ 和 $\frac{1}{4}T_1$ 個 $(t_1, s_1)$&lt;/p&gt;

&lt;p&gt;$(t_0, s_0)$ 個數 $\to N_{00} = \frac{1}{6}T_0+\frac{a}{a+3b}S_0$
$(t_0, s_1)$ 個數 $\to N_{01} = \frac{5}{6}T_0+\frac{5a}{5a+b}S_1$
$(t_1, s_0)$ 個數 $\to N_{10} = \frac{3}{4}T_1+\frac{3b}{a+3b}S_0$
$(t_1, s_1)$ 個數 $\to N_{11} = \frac{1}{4}T_1+\frac{b}{5a+b}S_1$&lt;/p&gt;

&lt;p&gt;因此可以使用完整 data 的 MLE estimation:
&lt;script type=&quot;math/tex&quot;&gt;a'=\frac{N_{00}+N_{01}}{6 N} \quad b'=\frac{N_{10}+N_{11}}{4 N}&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;to-do-next&quot;&gt;To Do Next&lt;/h2&gt;
&lt;p&gt;Go through GMM example.&lt;/p&gt;

&lt;p&gt;下一步 go through HMM model or simplest z -&amp;gt; o graph model.&lt;/p&gt;

&lt;p&gt;What is the mutual information of $o$ and $z$ in this case?&lt;/p&gt;

&lt;p&gt;假設可以有一個 close from Q function, e.g. GMM
&lt;strong&gt;In summary:  M-Step maximize the lower bound;  E-Step close the gap&lt;/strong&gt; 
E-Step
&lt;script type=&quot;math/tex&quot;&gt;q^{(t+1)}(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}; \boldsymbol{\theta}^{(t)})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;M-Step 
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\theta}^{(t+1)}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} E_{q(z)} \ln p(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}^{(t)})&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="softmax" /><category term="EM" /><summary type="html"></summary></entry><entry><title type="html">Jekyll Memo for Github Blog</title><link href="http://localhost:4000/language/2021/06/30/Jekyll-Memo/" rel="alternate" type="text/html" title="Jekyll Memo for Github Blog" /><published>2021-06-30T16:29:08+08:00</published><updated>2021-06-30T16:29:08+08:00</updated><id>http://localhost:4000/language/2021/06/30/Jekyll-Memo</id><content type="html" xml:base="http://localhost:4000/language/2021/06/30/Jekyll-Memo/">&lt;p&gt;幾個重點&lt;/p&gt;

&lt;h2 id=&quot;header&quot;&gt;Header&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;title line:  no other :,  wrong example:  title: Math AI : xxx =&amp;gt; the second : to be removed!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;tags: [xxx, xxx, xxx]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;table&quot;&gt;Table&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;目前 Jekyll + Next theme 造成 table column width 非常寬。 I don’t know the exact reason.  I changed the xxx/xxx.github.io/_sass/_common/scaffolding/tables.scss
    &lt;ul&gt;
      &lt;li&gt;width: 300px;&lt;/li&gt;
      &lt;li&gt;table-layout: auto;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;equation&quot;&gt;Equation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\$\$ math equation \$\$ =&amp;gt; leave empty lines “before” and “after” \$\$ \$\$! 也就是上下各要空一行！&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;${{ }}$  =&amp;gt; ${ \{ \}}$.  如果要打 {, 一定要加 \{.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Equation number:  必須先加上 header 如下。Reference: https://jdhao.github.io/2018/01/25/hexo-mathjax-equation-number/&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script type=&quot;text/x-mathjax-config&quot;&amp;gt;
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: &quot;AMS&quot; } }
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Equation 本體&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$\begin{equation}
E=mc^2
\end{equation}\label{eq1}$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;或是&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$\begin{align}
E=mc^2  \\         =&amp;gt; auto number 
p = mv \nonumber \\  =&amp;gt; without equation number
F = ma  \label{eqF} \\.  
\end{align}$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Equation citation use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$\eqref{eq1}$&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mweb 可以直接產生 equation number!&lt;/li&gt;
  &lt;li&gt;Typora 需要 enable :preference :Markdown :Auto Numbering Math Equations”.  不過結果很奇怪。所有的 equation 都有 number in Typora!  但 Jekyll 正常。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;image&quot;&gt;Image&lt;/h2&gt;
&lt;p&gt;Markdown resize image 似乎有問題，需要另外的 plug-in =&amp;gt; No!&lt;/p&gt;

&lt;p&gt;我找到一個 work around in Mweb!  使用 &amp;lt;img src …., width=””&amp;gt; 取代 Mweb copy and paste image.&lt;/p&gt;

&lt;p&gt;不過後來我發現 typora 可以直接做，所有 method 2 is using Typora&lt;/p&gt;

&lt;h3 id=&quot;method-1-mweb&quot;&gt;Method 1: Mweb&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Mweb: copy and paste image 自動產生。如下圖中的
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;![-w414](/media/16286850167880.jpg )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;此時同時在 editing window and preview window 都有圖。如下圖左和右上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;我找到的 work around 加在後一行。只會在 preview window 有圖。如下圖右下。
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;img src=&quot;/media/16286850167880.jpg&quot; width=&quot;414&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/16289455797795.jpg&quot; alt=&quot;-w993&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;問題是這種 image resize 只對 mweb 有效。在 Jekyll 之後的截圖如下 (127.0.0.1:4000)，就不對。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jekyll 直接忽略 [-w414] in the first image! 因此第一行產生原圖。但接受第二行的 image size
&lt;img src=&quot;/media/16289465237748.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I check the html source code&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/media/16289463211491.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;第一行轉譯的結果：alt=”-w245” 顯然被忽略。&lt;/li&gt;
      &lt;li&gt;第二行轉譯的結果：width=”245” 是正確結果。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;我找到的方法是把第一行改成第二行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;method-2-typora-how&quot;&gt;Method 2: Typora, How?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;首先要解決的是 root path 的問題！ Jekyll (and therefore github) 有 root path 的觀念。For my local root directory:  /Users/allenlu/Onedrive/allenlu2009.github.io.   文章是在 root: /_posts/xxx;  image 是在 /media/xxx&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mweb 似乎自動解決這個問題。 Image 直接 refer to:  /media/xxx.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Typora 如何設定？ 有兩個方法&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;直接在本文加上： typora-root-url: ../../allenlu2009.github.io&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/media/image-20210814233107185.png&quot; alt=&quot;image-20210814233107185&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Typora: Format: Image: Use Image Root Path: set to the above directory&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Typora insert image 必須先設定 image save path&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Preference: Image: Copy image to custom folder:
        &lt;ul&gt;
          &lt;li&gt;/Users/allenlu/OneDrive/allenlu2009.github.io/media&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Typora 在設定 display path 之後和 Jekyll 一樣，可以 display image, 但是不會 scale image size!!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果 mweb 改成 “&amp;lt;src img xxx&amp;gt;”  之後 OK.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;不過我發現有更好的方法，就是直接用 typora 的 image zoom 設定。自動就會轉成 &amp;lt;src img,  , zoom xxx&amp;gt; 可以 image resize!!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;editor&quot;&gt;Editor&lt;/h3&gt;

&lt;p&gt;Mweb&lt;/p&gt;

&lt;p&gt;Typora&lt;/p&gt;

&lt;p&gt;VS Code&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VS code default math rendering tool is KaTex, which is different from MathJax.  KaTex does not support /label and cross reference.   So I install “Mardown Preview Enhance” and switch the default math engine from KaTex to MathJax.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/image-20210911004231947.png&quot; alt=&quot;image-20210911004231947&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;問題是 MathJax mode is buggy!!  Not support \boldsymbol!!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;結論&quot;&gt;結論&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;使用 Typora for image resize (zoom), 當然要設好 typora-root-url (for display), and image save path.  結果是 typora, Mweb, Jekyll/github OK.&lt;/li&gt;
  &lt;li&gt;使用 Mweb,  需要手動改變 image to &amp;lt;src img …., width=”xxx”&amp;gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;推薦使用 1!!!&lt;/p&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="Jekyll" /><category term="Github" /><summary type="html">幾個重點</summary></entry><entry><title type="html">Typora and Mermaid</title><link href="http://localhost:4000/ai/2021/02/16/Typora-Mermaid/" rel="alternate" type="text/html" title="Typora and Mermaid" /><published>2021-02-16T16:29:08+08:00</published><updated>2021-02-16T16:29:08+08:00</updated><id>http://localhost:4000/ai/2021/02/16/Typora-Mermaid</id><content type="html" xml:base="http://localhost:4000/ai/2021/02/16/Typora-Mermaid/">&lt;p&gt;本文測試 Typora 加上 Mermaid script.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;sequenceDiagram
    participant Alice
    participant Bob
    Alice-&amp;gt;John: Hello John, how are you?
    loop Healthcheck
        John-&amp;gt;John: Fight against hypochondria
    end
    Note right of John: Rational thoughts &amp;lt;br/&amp;gt;prevail...
    John--&amp;gt;Alice: Great!
    John-&amp;gt;Bob: How about you?
    Bob--&amp;gt;John: Jolly good!
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
A[方形] --&amp;gt;B(圆角)
    B --&amp;gt; C{条件a}
    C --&amp;gt;|a=1| D[结果1]
    C --&amp;gt;|a=2| E[结果2]
    F[横向流程图]
&lt;/code&gt;&lt;/pre&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="softmax" /><summary type="html">本文測試 Typora 加上 Mermaid script.</summary></entry><entry><title type="html">Math ML - Modified Softmax w/ Margin</title><link href="http://localhost:4000/ai/2021/01/16/softmax/" rel="alternate" type="text/html" title="Math ML - Modified Softmax w/ Margin" /><published>2021-01-16T16:29:08+08:00</published><updated>2021-01-16T16:29:08+08:00</updated><id>http://localhost:4000/ai/2021/01/16/softmax</id><content type="html" xml:base="http://localhost:4000/ai/2021/01/16/softmax/">&lt;h1 id=&quot;math-ml---modified-softmax-w-margin&quot;&gt;Math ML - Modified Softmax w/ Margin&lt;/h1&gt;
&lt;p&gt;[@rashadAdditiveMargin2020] and [@liuLargeMarginSoftmax2017]
Softmax classification 是陳年技術，可還是有人在老幹上長出新枝。其中一類是在 softmax 加上 maximum margin 概念 (sometimes refers to metric learning), 另一類是在 softmax 所有 dataset 中找出 “supporting vectors” 減少 computation 卻不失準確率。實際做法都是從修改 loss function 著手。本文聚焦在第一類增加 margin 的 算法。&lt;/p&gt;

&lt;h2 id=&quot;softmax-in-dl-or-ml-recap&quot;&gt;Softmax in DL or ML Recap&lt;/h2&gt;
&lt;p&gt;Softmax 最常用於 DL (i.e. deep layers) 神經網絡最後一層(幾層)的 multi-class classification 如下圖。
&lt;script type=&quot;math/tex&quot;&gt;\sigma(j)=\frac{\exp \left(\mathbf{w}_{j}^{\top} \mathbf{x}\right)}{\sum_{k=1}^{K} \exp \left(\mathbf{w}_{k}^{\top} \mathbf{x}\right)}=\frac{\exp \left(z_{j}\right)}{\sum_{k=1}^{K} \exp \left(z_{k}\right)}&lt;/script&gt;
and
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial}{\partial z_{i}} \sigma\left(z_{j}\right)=\sigma\left(z_{j}\right)\left(\delta_{i j}-\sigma\left(z_{i}\right)\right)&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input vector, $\mathbf{x}$, dimension $n\times 1$.&lt;/li&gt;
  &lt;li&gt;Weight matrix, $\mathbf[w_1’, w_2’, .., w_K’]’$, dimension $K\times n$&lt;/li&gt;
  &lt;li&gt;Output vector, $\mathbf{z}$, dimension $K\times 1$.&lt;/li&gt;
  &lt;li&gt;Softmax output vector, $0\le\sigma(j)\le 1, j=[1:K]$, dimension $K\times 1$.&lt;/li&gt;
  &lt;li&gt;注意 bias 如果是一個 fixed number, $b$, softmax 分子分母會抵銷。bias 如果不同 $b_1, b_2, …, b_n$，可以擴展 $\mathbf{x’ = [x, }1]$ and $\mathbf{w’_j = [w_j}, b_j]$, 同樣如前適用。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16103750431293.jpg&quot; alt=&quot;-w718&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Softmax 也常用於 ML (i.e. shallow layers) 的 multi-class classification, 常和 SVM 一起比較。為了處理 nonlinear dataset or decision boundary, Softmax + kernel method 是一個選項。&lt;/p&gt;

&lt;p&gt;Softmax 另外用於 attention network, TBD.&lt;/p&gt;

&lt;h3 id=&quot;parameter-notation-and-range-for-ml-and-dl&quot;&gt;Parameter Notation and Range for ML and DL&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$N$: number of data points.  100 to 10,000 for ML, &amp;gt; 1M for DL.&lt;/li&gt;
  &lt;li&gt;$n$: input vector dimension. maybe from 1~ to 100~ for ML, 1000-4000 for DL.&lt;/li&gt;
  &lt;li&gt;$K$ or $m$ or $C$: output vector dimension, number of classes, maybe from 1 (binary) to 100 (Imaginet)&lt;/li&gt;
  &lt;li&gt;$k$: kernel feature space dimension, maybe from 10s’ - $\infty$ for ML.  Usually not use for DL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Summarize the result in table.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;N&lt;/th&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th&gt;k&lt;/th&gt;
      &lt;th&gt;K&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ML&lt;/td&gt;
      &lt;td&gt;100-10,000&lt;/td&gt;
      &lt;td&gt;1s’- 100s’&lt;/td&gt;
      &lt;td&gt;10s’- $\infty$&lt;/td&gt;
      &lt;td&gt;1s’-10s’&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DL&lt;/td&gt;
      &lt;td&gt;&amp;gt; 1M&lt;/td&gt;
      &lt;td&gt;1000-4000&lt;/td&gt;
      &lt;td&gt;NA&lt;/td&gt;
      &lt;td&gt;10-100&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;softmax-w-margin-via-training&quot;&gt;Softmax w/ Margin Via Training&lt;/h2&gt;
&lt;p&gt;根據前文討論，$w_i$ vectors 代表和 class &lt;em&gt;i&lt;/em&gt; data 的&lt;strong&gt;相似性&lt;/strong&gt;。&lt;br /&gt;
普通的 softmax classification 如下圖左所示。&lt;/p&gt;

&lt;p&gt;Decision boundary 是 data point 和 $w_1$ and $w_2$ 的機率一樣。
因為 softmax (or logistic regression) 只要求 $\sigma_1(x) &amp;gt; \sigma_2(x)$ or vice versa to classify $x \in$ class 1 (or class 2).  &lt;strong&gt;這裡完全沒有 margin 的觀念。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16103799219592.jpg&quot; alt=&quot;-w480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;推廣到 multiple class 更是如此，如下圖。因為是取 $\sigma(j)$ 的最大值。除了 $\sigma(j) &amp;gt; 0.5$ 有明顯的歸類。但在三不管地帶，很可能雜錯在一起。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;因爲 training 是基於 loss function, 解法是在 loss function 加入 margin term 做為 driving force (check the back-prop gradient!), 讓 training process 竭盡所能 “擠出” margin, 如上圖右。&lt;/strong&gt;
&lt;img src=&quot;/media/16102567367645/16103795147028.jpg&quot; alt=&quot;-w528&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;如何在-softmax-加入-margin-for-training&quot;&gt;如何在 softmax 加入 margin for training&lt;/h2&gt;
&lt;p&gt;SVM 是從 decision boundary 的平行線距離著手（margin = 1/|w|, minimize |w| ~ maximum margin)。
本文討論 Softmax 加上 margin 有三種方式，都是從&lt;strong&gt;角度&lt;/strong&gt; $\theta$ 著手，概念如圖二右 (平面角度)，或是下圖右 (球面角度)。maximize $\theta$ 剛好和 minimize |w| 正交 (orthogonal). 這是巧合嗎？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16105488389665.jpg&quot; alt=&quot;-w475&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我們先看 Softmax 的 loss function 如下圖。先是 softmax function, inference/test 只要 再來通過 cross-entropy loss.  Cross-entropy loss 對應 log likelihood. 
&lt;script type=&quot;math/tex&quot;&gt;L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}\right)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $f_{y_{i}}=\boldsymbol{W}&lt;em&gt;{y&lt;/em&gt;{i}}^{T} \boldsymbol{x}&lt;em&gt;{i}$ 代表 data $x_i$ 和 $W&lt;/em&gt;{y_i}$ 的相似性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16104627418371.jpg&quot; alt=&quot;-w300&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;三種用角度增加-softmax-inter-class-margin&quot;&gt;三種用角度增加 SoftMax inter-class margin&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;L-Softmax (Large Margin Softmax) [@liuLargeMarginSoftmax2017]&lt;/li&gt;
  &lt;li&gt;A-Softmax (Angular Softmax) [@liuSphereFaceDeep2018]&lt;/li&gt;
  &lt;li&gt;AM-Softmax (Additive Margin Softmax)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;l-softmax-large-margin-softmax-cos-theta-to-cos-mtheta&quot;&gt;L-Softmax (Large Margin Softmax): $\cos \theta \to \cos (m\theta)$&lt;/h4&gt;
&lt;p&gt;因為 $f_{j}=\left| \boldsymbol{W_j} \right|\left| \boldsymbol{x_i} \right|\cos\left(\theta_{j}\right)$.  如何在 $x_i$ 和 $W_j$ 加上 margin？  一個方法就是把 $\cos \theta$ 改成 $\cos m\theta$, why?&lt;/p&gt;

&lt;p&gt;從相似性來看，$\cos(m\theta)$ 在同樣的角度”相似性”掉的比較快。因此在 training 時會強迫把同一 feature 的 data 擠壓在一起, &lt;strong&gt;reduce the intra-class distance. 達到增加 inter-class margin 的目的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;另外可以從 decision boundary 理解。Softmax 的 decision boundary,&lt;/p&gt;

&lt;p&gt;$x\in$ Class 1:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right)&amp;gt;\left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$&lt;/p&gt;

&lt;p&gt;$x\in$ Class 2:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right) &amp;lt; \left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$&lt;/p&gt;

&lt;p&gt;and $\theta_1 + \theta_2 = \theta$ which is the angle between $W_1$ and $W_2$&lt;/p&gt;

&lt;p&gt;如果把 $\cos \theta \to \cos (m\theta)$,&lt;/p&gt;

&lt;p&gt;$x\in$ Class 1:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( m\theta_{1}\right)&amp;gt;\left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$.
 &lt;/p&gt;

&lt;p&gt;Assuming $|W_1| = |W_2| \to \theta_1 &amp;lt; \theta_2/m$, 因為 $\cos\theta$ 是遞減函數。&lt;/p&gt;

&lt;p&gt;$x\in$ Class 2:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right) &amp;lt; \left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(m\theta_{2}\right)$.&lt;/p&gt;

&lt;p&gt;Assuming $|W_1| = |W_2| \to \theta_1/m &amp;gt; \theta_2$.&lt;/p&gt;

&lt;p&gt;此時我們有兩個 decision boundaries, 兩個 boundaries 之間可以視為 decision margin, 如下圖。
&lt;img src=&quot;/media/16102567367645/16107219487390.jpg&quot; alt=&quot;-w400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In summary, 就是在 labelled $c$ class 的 data 時，就把對應的 $\cos\theta_c$ 改成 $\cos (m\theta_c)$. $m$ 愈大，margin 就愈大。但過之猶如不及，如果 $m$ 太大，可能無法正確 capture features (TBC)? $m$ 應該有一個 optimal value.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{i}=-\log \left(\frac{e^{\left\|\boldsymbol{W}_{y_{i}}\right\|\left\|\boldsymbol{x}_{i}\right\| \psi\left(\theta_{y_{i}}\right)}}{e^{\left\|\boldsymbol{W}_{y_{i}}\right\|\left\|\boldsymbol{x}_{i}\right\| \psi\left(\theta_{y_{i}}\right)}+\sum_{j \neq y_{i}} e^{\left\|\boldsymbol{W}_{j}\right\|\left\|\boldsymbol{x}_{i}\right\| \cos \left(\theta_{j}\right)}}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\psi(\theta)=\left\{\begin{array}{l}
\cos (m \theta), \quad 0 \leq \theta \leq \frac{\pi}{m} \\
\mathcal{D}(\theta), \quad \frac{\pi}{m}&lt;\theta \leq \pi
\end{array}\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;為什麼會有 $D(\theta)$？ 原因是要維持 $\psi(\theta)$ 的&lt;strong&gt;遞減性，連續性，和可微分性&lt;/strong&gt; over $[0, \pi]$.  一旦定義出 $\psi(\theta)$ over $[0, \pi]$. 左右 flip (y 軸對稱) 得到 $\theta\in[-\pi, 0]$. 其他的 $\theta$ 都可以移到 $[-\pi, \pi]$.&lt;/p&gt;

&lt;p&gt;舉一個例子如下式，$\psi(\theta)$ 的 curve 如下圖。
&lt;script type=&quot;math/tex&quot;&gt;\psi(\theta)=(-1)^{k} \cos (m \theta)-2 k, \quad \theta \in\left[\frac{k \pi}{m}, \frac{(k+1) \pi}{m}\right]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16107278677656.jpg&quot; alt=&quot;-w386&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;a-softmax-angular-softmax-cos-theta-to-cos-mtheta-and-w1&quot;&gt;A-Softmax (Angular Softmax): $\cos \theta \to \cos (m\theta)$ and $|W|=1$&lt;/h4&gt;
&lt;p&gt;在 L-Softmax 可以同時調整 $|W|$ and $\theta$, 在 A-Softmax 進一步限制 $|W|=1$, 其他都和 L-Softmax 相同。A-Soft 的 Loss function 如下， 
&lt;script type=&quot;math/tex&quot;&gt;L_{\mathrm{ang}}=\frac{1}{N} \sum_{i}-\log \left(\frac{e^{\left\|\boldsymbol{x}_{i}\right\| \cos \left(m \theta_{y_{i}, i}\right)}}{e^{\left\|\boldsymbol{x}_{i}\right\| \cos \left(m \theta_{y_{i}, i}\right)}+\sum_{j \neq y_{i}} e^{\left\|\boldsymbol{x}_{i}\right\| \cos \left(\theta_{j, i}\right)}}\right)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;後來有再修正 $\psi(\theta)$, 多加一個 hyper-parameter $\lambda$, angle similarity curve 如下圖。注意 A-Softmax 的 $\psi(0)=1.$
&lt;script type=&quot;math/tex&quot;&gt;\psi(\theta)=\frac{(-1)^{k} \cos (m \theta)-2 k+\lambda \cos (\theta)}{1+\lambda}&lt;/script&gt;
&lt;img src=&quot;/media/16102567367645/16108061480853.jpg&quot; alt=&quot;-w427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因為 $|W|=1$, A-Softmax 一個用途是 hyper-sphere explanation 如下圖。理論上 L-Softmax 包含 A-Softmax, 但在某一些情況下，A-Softmax 似乎效果更好，less is more? (同一作者，2017 L-SoftMax; 2018 A-Softmax).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16108029553353.jpg&quot; alt=&quot;-w648&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;am-softmax-additive-margin-softmax-cos-theta-to-cos-theta--m&quot;&gt;AM-Softmax (Additive Margin Softmax): $\cos \theta \to \cos \theta -m$&lt;/h4&gt;
&lt;p&gt;AM-Softmax 非常有趣，它把 $\cos\theta \to \cos(m\theta) \to \cos\theta -m$, 也就是，
&lt;script type=&quot;math/tex&quot;&gt;\psi(\theta)=\cos \theta-m&lt;/script&gt;
AM-Softmax 的 loss function, 但多了一個 hyper-parameter $s$(?)
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathcal{L}_{A M S} &amp;=-\frac{1}{n} \sum_{i=1}^{n} \log \frac{e^{s \cdot\left(\cos \theta_{y_{i}}-m\right)}}{e^{s \cdot\left(\cos \theta_{y_{i}}-m\right)}+\sum_{j=1, j \neq y_{i}}^{c} e^{s \cdot \cos \theta_{j}}} \\
&amp;=-\frac{1}{n} \sum_{i=1}^{n} \log \frac{e^{s \cdot\left(W_{y_{i}}^{T} \boldsymbol{f}_{i}-m\right)}}{e^{s \cdot\left(W_{y_{i}}^{T} \boldsymbol{f}_{i}-m\right)}+\sum_{j=1, j \neq y_{i}}^{c} e^{s W_{j}^{T} \boldsymbol{f}_{i}}} .
\end{aligned} %]]&gt;&lt;/script&gt;
這有很多好處：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;不用再分段算 $\psi(\theta)$, forward and backward 計算變成很容易。&lt;/li&gt;
  &lt;li&gt;$m$ 是 continuous variable, 不是 discrete variable in A-Softmax. $m$ 可以 fine-grain optimized hyper-parameter. 而且是 differentiable, 我認為可以是 trainable variable.&lt;/li&gt;
  &lt;li&gt;AM-Softmax 同時 push angle and magnitude?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;qa&quot;&gt;Q&amp;amp;A&lt;/h2&gt;
&lt;p&gt;Q. Data 不是固定的嗎？為什麼會隨 loss function 改變？
A. 此處是假設 CNN network 的最後一層是 Softmax, 因此 input data 對應的 feature extraction 並非固定而且會隨 loss function 改變如下圖。如果 input data 直接進入 Softmax with or without margin, the input data 顯然不會改變，但是 decision boundary may change? (next Q)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/16102567367645/16107599078389.jpg&quot; alt=&quot;-w456&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Q. 在 inference/test 時，以上的公式 (check class $c$) 加起來不等於 1？ 如何解決？
A: 以上的公式只用於 training 增加 margin? 在 inference/test 時，仍然用原來的 softmax 公式，因此機率仍然為 1.&lt;/p&gt;

&lt;p&gt;Q. 以上 $cos(m \theta)$ 的 $m$ 一定要整數嗎？
A. 整數可以定義 continuous and differentiable loss function in $0-\pi$ 角度。上上圖的角度顯示 $0-\pi/2$ 角度，$\pi/2 - \pi$ 是 $0-\pi/2$ 的左右 flip curve.  如果 $m$ 不是整數，在 $\pi/2$ is non-differentiable.  另外也讓 loss function 的分段比較麻煩。不過我認為這都不是什麼問題。重點是 $m$ 不是整數有沒有用？ 我認為有用，可以視為另一個 hyper-parameter, or trainable parameter for optimization!  $m$ 太小沒有 margin, $m$ 太大會 filter out some features (under-fit)?&lt;/p&gt;

&lt;h2 id=&quot;策略同時使用角度-maximize-theta-and-magnitude-minimize-w&quot;&gt;策略：同時使用角度 maximize $\theta$ and Magnitude minimize $|w|$！&lt;/h2&gt;
&lt;p&gt;Magnitude margin: 增加 inter-class margin?
Angle margin: compress intra-class?
先 push 角度，再 push w, 再角度, ….
角度 m, make it differentiable!&lt;/p&gt;

&lt;h2 id=&quot;to-do&quot;&gt;To Do&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;check the SVM, check the logistic regression, check import vector&lt;/li&gt;
  &lt;li&gt;Use binary classification as an example&lt;/li&gt;
  &lt;li&gt;Pro and Con of the three types.&lt;/li&gt;
  &lt;li&gt;Most importantly, try to use both amplitude and angle for learning!!  TBD&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;Liu, Weiyang, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le
Song. 2018. “SphereFace: Deep Hypersphere Embedding for Face
Recognition.” January 29, 2018. &lt;a href=&quot;http://arxiv.org/abs/1704.08063&quot;&gt;http://arxiv.org/abs/1704.08063&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Liu, Weiyang, Yandong Wen, Zhiding Yu, and Meng Yang. 2017.
“Large-Margin Softmax Loss for Convolutional Neural Networks.” November
17, 2017. &lt;a href=&quot;http://arxiv.org/abs/1612.02295&quot;&gt;http://arxiv.org/abs/1612.02295&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Rashad, Fathy. n.d. “Additive Margin Softmax Loss (AM-Softmax).” Medium.
Accessed December 27, 2020.
&lt;a href=&quot;https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b&quot;&gt;https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Wang, Feng, Weiyang Liu, Haijun Liu, and Jian Cheng. 2018. “Additive
Margin Softmax for Face Verification.” May 30, 2018.
&lt;a href=&quot;https://doi.org/10.1109/LSP.2018.2822810&quot;&gt;https://doi.org/10.1109/LSP.2018.2822810&lt;/a&gt;.&lt;/p&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="softmax" /><summary type="html">Math ML - Modified Softmax w/ Margin [@rashadAdditiveMargin2020] and [@liuLargeMarginSoftmax2017] Softmax classification 是陳年技術，可還是有人在老幹上長出新枝。其中一類是在 softmax 加上 maximum margin 概念 (sometimes refers to metric learning), 另一類是在 softmax 所有 dataset 中找出 “supporting vectors” 減少 computation 卻不失準確率。實際做法都是從修改 loss function 著手。本文聚焦在第一類增加 margin 的 算法。</summary></entry><entry><title type="html">Math AI - G-CNN (Group + CNN)</title><link href="http://localhost:4000/ai/2020/05/08/G-CNN/" rel="alternate" type="text/html" title="Math AI - G-CNN (Group + CNN)" /><published>2020-05-08T16:29:08+08:00</published><updated>2020-05-08T16:29:08+08:00</updated><id>http://localhost:4000/ai/2020/05/08/G-CNN</id><content type="html" xml:base="http://localhost:4000/ai/2020/05/08/G-CNN/">&lt;h1 id=&quot;math-ai---g-cnn-group--cnn&quot;&gt;Math AI - G-CNN (Group + CNN)&lt;/h1&gt;
&lt;p&gt;Where is group theory (G-CNN) + Curved Space (Spherical CNN)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Manifold learning 是機器學習的分支，屬於淺層學習 (shallow learning).  Manifold learning 的技巧 (kernel PCA?, Laplacian Eigenmap, etc.) 是否能用於&lt;strong&gt;深度學習&lt;/strong&gt;？ Yes, via kernel!   PCA =&amp;gt; CNN kernel;  LE etc. =&amp;gt; geometric kernel?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Why 結合深度學習和 manifold learning?
    &lt;ul&gt;
      &lt;li&gt;深度學習 based on CNN kernel =&amp;gt; translation covariant (not invariant, invariant 是指純量 independent of coordinate system, e.g. Lagrangian, action, or $ds^2$.  Covariant means coordinate …) on 2D Euclidean plane,  Need based on ??? kernel  =&amp;gt; translation/rotation covariant on manifold  =&amp;gt; 結合深度學習和 manifold learning&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;可以減少 training set!&lt;/strong&gt;  因為 manifold learning 自帶 translation/rotation covariant, 甚至可以 extend to manifold deformation (e.g.姿體移動?)  可以結合 prior information? (姿體移動，蛋白質移動,旋轉,鏡像 …)&lt;/li&gt;
      &lt;li&gt;Can this resist adversarial attack?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;translation equivariant - CNN, plus rotation/mirror equivariant - g-CNN&lt;/li&gt;
  &lt;li&gt;then sphere equivariant - sphere CNN (non-flat); finally ??&lt;/li&gt;
  &lt;li&gt;How about scale invariant or equivariant?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;終於了解 G-CNN 的意義，就是把 kernel 2D convolution (Z2 commutative group) expand to a 4D G-convolution (p4m: Di4 non-commutative group).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;只有 input image 是 (x, y) base, 經過 layer-1 G-convolution 轉為 p4m g space.  所有之後的 layers’ convolution 都是在 g space 做, i.e. input and output activation 都是在 {4D g space + 1D Depth=5D} space instead of {2D (x,y) + 1D depth = 3D} space.  到了最後 fully connected 再變成分類網路。  這真是 particle physicist 才會有的高維思維！一般人還是習慣每一層 input output activation 老老實實在 2D (x,y) space.  (example: https://arxiv.org/pdf/1807.11156.pdf).  I like this idea: Go high dimension all the way!  In some sense, channel or depth dimension 也是一個人造的 dimension!&lt;/li&gt;
  &lt;li&gt;More parameters?  Should be.&lt;/li&gt;
  &lt;li&gt;Still can find the (x,y) for location?  Yes, it is a superset!&lt;/li&gt;
  &lt;li&gt;Use 1D convolution with mirror group as an example.&lt;/li&gt;
  &lt;li&gt;How about broken symmetry? 或是 miss some kernel?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;再推廣-group-equivariance-estevespolartransformer2018&quot;&gt;再推廣 Group Equivariance [@estevesPOLARTRANSFORMER2018]&lt;/h2&gt;
&lt;p&gt;Equivariant representations are highly sought after as they encode both class and deformation information in a predictable way. Let $G$ be a transformation group and $L_g I$ be the group action applied to an image $I$. A mapping $\Phi : E \to F$ is said to be equivariant to the group action $L_g$, $g \in G$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Phi\left(L_{g} I\right)=L_{g}^{\prime}(\Phi(I))&lt;/script&gt;

&lt;p&gt;where $L_g$ and $L’&lt;em&gt;g$ correspond to application of $g$ to $E$ and $F$ respectively and satisfy $L&lt;/em&gt;{gh} = L_g L_h$ and $L’_{gh} = L’_g L’_h$.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Invariance is a special case of equivariance where $L’_g = I$.&lt;/li&gt;
  &lt;li&gt;Another special case is $L_g = L’_g$.&lt;/li&gt;
  &lt;li&gt;Image classification and CNN, $g \in G$ can be thought of as an image deformation and $\Phi$ a mapping from the image to a feature map.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next step:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Image $I$ is a function of coordinate, x, $I = f(x)$ at first layer.&lt;/li&gt;
  &lt;li&gt;Group operation on f(x) is 
 $L_g f(x) = f(g^{-1}x)$.  原因很簡單，就是在 $x = gx’$ 會得到原來的 $f(x’)$.&lt;/li&gt;
  &lt;li&gt;2D discrete convolution, $L_g f(x) = &lt;a href=&quot;g^{-1}x&quot;&gt;f\circ \phi&lt;/a&gt; $ 定義如下。$x, y \in Z^2$&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[f * \phi](x)=\sum_{y \in \mathbb{Z}^{2}} f(y) \phi(x-y)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[f \star \phi](x)=\sum_{y \in \mathbb{Z}^{2}} f(y) \phi(y-x)&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;CNN 3D convolution, $L_g f(x) = &lt;a href=&quot;g^{-1}x&quot;&gt;f\circ \phi&lt;/a&gt; $ 定義如下。$x, y \in Z^2$; $k$ and $i$ 分別代表 input/output channel depth&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[f * \phi^i](x)=\sum_{y \in \mathbb{Z}^{2}} \sum_{k=1}^{K^{l}} f_{k}(y) \phi^i_{k}(x-y)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[f \star \phi^i](x)=\sum_{y \in \mathbb{Z}^{2}} \sum_{k=1}^{K^{l}} f_{k}(y) \phi^i_{k}(y-x)&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;推廣到 2D group convolution.  $g, h \in G$&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(f *_{G} \phi)(g)=\int_{h \in G} f(h) \phi(h^{-1} g) d h&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(f \star_{G} \phi)(g)=\int_{h \in G} f(h) \phi(g^{-1} h) d h&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;推廣到 3D Group CNN or G-CNN.  $g, h \in G$, $k$ and $i$ 分別代表 input/output channel depth
&lt;script type=&quot;math/tex&quot;&gt;\begin{array}{l}
{\left[f * \phi^i\right](g)=\sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \phi^i_{k}(h^{-1}g)} \\
{\left[f \star \phi^i\right](g)=\sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \phi^i_{k}(g^{-1}h)}
\end{array}&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Convolution and CNN 具有 translational equivariance and independent of kernel $\phi$.  直觀而言，就是把 input image (or feature map) 和 kernel filter 的 symmetry group (e.g. translation, rotation, reflection) 做 similarity (inner product), 但保留印記 (coordinate (x,y), reflection (m=1, -1), rotation (r=0, 1, 2, 3)) 到 output feature map.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2D convolution or 3D CNN 的 $g^{-1} h = y-x$  and $h^{-1} g = (g^{-1} h)^{-1} = x-y$ 是 $Z^2$ 反元素。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;其中 layer1 的 input image 因為只有 2D coordinate (x,y) + 1D depth (c=3, e.g. RGB) = 3D tensor, 但是 output feature map 變成 2D coordinate (x,y) + 1D reflection(m) + 1D rotation(r) + 1D depth (c) = 5D tensor.&lt;/li&gt;
  &lt;li&gt;其他 layers 的 input and output 都是 5D tensors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;group-equivariant-operation&quot;&gt;Group Equivariant Operation&lt;/h2&gt;

&lt;p&gt;參考 [@prismGroupEquivariant2019] and [@cohenGroupEquivariant2019].&lt;/p&gt;

&lt;p&gt;在這篇文章中，作者以初學者的角度，從最基本的概念開始，解釋對稱性並通俗地引入群論的理論框架。所謂對稱性，就是目標在經歷一定變換以後保持不變的性質。而這裡用到的對稱性群（symmetry group），可理解為一系列滿足某些限制條件的對稱性變換的集合。下面是文中對對稱性群的定義：&lt;/p&gt;

&lt;p&gt;而在卷積網絡裡面涉及到的，最簡單的例子就是二維整數平移操作所組成的群 $\mathbb{Z}^2$。&lt;/p&gt;

&lt;p&gt;接下來，我們簡單回顧一下傳統卷積網絡的等變（Equivariance）性質。平移等變性質是CNN對目標的響應能夠不受目標在圖像中的位置影響的基礎。《深度學習》花書裡面是這樣描述等變性質：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果一個函數滿足，輸入改變而輸出也以同樣的方式改變的這一性質，我們就說它是等變的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;簡單的例子，就是當目標出現在輸入圖片中的不同位置，輸出的feature map應該是只是進行了平移變換。
&lt;img src=&quot;/media/15790137525682/15866188574631.jpg&quot; alt=&quot;-w600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而從數學上，從算符対易性的角度，等變性質可以這樣定義：對於群對稱 $g \in G$ ，其算符 $L_g$ 和函數 $f(x)$，有 $f(L_g x) = L_g(f(x))$ ，也就是 $f$ 與 $L_g$ 対易，則稱$f(x)$ 對於變換 $g$ 有等變性。&lt;/p&gt;

&lt;p&gt;在深度學習當中，我們更希望卷積網絡具有等變性，而不是不變性（Invariance）:
&lt;img src=&quot;media/15790137525682/15866220196651.jpg&quot; alt=&quot;-w600&quot; /&gt;
在畢卡索的這幅畫中，臉部五官都在，但是顯然被解構和調整。如果神經網絡對目標的響應具有「不變性」，顯然仍然會認為這就是一張普通人臉。&lt;/p&gt;

&lt;p&gt;接下來作者引入一個結論：&lt;/p&gt;

&lt;p&gt;這個公式的含義是：要得到經過 [公式] 變換的feature map [公式] 在 [公式] 處的值，可以通過計算在 [公式] 位置上面 [公式] 的值。舉例來說，如果 [公式] 是平移操作t，則 [公式] ，那我們只需計算在 [公式] 這一點feature的值便可得到。這個公式將在推到等變性的時候用到。&lt;/p&gt;

&lt;p&gt;對於傳統卷積網絡， [公式] 則對應平移操作 [公式] 。也就是說，由於平移操作雖然會對卷積操作的輸出產生改變，但是這種改變是線性的，可以預測的。反之，不等變的操作則會對輸出帶來非線性的影響。&lt;/p&gt;

&lt;p&gt;為了證明傳統卷積網絡裡面，平移與卷積操作対易，首先明確定義傳統卷積操作和互相關操作：&lt;/p&gt;

&lt;p&gt;在這裡，filter對輸入層的滑動掃描被看做對其平移操作。需要注意的是在傳統的卷積網絡裡面，前向過程事實上用的是互相關操作卻被泛泛稱為「卷積操作」。&lt;/p&gt;

&lt;p&gt;然後文章中很容易證明瞭互相關操作( [公式] )和卷積（ [公式] ）操作都與平移操作 [公式] 対易（commute）:&lt;/p&gt;

&lt;p&gt;[公式]&lt;/p&gt;

&lt;p&gt;[公式]&lt;/p&gt;

&lt;p&gt;由這兩個操作対易，從而得出結論：卷積是平移操作的等變映射。&lt;/p&gt;

&lt;p&gt;另外一方面，作者發現旋轉操作與卷積操作是不対易的，「correlation is not an equivariant map for the rotation group」，但是feature map的堆疊卻可能是等變的。也正是因為旋轉操作不是卷積的等變映射，往傳統的CNN裡面輸入旋轉了的圖像，圖像識別的效果則會大打折扣。為瞭解決這個問題，最傳統直接的方法是數據增強，直接把圖像旋轉再輸入網絡進行訓練，但是這種方法顯然不是最優的。為了改進網絡本身來解決這個問題，考慮一個簡單的具有四重旋轉對稱軸的對稱性群 [公式] (wiki). 對於這個群，有四種對稱性操作：平移，旋轉90°，旋轉180°，旋轉270°。我們要設計一個新的CNN結構，使得當輸入圖像有以上變換時，網絡仍然具有等變性質。&lt;/p&gt;

&lt;p&gt;為了這個目的，仿照(2)(3)，根據(1)的結論，作者提出的 G-correlation，其定義為：&lt;/p&gt;

&lt;p&gt;對於第一層G-CNN（first-layer G-correlation）， [公式] 和[公式] 定義在平面 [公式] 上：&lt;/p&gt;

&lt;p&gt;[公式]&lt;/p&gt;

&lt;p&gt;對於接下來的G-CNN層（full G-correlation）， [公式] 和[公式] 定義在群G上：&lt;/p&gt;

&lt;p&gt;[公式]&lt;/p&gt;

&lt;p&gt;由此帶來的改變是，作者很容易證明瞭G-CNN對於群G的變換操作是等變的（「G-correlation is an equivariant map for the translation group」）: [公式]&lt;/p&gt;

&lt;p&gt;詳細推導見文章。值得注意的是， 經師弟提醒，對第一層G-CNN的等變性推到，需要把 [公式] 和 [公式] 拓展到群 [公式] 上，否則將無法推導（因為 [公式] 顯然不再屬於群 [公式] ）。&lt;/p&gt;

&lt;p&gt;也就是說，G-CNN推廣了對feature map的變換操作，從傳統的只有平移變換的群 [公式] 到某個對稱性群 [公式] 。而且推廣以後，G-CNN卷積層對於該群的對稱性變換操作具有等變性質。&lt;/p&gt;

&lt;p&gt;雖然作者在文中沒有提及，不難看到，G-CNN可以自然退化到傳統的CNN。當對稱性群G只有平移 [公式] 一種對稱性操作，也就是 [公式] 時，則G-CNN也就是傳統的CNN。&lt;/p&gt;

&lt;p&gt;總而言之，當輸入圖像是按照特定角度旋轉的，G-CNN網絡的輸出結果應該是按照預定規律變化的。因此，G-CNN具備了更強的旋轉輸入圖像特徵提取的能力。&lt;/p&gt;

&lt;p&gt;可以完全從 math 角度來看深度學習。
CNN 的核心是 convolution, math 抽象來看是 Euclidean translation invariance (Z^2).  更進一步的是 Euclidean rotation invariance (U(1), SO(2) group?).  或者 manifold (sphere) translation/rotation invariance.&lt;/p&gt;

&lt;p&gt;Gauge Convolutional Networks
[@xinzhiyuanGeometricalDeep2020] and [@pavlusIdeaPhysics2020]
https://kknews.cc/tech/gpkgx3e.html&lt;/p&gt;

&lt;h2 id=&quot;math-formulation&quot;&gt;Math Formulation&lt;/h2&gt;
&lt;p&gt;前面說的都是描述性的語言，再來是干貨。
先澄清一些&lt;em&gt;無關&lt;/em&gt;的 ideas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Symmetric Group&lt;/strong&gt; 
Group theory 中的 symmetric group 有明確的定義，就是 n symbol 所有 permutation (i.e. self bijections) 形成的 group, 稱為 $S_n$, with $n!$ group element. 下圖是 $S_4$ 的 Cayley graph, total 4! = 24 elements. 所有的 finite group 可以證明都是某個 symmetric group 的子群。&lt;strong&gt;不過這裡的 symmetric group 和本文無關。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;media/15790137525682/15873110898403.jpg&quot; alt=&quot;-w400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Symmetry Brings Conservation (Noether Theorem)&lt;/strong&gt; (check 廣義相對論 article)
A physic law is invariant of different observer.  For example the Lagrangian is invariant (or covariant?) under certain coordinate transformation (different observers).  We called these coordinate transformation as symmetry operation.  These symmetry operation corresponds to a specific conservation law.&lt;/p&gt;

&lt;p&gt;再來進入主題。&lt;/p&gt;

&lt;h3 id=&quot;equivariance-math-formulation-of-neural-network&quot;&gt;Equivariance Math Formulation of Neural Network&lt;/h3&gt;

&lt;p&gt;$y = \Phi(x)$ where $\Phi$ represents (part of) the neural network. $y$ is network output feature tensor; x is input image tensor.  Tensor can be viewed as a high dimension matrix.&lt;br /&gt;
$\Phi$ 可以是一個複雜的 cascaded nonlinear function (with CNN, ReLU, Pooling, etc.) or a simple linear function with tensor input and tensor output.&lt;/p&gt;

&lt;p&gt;$\Phi$ 可以是 injective/bijective or non-injective.  例如，input image tensor 是 WxHxCin, 如果 output tensor 是 WxHxCout (stride=1) and Cout $\ge$ Cin, 一般是 injective or bijective.  如果 stride &amp;gt; 1 or Cout &amp;lt; Cin, 則是 non-injective, 也就是存在 $x’\ne x$, and $\Phi(x’) = \Phi(x)$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;media/15790137525682/15873841531452.jpg&quot; alt=&quot;-w766&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$x’ = T x$ where $T$ is a linear transformation (&lt;strong&gt;a multi-dimension matrix&lt;/strong&gt;) corresponding to a new observer (coordinate).  此處 T 是 bijective transform, or full rank transformation, 例如 translation, rotation, affine transformation.&lt;/p&gt;

&lt;p&gt;The new observer obtains the new output feature tensor 
$y’ = \Phi(x’) = \Phi(T x)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Our goal is to explore the relationship between $\Phi(T x)$ and $\Phi(x)$.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In general, $\Phi(T x)$ 和 $\Phi(x)$ 可能有各種不同的關係。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;如果 $\Phi(T x) = \Phi(x) \; \forall x$, 滿足的所有 $T$ 稱為 $\Phi$ 的 invariant group.
    &lt;ul&gt;
      &lt;li&gt;Ex. $\Phi$ is norm of x, $T_g$ 是所有 metric-perserve transformation (translation, rotation, mirror, etc.)&lt;/li&gt;
      &lt;li&gt;It losses all T information, all completely independent of coordinate.&lt;/li&gt;
      &lt;li&gt;Usually for scalar.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果 $\Phi(T x) = T \Phi(x) \; \forall x$, 滿足的所有 $T$ 稱為 $\Phi$ 的 equivariant group.
    &lt;ul&gt;
      &lt;li&gt;Keep spatial information&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;equivariant-group-phi-is-linear-and-bijective-full-rank&quot;&gt;Equivariant Group: $\Phi$ is Linear and Bijective (full rank)&lt;/h3&gt;

&lt;p&gt;If $\Phi$ is a linear network, 可視為一個 matrix $\Phi$, i.e. $\Phi(T x) = \Phi T x$.  為了簡化，假設 $\Phi$ and $T$ 是 2D matrix.&lt;/p&gt;

&lt;p&gt;現在需要找到 given $\Phi$, 什麼 $T$ 可以得到 
$\Phi(T x) = \Phi T x = T \Phi x = T \Phi(x)$ for $\forall x$
$\Rightarrow \Phi T = T \Phi$, 也就是 $\Phi$ and $T$ commute, 因此變成 commuting matrices problem, 可以參考 [@wikiCommutingMatrices2019].&lt;/p&gt;

&lt;p&gt;One sufficient (not a necessary) condition: $\Phi$ and $T$ are simultaneously diagonalizable, i.e. 
$\Phi = P^{-1} D P$ and $T = P^{-1} Q P$ where $D$ and $Q$ 都是 diagonal matrix. 
$\Phi T = P^{-1} D Q P = P^{-1} Q D P = T \Phi$&lt;/p&gt;

&lt;p&gt;也就是只要 $T = P^{-1} Q P$ where P comes from eigenvectors of $\Phi$,  $\Phi T x = T \Phi x \; \forall x$.  Commuting matrices preserve each other’s eigenspaces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;這樣的 $T$ form a commuting (Abelian) group $T_g$ (assuming T is full rank, exclude 0 in the eigenvalues of T and Q)&lt;/strong&gt;, 因為 $T_1  T_2 = P^{-1} Q_1 P P^{-1} Q_2 P = P^{-1} (Q_1 Q_2) P = P^{-1} (Q_2 Q_1) P = T_2 T_1 = T_3$ (multiplication closure and commuting), 並且每一個 $T$ 都存在唯一的反元素 $P^{-1} Q^{-1} P$ (inverse closure).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In summary, given a linear and bijective network $\Phi$, 可以定義一個 equivarient commutative group $T_g$ such that $\Phi(T x) = T \Phi(x) \; \forall x$.  這個群的元素(matrix) 的 eigenvectors 都和 $\Phi$ eigenvectors 一致。&lt;/strong&gt;  也可以把 $\Phi$ 視為這個 group, $T_g$ 的一個 element.&lt;/p&gt;

&lt;h4 id=&quot;simple-phi-2d-matrix-equivariant-group-example&quot;&gt;Simple $\Phi$: 2D Matrix Equivariant Group Example&lt;/h4&gt;
&lt;p&gt;Ex1: $\Phi$ = [1, 0; 0, 2]  $\Rightarrow T_g =[k_1, 0; 0, k_2]$. 所有 &lt;strong&gt;unequal scaling 都是 equivariant group.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ex2: $\Phi$ = [2, 1; 1, 2]  $\Rightarrow T_g =[c, s; s, c]$.  所有 &lt;strong&gt;hyperbolic rotation 都是 equivariant group (with a normalization constant).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ex3: $\Phi$ = [2, -1; 1, 2]  $\Rightarrow T_g =[c, -s; s, c]$.  所有 &lt;strong&gt;rotation 都是 equivariant group (with a normalization constant).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ex4: &lt;strong&gt;Horizontal shear 也是一個 equivariant group.&lt;/strong&gt;&lt;br /&gt;
Proof: $[1, k_1; 0, 1] \times [1, k_2; 0, 1] = [1, k_1+k_2; 0, 1] \to$ multiplication closure and 反元素是 $[1, -k; 0, 1] \to$ inverse closure.&lt;/p&gt;

&lt;p&gt;Ex5: Uniform scaling 也是一個 (trivial) equivariant group.&lt;/p&gt;

&lt;p&gt;下圖摘自 [@wikiEigenvaluesEigenvectors2020].
&lt;img src=&quot;media/15790137525682/15875295928215.jpg&quot; alt=&quot;-w700&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;discrete-convolution-wikitoeplitzmatrix2020&quot;&gt;Discrete Convolution: [@wikiToeplitzMatrix2020]&lt;/h4&gt;
&lt;p&gt;Discrete convolution (離散卷積) 廣泛用於數位訊號處理和深度學習 for audio and video.  Discrete convolution 基本是 linear bijective operation, 同樣適用 equivariant group 的結論。我們用 1D discrete convolution 為例如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y=h * x=\left[\begin{array}{ccccc}
h_{1} &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
h_{2} &amp; h_{1} &amp; &amp; \vdots &amp; \vdots \\
h_{3} &amp; h_{2} &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; h_{3} &amp; \cdots &amp; h_{1} &amp; 0 \\
h_{m-1} &amp; \vdots &amp; \ddots &amp; h_{2} &amp; h_{1} \\
h_{m} &amp; h_{m-1} &amp; &amp; \vdots &amp; h_{2} \\
0 &amp; h_{m} &amp; \ddots &amp; h_{m-2} &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; h_{m-1} &amp; h_{m-2} \\
\vdots &amp; \vdots &amp; &amp; h_{m} &amp; h_{m-1} \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; h_{m}
\end{array}\right]\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{3} \\
\vdots \\
x_{n}
\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;$y = h * x = \Phi x$ where $\Phi$ is a $n\times n$ matrix, 就是把 m-tap kernel filter $[h_1, h_2, …, h_m]$ &lt;strong&gt;shift (平移)&lt;/strong&gt; n 次造出的 matrix, 稱為 Toeplitz matrix. 一般 n » m, 因此是 “band matrix” with high sparsity. 後面會看到 $\Phi$ 的 equivariant group $T_g$ 和這個操作直接相關。&lt;/p&gt;

&lt;p&gt;下一步是要找出 $\Phi$ 的 eigenvectors 以及構成的 commutative group. 可以參考 [@grayToeplitzCirculant1971], excel article about Toeplitz matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;有一個 “trick” 就是用 Circulant matrix 取代 Toeplitz matrix by using cyclic shift to replace regular shift!&lt;/strong&gt;  因為 n » m, 實務上Toeplitz 和 Circulant matrix 得到的 $y$ 差異很小。但 Circulant matrix 好求解而且具有物理意義。&lt;/p&gt;

&lt;p&gt;Follow [@grayToeplitzCirculant1971] 的 notation on p.31, 我們用 $C$ 代替 $\Phi$.&lt;/p&gt;

&lt;p&gt;A $n\times n$ circulant matrix $C$ has the form 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
C=\left[\begin{array}{cccccc}
c_{0} &amp; c_{1} &amp; c_{2} &amp; &amp; \cdots &amp; c_{n-1} \\
c_{n-1} &amp; c_{0} &amp; c_{1} &amp; c_{2} &amp; &amp; \vdots \\
&amp; &amp; c_{n-1} &amp; c_{0} &amp; c_{1} &amp; \ddots &amp; \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; &amp; c_{2} \\
&amp; &amp; &amp; &amp; &amp; c_{1} \\
c_{1} &amp; \cdots &amp; &amp; c_{n-1} &amp; &amp; c_{0}
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;circulant-matrix-eigenvalues-and-eigenvectors&quot;&gt;Circulant matrix eigenvalues and eigenvectors&lt;/h4&gt;
&lt;p&gt;The eigenvalues $\psi_m$ and the eigenvectors $y^{(m)}$ are the solution of
&lt;script type=&quot;math/tex&quot;&gt;C y = \psi y&lt;/script&gt;
我們引入一個 variable $\rho$, which is one of the n distinct complex root of unity ($\rho_m = e^{-2\pi i m/n}$, $m = 0, … n-1$), we have the eigenvalue and eigenvector
&lt;script type=&quot;math/tex&quot;&gt;\psi=\sum_{k=0}^{n-1} c_{k} \rho^{k}&lt;/script&gt; 
and 
&lt;script type=&quot;math/tex&quot;&gt;y=n^{-1 / 2}\left(1, \rho, \rho^{2}, \ldots, \rho^{n-1}\right)^{\prime}&lt;/script&gt;
帶入 $\rho_m$, we have eigenvalue $(m = 0, … n)$
&lt;script type=&quot;math/tex&quot;&gt;\psi_{m}=\sum_{k=0}^{n-1} c_{k} e^{-2 \pi i m k / n}&lt;/script&gt;
&lt;strong&gt;!!注意：$\psi_{m}$ is the DFT of $c_k$&lt;/strong&gt;, i.e. $\psi = DFT(c)$.  反之，$c = IDFT(\psi)$
&lt;script type=&quot;math/tex&quot;&gt;c_{m}= \frac{1}{n} \sum_{k=0}^{n-1} \psi_{k} e^{2 \pi i m k / n}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;$\psi_{m}$ 對應的 (column) eigenvector 
&lt;script type=&quot;math/tex&quot;&gt;y^{(m)}=\frac{1}{\sqrt{n}}\left(1, e^{-2 \pi i m / n}, \cdots, e^{-2 \pi i m(n-1) / n}\right)^{\prime}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;檢查幾個 eigenvalue. First, $m=0$ is the DC component of $c_k$
&lt;script type=&quot;math/tex&quot;&gt;\psi_{0}=\sum_{k=0}^{n-1} c_{k}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;對應的 (column) eigenvector
&lt;script type=&quot;math/tex&quot;&gt;y^{(0)}=\frac{1}{\sqrt{n}}\left(1, 1, \cdots, 1\right)^{\prime}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;帶入驗證  $ C y^{(0)} = \psi_{0} y^{(0)}  $.&lt;/p&gt;

&lt;p&gt;Next $m=1$ is the 1st fundamental component of $c_k$
&lt;script type=&quot;math/tex&quot;&gt;\psi_{1}=\sum_{k=0}^{n-1} c_{k} e^{-2 \pi i k / n}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;對應的 (column) eigenvector 
&lt;script type=&quot;math/tex&quot;&gt;y^{(1)}=\frac{1}{\sqrt{n}}\left(1, e^{-2 \pi i / n}, \cdots, e^{-2 \pi i (n-1) / n}\right)^{\prime}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Next $m=2$ is the 2nd fundamental component of $c_k$
&lt;script type=&quot;math/tex&quot;&gt;\psi_{2}=\sum_{k=0}^{n-1} c_{k} (e^{-2 \pi i k / n})^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;對應的 (column) eigenvector 
&lt;script type=&quot;math/tex&quot;&gt;y^{(2)}=\frac{1}{\sqrt{n}}\left(1, (e^{-2 \pi i / n})^2, \cdots, (e^{-2 \pi i (n-1) / n})^2\right)^{\prime}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;可以驗證  $ C y^{(m)} = \psi_{m} y^{(m)}  $.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;我們用 one equation to summarize the results. 其實就是 $C$ 的 eigenvalue decomposition 如下。$\Psi$ 是 diagonal matrix with eigenvalues, 剛好就是 $C$ matrix 第一列 (row 1) 的 DFT 結果。&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;CU = U \Psi \quad\quad C = U \Psi U^{-1} = U \Psi U^{*}&lt;/script&gt;  where
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
U &amp;=\left[y^{(0)}\left|y^{(1)}\right| \cdots | y^{(n-1)}\right] \\
&amp;=n^{-1 / 2}\left[e^{-2 \pi i m k / n} ; m, k=0,1, \ldots, n-1\right]
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
= n^{-1/2} \left[\begin{array}{cccccc}
1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 \\
1 &amp; \omega &amp; \omega^{2}  &amp;  \cdots &amp; \omega^{n-1} \\
1 &amp; \omega^2 &amp; (\omega^{2})^2  &amp;  \cdots &amp; (\omega^{n-1})^2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
1 &amp; \omega^{n-1} &amp; (\omega^{2})^{n-1} &amp; \cdots &amp; (\omega^{n-1})^{n-1}
\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
U^{-1} = U^{*} = n^{-1/2} \left[\begin{array}{cccccc}
1 &amp; 1 &amp; 1 &amp;  \cdots &amp; 1 \\
1 &amp; \bar{\omega} &amp; \bar{\omega}^{2}  &amp;  \cdots &amp; \bar{\omega}^{n-1} \\
1 &amp; \bar{\omega}^2 &amp; (\bar{\omega}^{2})^2  &amp;  \cdots &amp; (\bar{\omega}^{n-1})^2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
1 &amp; \bar{\omega}^{n-1} &amp; (\bar{\omega}^{2})^{n-1} &amp; \cdots &amp; (\bar{\omega}^{n-1})^{n-1}
\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;with $\omega = e^{-2 \pi i / n}$ and $\bar{\omega} = \omega^{*} = e^{+2 \pi i / n}$&lt;/p&gt;

&lt;p&gt;Complex conjugate frequency sequence
另一種的順序是 complex conjugate (Nyquist) frequency sequence, 就是 [DC, +f, -f, +2f, -2f, …, AC]  如果 n 是偶數，AC = [1, -1, 1, -1…].  如果 n 是奇數，….&lt;/p&gt;

&lt;h4 id=&quot;equivariant-phi-is-circulant-matrix-for-discrete-convolution&quot;&gt;Equivariant: $\Phi$ is Circulant Matrix for Discrete Convolution&lt;/h4&gt;
&lt;p&gt;Given $\Phi = C$, a circulant matrix, 現在需要找到 equivariant group $T$ to make $\Phi(T x) = \Phi T x = T \Phi x = T \Phi(x)$.  答案是  $T_g= U Q U^{&lt;em&gt;}$ where $U$ and $U^{&lt;/em&gt;}$ 就是以上的 matrices (n 點分圓函數) and $Q$ is a diagonal matrix.&lt;/p&gt;

&lt;p&gt;注意 $U$ and $U^{&lt;em&gt;}$ 是 complex matrix, Q in general 也是 complex matrix.  但實際應用會限制 $T_g = U Q U^{&lt;/em&gt;}$ 必須是 real matrix.  因此會要求 Q matrix 滿足一些特性。因為 Q matrix 其實是另一個 circulant matrix 的 row 1 FFT 結果。&lt;/p&gt;

&lt;p&gt;In summary, circulant matrix 本身 forms a commutative group, i.e. $A B = B A = C$ (multiplication closure and commuting) is circulant matrix, $A^{-1}$ 也是 circulant matrix, 甚至 $A + B$ 也是 circulant matrix [@wikiCirculantMatrix2020].&lt;/p&gt;

&lt;p&gt;整理一下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$y = \Phi(x) = h * x$ performs discrete convolution (i.e. 1D CNN) where $x$ and $y$ are input and output signals of n-dimension.  $h$ is the kernel filter of m dimension. 一般 n » m.  可以用 $n\times n$ Circulant matrix multiplication 近似 discrete convolution by zero padding, i.e. $y = C x$.  $C$ 是把 $h$ 放在 $C$ 的 row1, 再 cyclic right shift by 1 放在 row 2, and so on.  &lt;strong&gt;In summary, discrete convolution is equivalent to Circulant matrix multiplication.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;$n \times n$ Circulant matrices form a commutative group, $T_g$, i.e. $\Phi(T_g x) = T_g \Phi(x)$ as long as $T_g$ is a $n\times n$ Circulant matrix.  Actually, $\Phi \in T_g$.  $T_g$ is equivariant operation.&lt;/li&gt;
  &lt;li&gt;Circulant group 的 generating element is $g$ = [0, 1, 0…, 0; 0, 0, 1, …,0,; ….; 1, 0, 0, …, 0]‘ 代表 right cyclic shift by 1; $g^2 = g&lt;em&gt;g, g^k = g&lt;/em&gt;g&lt;em&gt;..&lt;/em&gt;g$. Therefore, $I, g^2, g^3, ..g^{n-1}$ 構成 basis for 所有 $n\times n$ Circulant matrix.  For any Circulant matrix by $[a_0, a_1, …, a_{n-1}] = a_0 I + a_1 g^2 + …, + a_{n-1} g^{n-1}$.  也就是說，Circulant matrix can be decomposed to translation matrix superposition.&lt;/li&gt;
  &lt;li&gt;Discrete convolution is therefore translation multiplication commutable =&amp;gt; translation equivariant, i.e. $\Phi ( T_g x) = T_g (\Phi x)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A discrete convolution example in appendix A.&lt;/p&gt;

&lt;h4 id=&quot;equivariant-phi-is-circulant-matrix-for-2d-discrete-convolution&quot;&gt;Equivariant: $\Phi$ is Circulant Matrix for 2D Discrete Convolution&lt;/h4&gt;
&lt;p&gt;$y(t) = h(t) * x(t)$ 可以直接推廣到 2D,  $y(u, v) = h(u, v) * x(u, v)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;因為 $u$ and $v$ are independent on the Cartesian coordinate.  注意這並不代表 $y, h, x$ are $u, v$ separable.&lt;/li&gt;
  &lt;li&gt;Circulant group 是兩個 Circulant group 的 &lt;strong&gt;direct sum&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Generator 是 $g_u$ and $g_v$.&lt;/li&gt;
  &lt;li&gt;The DFT core is exp(-2piinu/.) exp(-2pimv/.)&lt;/li&gt;
  &lt;li&gt;How about eigenvalue and eigenvectors?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;how-about-other-equivariant-for-1d-signal-processing&quot;&gt;How about other equivariant for 1D signal processing?&lt;/h4&gt;
&lt;p&gt;Mirror, scale equivariant?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\Phi(x)$ 
condition of Q?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;use-cohens-paper-notation-and-concept&quot;&gt;Use Cohen’s paper notation and concept&lt;/h2&gt;
&lt;p&gt;以上的推導太狹隘，接下來採用 Cohen’s paper notation and ideas.&lt;/p&gt;

&lt;h3 id=&quot;the-group-p4m-non-commutative-group&quot;&gt;The group $p4m$ (non-commutative group)&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g(m, r, u, v)=\left[\begin{array}{ccc}
(-1)^{m} \cos \left(\frac{r \pi}{2}\right) &amp; -(-1)^{m} \sin \left(\frac{r \pi}{2}\right) &amp; u \\
\sin \left(\frac{r \pi}{2}\right) &amp; \cos \left(\frac{r \pi}{2}\right) &amp; v \\
0 &amp; 0 &amp; 1
\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;以上是 2D Cartesian coordinate (+1D depth) generates to 4D symmetry G space (+1D depth).  分為兩種 case: (1) input 仍然是 3D tensor, but output is converted to 5D tensor.  僅用於神經網絡的第一層。之後就轉換成 (2) both input/output 都是 5D tensors.  原文有簡化版 p4 (no mirror reflection) and 2D translation only.&lt;/p&gt;

&lt;p&gt;此處考慮更簡單的 case, 1D translation and 1D translation + mirror reflection.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g(m, u)=\left[\begin{array}{ccc}
(-1)^{m} &amp; u \\
0 &amp; 1
\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g^{-1}(m, u)=\left[\begin{array}{ccc}
(-1)^{m} &amp; (-u)(-1)^m \\
0 &amp; 1
\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Next step:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Function f(x)&lt;/li&gt;
  &lt;li&gt;Group operation on f(x) is 
 $L_g f(x) = f(g^{-1}x)$.  原因很簡單，就是在 $x‘=gx$ 會得到原來的函數。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;考慮 CNN convolution 函數，定義如下。$x, y \in Z^2$
&lt;script type=&quot;math/tex&quot;&gt;\begin{array}{l}
{\left[f * \psi^{i}\right](x)=\sum_{y \in \mathbb{Z}^{2}} \sum_{k=1}^{K^{l}} f_{k}(y) \psi_{k}^{i}(x-y)} \\
{\left[f \star \psi^{i}\right](x)=\sum_{y \in \mathbb{Z}^{2}} \sum_{k=1}^{K^{l}} f_{k}(y) \psi_{k}^{i}(y-x)}
\end{array}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;推廣到 G-CNN convolution.  $g, h \in G$
&lt;script type=&quot;math/tex&quot;&gt;\begin{array}{l}
{\left[f * \psi^{i}\right](g)=\sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1}g)} \\
{\left[f \star \psi^{i}\right](g)=\sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(g^{-1}h)}
\end{array}&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上式是 forward pass 的 convolution ($\ast$).  下式是 backward pass 的 correlation ($\star$).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Combine 2 and 3, $L_u &lt;a href=&quot;g&quot;&gt;f \star \psi&lt;/a&gt; = &lt;a href=&quot;u^{-1}g&quot;&gt;f \star \psi&lt;/a&gt; \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}((u^{-1}g)^{-1}h) \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(g^{-1}uh)   \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(u^{-1}h) \psi_{k}^{i}(g^{-1}h) \  = &lt;a href=&quot;g&quot;&gt;[L_u f] \star \psi&lt;/a&gt;$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Combine 2 and 3, $L_u &lt;a href=&quot;g&quot;&gt;f * \psi&lt;/a&gt; = &lt;a href=&quot;u^{-1}g&quot;&gt;f * \psi&lt;/a&gt; \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1} (u^{-1}g)) \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1}u^{-1}g)   \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(u^{-1}h) \psi_{k}^{i}(h^{-1}g) \  = &lt;a href=&quot;g&quot;&gt;[L_u f] * \psi&lt;/a&gt;$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我們用一個 1D convolution 來驗證。 
Example: g = [(-1)^m, u; 0, 1]   g^-1 = [(-1)^m, -u*(-1)^m; 0 , 1]
&lt;script type=&quot;math/tex&quot;&gt;[f \star \psi^{i}](g) = [f \star \psi^{i}](x, m) = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(g^{-1}h) \\
= \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(y) \psi_{k}^{i}((-1)^{m}(y-x))\\ \ne \sum_{y \in \mathbb{Z}^{2}} \sum_{k=1}^{K^{l}} f_{k}(y) (\psi_{k}^{i}(x-y) +  \psi_{k}^{i}(y-x) )&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Does it make sense?   If $\psi$ is an odd function, $f \star \psi^{i} =0$?
No, g = (x, m) =&amp;gt; m should be kept instead of disappear after summation!!&lt;/p&gt;

&lt;p&gt;Let’s look at another example, polar transform. [@estevesPOLARTRANSFORMER2018]&lt;/p&gt;

&lt;h3 id=&quot;polar-coordinate&quot;&gt;Polar Coordinate&lt;/h3&gt;

&lt;p&gt;A similarity transformation, i.e. conformal mapping, 旋轉(R)+scaling(s)+平移(t), $\rho \in $ SIM(2), acts on a point in $x \in R^2$ by
&lt;script type=&quot;math/tex&quot;&gt;\rho x \to s Rx + t \quad s \in R^+, R \in SO(2), t \in R^2&lt;/script&gt;
where &lt;em&gt;SO(2)&lt;/em&gt; is the rotation group.&lt;/p&gt;

&lt;p&gt;Equivariance to SIM(2) is achieved by (1) learning the center of the dilated rotation, (2) shifting the original image accordingly then (3) transforming the image to canonical coordinates.&lt;/p&gt;

&lt;p&gt;Q1: How to find the center of rotation? Need an origin predictor.&lt;/p&gt;

&lt;p&gt;Transformation of the image $L_t I = I(t-t_o)$ reduces the SIM(2) deformation to a dilated-rotation if $t_o$ is the true translation. After centering, we perform $SO(2) \times R^+$ convolutions on the new image $I_o = I(x-t_o)$.&lt;/p&gt;

&lt;p&gt;Layer 1 convolution 變成：
&lt;script type=&quot;math/tex&quot;&gt;f(r)=\int_{x \in \mathbb{R}^{2}} I_{o}(x) \phi\left(r^{-1} x\right) d x&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{s} f(s) \phi\left(s^{-1} r\right) d s=\int_{s} \lambda(\xi, \theta) \phi\left(\xi_{r}-\xi, \theta_{r}-\theta\right) d \xi d \theta&lt;/script&gt;

&lt;p&gt;In summary,
本文 (polar transformation) 比較像是 coordinate transformation instead of adding group dimension.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No. 從原始的 $t \in R^2$, 多了 rotation and scale dimension $SO(2) \times R^+$.&lt;/li&gt;
  &lt;li&gt;But yes, 就 convolution 而言，feature extraction 已經不是 (x,y) convolution, 而是 $\epsilon, \theta$&lt;/li&gt;
  &lt;li&gt;location information 仍然存在，但用 origin predictor 取代 (x,y) convolution learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;media/15884328451601/15890439753317.jpg&quot; alt=&quot;-w907&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;equivariant-phi-is-cnn-and-bijective-reversible-stride1-ignore-boundary&quot;&gt;Equivariant: $\Phi$ is CNN and Bijective (reversible, stride=1, ignore boundary)&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;(Translation) Equivariant:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;= T’y = T’f(x)&lt;/strong&gt; where T’ is another coordinate which could be different from T because of scaling, etc.   But both T and T’ are linear operators. This orange part is the crucial step assuming translation equivariant!!   However, T is translation equivariant, but NOT rotational equivariant. 
y’ = T’y = T’ f(x) = T’ f(T^-1 x’)  assuming linear inversible operation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Use [@cohenGroupEquivariant2019] notation $f \to \Phi$ and $T \to T_g$ 
Original output feature is $\Phi(x)$, where $\Phi$ can be a nonlinear (complicated) mapping, such as convolution + pooling + ReLU.&lt;/p&gt;

&lt;p&gt;Given input image x is transformed by $T_g$ operator/transform, new output feature is $\Phi(T_g x)$.
如果具有 translation equivariant =&amp;gt; $\Phi(T_g x) = T’_g \Phi(x)$ where T’_g 是同樣的 translation operator/transform, but may have different scaling factor (stride &amp;gt; 1).&lt;/p&gt;

&lt;p&gt;所以 $T_g$ and $T’_g$ 需要有什麼特性？只需要 linear, i.e. $T(gh) = T(g)T(h)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如果 $T’_g = I$ for all g&lt;/strong&gt;, 是 special case, 稱為 invariant.  這和一般物理定義的 invariant 似乎不同?  對於&lt;strong&gt;深度學習 invariant 會失去 spatial information, $T’_g$ 而變得無用&lt;/strong&gt;, equivariance 是更有用。&lt;/p&gt;

&lt;p&gt;另一個極端是沒有 equivariant, 也就是 $\Phi(T_g x)$ 和 $\Phi(x)$ 沒有簡單的 linear mapping, 例如 Multi-layer Perceptron (MLP).&lt;/p&gt;

&lt;p&gt;Paper 另外一段話如下，似乎和 invariant 相抵觸? No, 是擴充到 non-injective (降維) network.
A network $\Phi$ &lt;strong&gt;can be&lt;/strong&gt; non-injective, meaning that non-identical vectors $x$ and $y$ in the input space become identical in the output space.  (for example, two instances of a face may be mapped onto a single vector indicating the presence of any face, e.g. 人臉偵測而非識別，兩個不同的人臉對應到相同的 feature map or bounding box).  If $\Phi$ is equivariant, then the G-transformed inputs $T_g x$ and $T_g y$ must also mapped to the same output.  Their “sameness” is preserved under symmetry transformations.&lt;/p&gt;

&lt;p&gt;數學表示：
Non-injective network: $\Phi(x) = \Phi(y)$ with $x \ne y$ 
If $\Phi$ is equivariant, then the G-transform (symmetry transform) has:
$\Phi(T_g x)  = T’_g \Phi(x) = T’_g \Phi(y) = \Phi(T_g y)$ with $x \ne y$&lt;/p&gt;

&lt;p&gt;g represents general group, in the paper considering three groups: Z2, p4, p4m.  Conclusion.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;On MNIST and CIFAR, G-CNN performs better than CNN at about same parameter number.&lt;/li&gt;
  &lt;li&gt;G-CNN also benefit from data augment.&lt;/li&gt;
  &lt;li&gt;Step 1: G-CNN to include translation, rotation, mirror on grid&lt;/li&gt;
  &lt;li&gt;Step 2: G-CNN on hexagon grid&lt;/li&gt;
  &lt;li&gt;Step 3: On 3D sphere and use G-FFT to compute sphere convolution for 3D application.&lt;/li&gt;
  &lt;li&gt;Step 4: Gauge CNN?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CNN, pooling, ReLU are translation equivariant (up to edge effect); but MLP is &lt;em&gt;NOT&lt;/em&gt; translation equivariant.&lt;/p&gt;

&lt;p&gt;Translation Equivariant:  There is a function (e.g. CNN)&lt;/p&gt;

&lt;p&gt;1D =&amp;gt; 2D convolution =&amp;gt; high dimension tensor convolution&lt;/p&gt;

&lt;p&gt;Step 1: Define the network operator $\Phi$
 Step 2: Find the commuting operator $T$, actually, a commutative group $T_g$.  $\Phi$ 可以視為 $T_g$ 的一個 element.
 Step 3: Find the group generator for the commutative group.&lt;/p&gt;

&lt;p&gt;What is the fundamental element of a group? =&amp;gt; generator &amp;lt;g, ..&amp;gt;!
所有的 group element 都可以從 generator &amp;lt;g, ..&amp;gt; 產生。
All Abelian group is isomorphic to direct sum of primed cycle group =&amp;gt; generator g, gg, ggg, …&lt;/p&gt;

&lt;h3 id=&quot;group-examples&quot;&gt;Group Examples&lt;/h3&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;Bronstein, Michael M., Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre
Vandergheynst. 2017. “Geometric Deep Learning: Going Beyond Euclidean
Data.” &lt;em&gt;IEEE Signal Processing Magazine&lt;/em&gt; 34 (4): 18–42.
&lt;a href=&quot;https://doi.org/10.1109/MSP.2017.2693418&quot;&gt;https://doi.org/10.1109/MSP.2017.2693418&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cohen, Taco S, T S Cohen, and Uva Nl. 2019. “Group Equivariant Convolutional Networks,” 10.&lt;/p&gt;

&lt;p&gt;Cohen, Taco S., Maurice Weiler, Berkay Kicanaoglu, and Max Welling.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;“Gauge Equivariant Convolutional Networks and the Icosahedral
CNN,” May. &lt;a href=&quot;http://arxiv.org/abs/1902.04615&quot;&gt;http://arxiv.org/abs/1902.04615&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pavlus, John. 2020. “An Idea from Physics Helps AI See in Higher
Dimensions.” Quanta Magazine. January 9, 2020.
&lt;a href=&quot;https://www.quantamagazine.org/an-idea-from-physics-helps-ai-see-in-higher-dimensions-20200109/&quot;&gt;https://www.quantamagazine.org/an-idea-from-physics-helps-ai-see-in-higher-dimensions-20200109/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;prism. 2019. 「Group Equivariant CNN to Spherical CNNs: 從群等變卷積網絡到球面卷積網絡.」 知乎專欄. 2019.
&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34042888&quot;&gt;https://zhuanlan.zhihu.com/p/34042888&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Winkels, Marysia, and Taco S. Cohen. 2018. 「3D G-CNNs for Pulmonary
Nodule Detection,」 April. &lt;a href=&quot;http://arxiv.org/abs/1804.04656&quot;&gt;http://arxiv.org/abs/1804.04656&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;XinZhiYuan. 2020. 「Geometrical Deep Learning 受愛因斯坦啟示：讓AI擺脫平面看到更高的維度.」 2020. &lt;a href=&quot;https://kknews.cc/tech/gpkgx3e.html&quot;&gt;https://kknews.cc/tech/gpkgx3e.html&lt;/a&gt;.&lt;/p&gt;</content><author><name>Allen Lu (from John Doe)</name></author><category term="python" /><category term="quantization" /><category term="model compression" /><category term="pruning" /><category term="distillation" /><summary type="html">Math AI - G-CNN (Group + CNN) Where is group theory (G-CNN) + Curved Space (Spherical CNN)</summary></entry></feed>