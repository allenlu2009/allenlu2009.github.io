
<!doctype html>














<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Jekyll, NexT" />





  <link rel="alternate" href="/atom.xml" title="NexT" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?v=5.1.1" />
















<meta name="description" content="```python import sys print(sys.version) import tensorflow print(tensorflow.__version__) import keras print(keras.__version__) import pandas as pd import numpy as np import math import random import matplotlib.pyplot as plt %matplotlib inline ``` 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info. 1.15.0 2.2.5 Using TensorFlow backend. #Generate a sin wave with no noise First, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves! Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles. ```python def noisy_sin(steps_per_cycle = 50, number_of_cycles = 500, random_factor = 0.4): &apos;&apos;&apos; number_of_cycles : The number of steps required for one cycle Return : pd.DataFrame() with column sin_t containing the generated sin wave &apos;&apos;&apos; random.seed(0) df = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[&quot;t&quot;]) df[&quot;sin_t&quot;] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)+ random.uniform(-1.0, +1.0) * random_factor)) df[&quot;sin_t_clean&quot;] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle))) print(&quot;create period-{} sin wave with {} cycles&quot;.format(steps_per_cycle,number_of_cycles)) print(&quot;In total, the sin wave time series length is {}&quot;.format(steps_per_cycle*number_of_cycles+1)) return(df) steps_per_cycle = 10 df = noisy_sin(steps_per_cycle=steps_per_cycle, random_factor = 0) n_plot = 8 df[[&quot;sin_t&quot;]].head(steps_per_cycle * n_plot).plot( title=&quot;Generated first {} cycles&quot;.format(n_plot), figsize=(15,3)) ``` create period-10 sin wave with 500 cycles In total, the sin wave time series length is 5001 ![png](/old/rnn_sin/output_3_2.png) Create a training and testing data. Here, the controversial &quot;length of time series&quot; parameter comes into play. For now, we set this parameter to 2. ```python def _load_data(data, n_prev = 100): &quot;&quot;&quot; data should be pd.DataFrame() &quot;&quot;&quot; docX, docY = [], [] for i in range(len(data)-n_prev): docX.append(data.iloc[i:i+n_prev].as_matrix()) docY.append(data.iloc[i+n_prev].as_matrix()) alsX = np.array(docX) alsY = np.array(docY) return alsX, alsY length_of_sequences = 2 test_size = 0.25 ntr = int(len(df) * (1 - test_size)) df_train = df[[&quot;sin_t&quot;]].iloc[:ntr] df_test = df[[&quot;sin_t&quot;]].iloc[ntr:] (X_train, y_train) = _load_data(df_train, n_prev = length_of_sequences) (X_test, y_test) = _load_data(df_test, n_prev = length_of_sequences) print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) ``` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. if __name__ == &apos;__main__&apos;: (3748, 2, 1) (3748, 1) (1249, 2, 1) (1249, 1) #Simple RNN model As a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit. * The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias * The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for bias In total, there are only 5 weights in this model. Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as: $$ \begin{aligned} h_{t}=\tanh \left(x_{t}^{T} w_{1 x}+h_{t-1}^{T} w_{1 h}+b_{1}\right) \\ x_{t+1}=h_{t}^{T} w_{2}+b_{2} \end{aligned} $$ Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to &quot;remember&quot; the past history of $x_t$ through the hidden unit $h_t$. ##batch_shape needs for BPTT.¶ * Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data. * This means that the each batch is treated as independent. * This batch_shape determines the size of this subset. * Every batch starts will the initial hidden unit $h_0=0$. * As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value. * The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!! Now, we define this model using Keras and show the model summary. ```python from keras.layers import Input from keras.models import Model from keras.layers.core import Dense, Activation from keras.layers.recurrent import SimpleRNN def define_model(length_of_sequences, batch_size = None, stateful = False): in_out_neurons = 1 hidden_neurons = 1 inp = Input(batch_shape=(batch_size, length_of_sequences, in_out_neurons)) rnn = SimpleRNN(hidden_neurons, return_sequences=False, stateful = stateful, name=&quot;RNN&quot;)(inp) dens = Dense(in_out_neurons,name=&quot;dense&quot;)(rnn) model = Model(inputs=[inp],outputs=[dens]) model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;) return(model,(inp,rnn,dens)) ## use the default values for batch_size, stateful model, (inp,rnn,dens) = define_model(length_of_sequences = X_train.shape[1]) model.summary() ``` WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. Model: &quot;model_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 2, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (None, 1) 3 _________________________________________________________________ dense (Dense) (None, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Now we train the model. The script was run without GPU. ```python hist = model.fit(X_train, y_train, batch_size=600, epochs=1000, verbose=False,validation_split=0.05) ``` WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. #Plot of val_loss and loss. The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001. ```python for label in [&quot;loss&quot;,&quot;val_loss&quot;]: plt.plot(hist.history[label],label=label) plt.ylabel(&quot;loss&quot;) plt.xlabel(&quot;epoch&quot;) plt.title(&quot;The final validation loss: {}&quot;.format(hist.history[&quot;val_loss&quot;][-1])) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_11_0.png) #The plot of true and predicted sin waves look nearly identical ```python y_pred = model.predict(X_test) plt.figure(figsize=(19,3)) plt.plot(y_test,label=&quot;true&quot;) plt.plot(y_pred,label=&quot;predicted&quot;) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_13_0.png) #What are the model weights? The best way to understand the RNN model is to create a model from scratch. Let&apos;s extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers. ```python ws = {} for layer in model.layers: ws[layer.name] = layer.get_weights() ws ``` {&apos;RNN&apos;: [array([[-0.43695387]], dtype=float32), array([[-0.64668506]], dtype=float32), array([0.00117508], dtype=float32)], &apos;dense&apos;: [array([[-3.7658346]], dtype=float32), array([-0.00123706], dtype=float32)], &apos;input_1&apos;: []} #What are the predicted values of hidden units? Since we used Keras&apos;s functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest. In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model &quot;rnn_model&quot; shares the weights obtained by the previous model&apos;s optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations. ```python rnn_model = Model(inputs=[inp],outputs=[rnn]) rnn_model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;) hidden_units = rnn_model.predict(X_test).flatten() ``` Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave. ```python upto = 100 predicted_sin_wave = ws[&quot;dense&quot;][0][0][0]*hidden_units + ws[&quot;dense&quot;][1][0] plt.figure(figsize=(19,3)) plt.plot(y_test[:upto],label=&quot;y_pred&quot;) plt.plot(hidden_units[:upto],label=&quot;hidden units&quot;) plt.plot(predicted_sin_wave[:upto],&quot;*&quot;, label=&quot;w2 * hidden units + b2&quot;) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_19_0.png) #Obtain predicted sin wave at the next time point given the current sin wave by hand We understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as: ```python def RNNmodel(ws,x,h=0): &apos;&apos;&apos; ws: predicted weights x : scalar current sign value h : scalar RNN hidden unit &apos;&apos;&apos; h = np.tanh(x*ws[&quot;RNN&quot;][0][0][0] + h*ws[&quot;RNN&quot;][1][0][0] + ws[&quot;RNN&quot;][2][0]) x = h*ws[&quot;dense&quot;][0][0][0] + ws[&quot;dense&quot;][1][0] return(x,h) ``` Naturally, you can obtain the predicted sin waves $(x_1,x_2,...,x_t)$ by looping around RNNmodel as: $x^∗_{t+1},h_{t+1} = $ RNNmodel$(x_t,h_t)$ Here $x^∗_t$ indicates the estimated value of $x$ at time point $t$. As our model is not so complicated, we can readily implement this algorithm as: ```python upto = 50 ## predict the first sin values xstars, hs_hand = [], [] for i, x in enumerate(df_test.values): if i == 0: h = 0 ## initial hidden layer value is zero xstar = x print(&quot;initial value of sin x_0 = {}, h_0 = {}&quot;.format(x,h)) hs_hand.append(h) xstars.append(xstar[0]) xstar, h = RNNmodel(ws,x, h) assert len(df_test.values) == len(xstars) ``` initial value of sin x_0 = [-1.27375647e-13], h_0 = 0 In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_test ```python plt.figure(figsize=(18,3)) plt.plot(df_test.values[:upto],label=&quot;true&quot;,alpha=0.3,linewidth=5) plt.plot(xstars[:upto],label=&quot;sin prediction (xstar)&quot;) plt.plot(hs_hand[:upto],label=&quot;hidden state (xstar)&quot;) plt.legend() ``` ![png](/old/rnn_sin/output_25_1.png) You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on? ```python &quot;validation loss {:3.2f}&quot;.format(np.mean((np.array(xstars) - df_test[&quot;sin_t&quot;].values)**2)) ``` &apos;validation loss 0.08&apos; Let&apos;s predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as: x1,x2x2,x3x3,x4... ```python y_test_from_keras = model.predict(X_test).flatten() ``` Notice that this predicted values are exactly the same as the ones calculated before. ```python np.all(predicted_sin_wave == y_test_from_keras) ``` True As the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars. ```python y_test_from_keras = [np.NaN, np.NaN] + list(y_test_from_keras.flatten()) h_test_from_keras = [np.NaN, np.NaN] + list(hidden_units.flatten()) ``` The plot shows that Keras&apos;s predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras&apos;s prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output? ```python plt.figure(figsize=(18,3)) plt.plot(df_test.values[:upto],label=&quot;true&quot;,alpha=0.3,linewidth=5) plt.plot(xstars[:upto],label=&quot;sin prediction (xstar)&quot;) plt.plot(hs_hand[:upto],label=&quot;hidden state (xstar)&quot;) plt.plot(y_test_from_keras[:upto],label=&quot;sin prediction (keras)&quot;) plt.plot(h_test_from_keras[:upto],label=&quot;hidden state (keras)&quot;) plt.legend() print(&quot;validation loss {:6.5f}&quot;.format(np.nanmean((np.array(y_test_from_keras) - df_test[&quot;sin_t&quot;].values)**2))) ``` validation loss 0.00021 ![png](/old/rnn_sin/output_35_1.png) Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role. As the BPTT algorithm only passed back 2 steps, the model assumes that: the hidden units are initialized to zero every 2 steps. the prediction of the next sin value (xt+1) is based on the hidden unit (ht) which is created by updating the hidden units twice in the past assuming that ht−1=0. x∗t,ht=RNNmodel(xt−1,0)xt+1,−=RNNmodel(xt,ht) Note that the intermediate predicted sin x∗t based on ht−1=0 should not be used as the predicted sin value. This is because the x∗t was not directly used to evaluate the loss function. Finally, obtain the Keras&apos;s predicted sin wave at the next time point given the current sin wave by hand. ```python def myRNNpredict(ws,X): X = X.flatten() h = 0 for i in range(len(X)): x,h = RNNmodel(ws,X[i],h) return(x,h) xs, hs = [], [] for i in range(X_test.shape[0]): x, h = myRNNpredict(ws,X_test[i,:,:]) xs.append(x) hs.append(h) ``` ```python print(&quot;All sin estimates agree with ones from Keras = {}&quot;.format( np.all(np.abs( np.array(xs) - np.array(y_test_from_keras[2:]) ) &amp;lt; 1E-5))) print(&quot;All hidden state estmiates agree with ones fome Keras = {}&quot;.format( np.all(np.abs( np.array(hs) - np.array(h_test_from_keras[2:]) ) &amp;lt; 1E-5)) ) ``` All sin estimates agree with ones from Keras = True All hidden state estmiates agree with ones fome Keras = True Now we understand how Keras is predicting the sin wave. In fact, Keras has a way to return xstar as predicted values, using &quot;stateful&quot; flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself. In order to predict in &quot;stateful&quot; mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1. ```python model_stateful,_ = define_model(length_of_sequences = 1, batch_size=1, stateful = True) model_stateful.summary() ``` Model: &quot;model_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (1, 1, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (1, 1) 3 _________________________________________________________________ dense (Dense) (1, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Assign the trained weights into the stateful model. ```python for layer in model.layers: for layer_predict in model_stateful.layers: if (layer_predict.name == layer.name): layer_predict.set_weights(layer.get_weights()) break ``` Now we predict in stateful mode. Here it is very important to reset_state() before the prediction so that h0=0. ```python pred = df_test.values[0][0] stateful_sin = [] model_stateful.reset_states() for i in range(df_test.shape[0]): stateful_sin.append(pred) pred = model_stateful.predict(df_test.values[i].reshape(1,1,1))[0][0] stateful_sin = np.array(stateful_sin) ``` ```python print(&quot;All predicted sin values with stateful model agree to xstars = {}&quot;.format( np.all(np.abs(np.array(stateful_sin) - np.array(xstars))&amp;lt; 1E-5))) ``` All predicted sin values with stateful model agree to xstars = True Now we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example. However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say K? Do we always have to go back all the K time steps, set ht−K=0 and then feed forward K steps in order to predict at the time point t? This may be computationally intense.">
<meta name="keywords" content="Jekyll, NexT">
<meta property="og:type" content="website">
<meta property="og:title" content="NexT">
<meta property="og:url" content="http://localhost:4000/old/rnn_sin/rnn_sin/">
<meta property="og:site_name" content="NexT">
<meta property="og:description" content="```python import sys print(sys.version) import tensorflow print(tensorflow.__version__) import keras print(keras.__version__) import pandas as pd import numpy as np import math import random import matplotlib.pyplot as plt %matplotlib inline ``` 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info. 1.15.0 2.2.5 Using TensorFlow backend. #Generate a sin wave with no noise First, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves! Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles. ```python def noisy_sin(steps_per_cycle = 50, number_of_cycles = 500, random_factor = 0.4): &apos;&apos;&apos; number_of_cycles : The number of steps required for one cycle Return : pd.DataFrame() with column sin_t containing the generated sin wave &apos;&apos;&apos; random.seed(0) df = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[&quot;t&quot;]) df[&quot;sin_t&quot;] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)+ random.uniform(-1.0, +1.0) * random_factor)) df[&quot;sin_t_clean&quot;] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle))) print(&quot;create period-{} sin wave with {} cycles&quot;.format(steps_per_cycle,number_of_cycles)) print(&quot;In total, the sin wave time series length is {}&quot;.format(steps_per_cycle*number_of_cycles+1)) return(df) steps_per_cycle = 10 df = noisy_sin(steps_per_cycle=steps_per_cycle, random_factor = 0) n_plot = 8 df[[&quot;sin_t&quot;]].head(steps_per_cycle * n_plot).plot( title=&quot;Generated first {} cycles&quot;.format(n_plot), figsize=(15,3)) ``` create period-10 sin wave with 500 cycles In total, the sin wave time series length is 5001 ![png](/old/rnn_sin/output_3_2.png) Create a training and testing data. Here, the controversial &quot;length of time series&quot; parameter comes into play. For now, we set this parameter to 2. ```python def _load_data(data, n_prev = 100): &quot;&quot;&quot; data should be pd.DataFrame() &quot;&quot;&quot; docX, docY = [], [] for i in range(len(data)-n_prev): docX.append(data.iloc[i:i+n_prev].as_matrix()) docY.append(data.iloc[i+n_prev].as_matrix()) alsX = np.array(docX) alsY = np.array(docY) return alsX, alsY length_of_sequences = 2 test_size = 0.25 ntr = int(len(df) * (1 - test_size)) df_train = df[[&quot;sin_t&quot;]].iloc[:ntr] df_test = df[[&quot;sin_t&quot;]].iloc[ntr:] (X_train, y_train) = _load_data(df_train, n_prev = length_of_sequences) (X_test, y_test) = _load_data(df_test, n_prev = length_of_sequences) print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) ``` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. if __name__ == &apos;__main__&apos;: (3748, 2, 1) (3748, 1) (1249, 2, 1) (1249, 1) #Simple RNN model As a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit. * The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias * The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for bias In total, there are only 5 weights in this model. Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as: $$ \begin{aligned} h_{t}=\tanh \left(x_{t}^{T} w_{1 x}+h_{t-1}^{T} w_{1 h}+b_{1}\right) \\ x_{t+1}=h_{t}^{T} w_{2}+b_{2} \end{aligned} $$ Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to &quot;remember&quot; the past history of $x_t$ through the hidden unit $h_t$. ##batch_shape needs for BPTT.¶ * Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data. * This means that the each batch is treated as independent. * This batch_shape determines the size of this subset. * Every batch starts will the initial hidden unit $h_0=0$. * As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value. * The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!! Now, we define this model using Keras and show the model summary. ```python from keras.layers import Input from keras.models import Model from keras.layers.core import Dense, Activation from keras.layers.recurrent import SimpleRNN def define_model(length_of_sequences, batch_size = None, stateful = False): in_out_neurons = 1 hidden_neurons = 1 inp = Input(batch_shape=(batch_size, length_of_sequences, in_out_neurons)) rnn = SimpleRNN(hidden_neurons, return_sequences=False, stateful = stateful, name=&quot;RNN&quot;)(inp) dens = Dense(in_out_neurons,name=&quot;dense&quot;)(rnn) model = Model(inputs=[inp],outputs=[dens]) model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;) return(model,(inp,rnn,dens)) ## use the default values for batch_size, stateful model, (inp,rnn,dens) = define_model(length_of_sequences = X_train.shape[1]) model.summary() ``` WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. Model: &quot;model_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 2, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (None, 1) 3 _________________________________________________________________ dense (Dense) (None, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Now we train the model. The script was run without GPU. ```python hist = model.fit(X_train, y_train, batch_size=600, epochs=1000, verbose=False,validation_split=0.05) ``` WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. #Plot of val_loss and loss. The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001. ```python for label in [&quot;loss&quot;,&quot;val_loss&quot;]: plt.plot(hist.history[label],label=label) plt.ylabel(&quot;loss&quot;) plt.xlabel(&quot;epoch&quot;) plt.title(&quot;The final validation loss: {}&quot;.format(hist.history[&quot;val_loss&quot;][-1])) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_11_0.png) #The plot of true and predicted sin waves look nearly identical ```python y_pred = model.predict(X_test) plt.figure(figsize=(19,3)) plt.plot(y_test,label=&quot;true&quot;) plt.plot(y_pred,label=&quot;predicted&quot;) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_13_0.png) #What are the model weights? The best way to understand the RNN model is to create a model from scratch. Let&apos;s extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers. ```python ws = {} for layer in model.layers: ws[layer.name] = layer.get_weights() ws ``` {&apos;RNN&apos;: [array([[-0.43695387]], dtype=float32), array([[-0.64668506]], dtype=float32), array([0.00117508], dtype=float32)], &apos;dense&apos;: [array([[-3.7658346]], dtype=float32), array([-0.00123706], dtype=float32)], &apos;input_1&apos;: []} #What are the predicted values of hidden units? Since we used Keras&apos;s functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest. In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model &quot;rnn_model&quot; shares the weights obtained by the previous model&apos;s optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations. ```python rnn_model = Model(inputs=[inp],outputs=[rnn]) rnn_model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;) hidden_units = rnn_model.predict(X_test).flatten() ``` Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave. ```python upto = 100 predicted_sin_wave = ws[&quot;dense&quot;][0][0][0]*hidden_units + ws[&quot;dense&quot;][1][0] plt.figure(figsize=(19,3)) plt.plot(y_test[:upto],label=&quot;y_pred&quot;) plt.plot(hidden_units[:upto],label=&quot;hidden units&quot;) plt.plot(predicted_sin_wave[:upto],&quot;*&quot;, label=&quot;w2 * hidden units + b2&quot;) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_19_0.png) #Obtain predicted sin wave at the next time point given the current sin wave by hand We understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as: ```python def RNNmodel(ws,x,h=0): &apos;&apos;&apos; ws: predicted weights x : scalar current sign value h : scalar RNN hidden unit &apos;&apos;&apos; h = np.tanh(x*ws[&quot;RNN&quot;][0][0][0] + h*ws[&quot;RNN&quot;][1][0][0] + ws[&quot;RNN&quot;][2][0]) x = h*ws[&quot;dense&quot;][0][0][0] + ws[&quot;dense&quot;][1][0] return(x,h) ``` Naturally, you can obtain the predicted sin waves $(x_1,x_2,...,x_t)$ by looping around RNNmodel as: $x^∗_{t+1},h_{t+1} = $ RNNmodel$(x_t,h_t)$ Here $x^∗_t$ indicates the estimated value of $x$ at time point $t$. As our model is not so complicated, we can readily implement this algorithm as: ```python upto = 50 ## predict the first sin values xstars, hs_hand = [], [] for i, x in enumerate(df_test.values): if i == 0: h = 0 ## initial hidden layer value is zero xstar = x print(&quot;initial value of sin x_0 = {}, h_0 = {}&quot;.format(x,h)) hs_hand.append(h) xstars.append(xstar[0]) xstar, h = RNNmodel(ws,x, h) assert len(df_test.values) == len(xstars) ``` initial value of sin x_0 = [-1.27375647e-13], h_0 = 0 In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_test ```python plt.figure(figsize=(18,3)) plt.plot(df_test.values[:upto],label=&quot;true&quot;,alpha=0.3,linewidth=5) plt.plot(xstars[:upto],label=&quot;sin prediction (xstar)&quot;) plt.plot(hs_hand[:upto],label=&quot;hidden state (xstar)&quot;) plt.legend() ``` ![png](/old/rnn_sin/output_25_1.png) You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on? ```python &quot;validation loss {:3.2f}&quot;.format(np.mean((np.array(xstars) - df_test[&quot;sin_t&quot;].values)**2)) ``` &apos;validation loss 0.08&apos; Let&apos;s predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as: x1,x2x2,x3x3,x4... ```python y_test_from_keras = model.predict(X_test).flatten() ``` Notice that this predicted values are exactly the same as the ones calculated before. ```python np.all(predicted_sin_wave == y_test_from_keras) ``` True As the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars. ```python y_test_from_keras = [np.NaN, np.NaN] + list(y_test_from_keras.flatten()) h_test_from_keras = [np.NaN, np.NaN] + list(hidden_units.flatten()) ``` The plot shows that Keras&apos;s predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras&apos;s prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output? ```python plt.figure(figsize=(18,3)) plt.plot(df_test.values[:upto],label=&quot;true&quot;,alpha=0.3,linewidth=5) plt.plot(xstars[:upto],label=&quot;sin prediction (xstar)&quot;) plt.plot(hs_hand[:upto],label=&quot;hidden state (xstar)&quot;) plt.plot(y_test_from_keras[:upto],label=&quot;sin prediction (keras)&quot;) plt.plot(h_test_from_keras[:upto],label=&quot;hidden state (keras)&quot;) plt.legend() print(&quot;validation loss {:6.5f}&quot;.format(np.nanmean((np.array(y_test_from_keras) - df_test[&quot;sin_t&quot;].values)**2))) ``` validation loss 0.00021 ![png](/old/rnn_sin/output_35_1.png) Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role. As the BPTT algorithm only passed back 2 steps, the model assumes that: the hidden units are initialized to zero every 2 steps. the prediction of the next sin value (xt+1) is based on the hidden unit (ht) which is created by updating the hidden units twice in the past assuming that ht−1=0. x∗t,ht=RNNmodel(xt−1,0)xt+1,−=RNNmodel(xt,ht) Note that the intermediate predicted sin x∗t based on ht−1=0 should not be used as the predicted sin value. This is because the x∗t was not directly used to evaluate the loss function. Finally, obtain the Keras&apos;s predicted sin wave at the next time point given the current sin wave by hand. ```python def myRNNpredict(ws,X): X = X.flatten() h = 0 for i in range(len(X)): x,h = RNNmodel(ws,X[i],h) return(x,h) xs, hs = [], [] for i in range(X_test.shape[0]): x, h = myRNNpredict(ws,X_test[i,:,:]) xs.append(x) hs.append(h) ``` ```python print(&quot;All sin estimates agree with ones from Keras = {}&quot;.format( np.all(np.abs( np.array(xs) - np.array(y_test_from_keras[2:]) ) &amp;lt; 1E-5))) print(&quot;All hidden state estmiates agree with ones fome Keras = {}&quot;.format( np.all(np.abs( np.array(hs) - np.array(h_test_from_keras[2:]) ) &amp;lt; 1E-5)) ) ``` All sin estimates agree with ones from Keras = True All hidden state estmiates agree with ones fome Keras = True Now we understand how Keras is predicting the sin wave. In fact, Keras has a way to return xstar as predicted values, using &quot;stateful&quot; flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself. In order to predict in &quot;stateful&quot; mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1. ```python model_stateful,_ = define_model(length_of_sequences = 1, batch_size=1, stateful = True) model_stateful.summary() ``` Model: &quot;model_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (1, 1, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (1, 1) 3 _________________________________________________________________ dense (Dense) (1, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Assign the trained weights into the stateful model. ```python for layer in model.layers: for layer_predict in model_stateful.layers: if (layer_predict.name == layer.name): layer_predict.set_weights(layer.get_weights()) break ``` Now we predict in stateful mode. Here it is very important to reset_state() before the prediction so that h0=0. ```python pred = df_test.values[0][0] stateful_sin = [] model_stateful.reset_states() for i in range(df_test.shape[0]): stateful_sin.append(pred) pred = model_stateful.predict(df_test.values[i].reshape(1,1,1))[0][0] stateful_sin = np.array(stateful_sin) ``` ```python print(&quot;All predicted sin values with stateful model agree to xstars = {}&quot;.format( np.all(np.abs(np.array(stateful_sin) - np.array(xstars))&amp;lt; 1E-5))) ``` All predicted sin values with stateful model agree to xstars = True Now we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example. However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say K? Do we always have to go back all the K time steps, set ht−K=0 and then feed forward K steps in order to predict at the time point t? This may be computationally intense.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://colab.research.google.com/assets/colab-badge.svg">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NexT">
<meta name="twitter:description" content="```python import sys print(sys.version) import tensorflow print(tensorflow.__version__) import keras print(keras.__version__) import pandas as pd import numpy as np import math import random import matplotlib.pyplot as plt %matplotlib inline ``` 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info. 1.15.0 2.2.5 Using TensorFlow backend. #Generate a sin wave with no noise First, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves! Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles. ```python def noisy_sin(steps_per_cycle = 50, number_of_cycles = 500, random_factor = 0.4): &apos;&apos;&apos; number_of_cycles : The number of steps required for one cycle Return : pd.DataFrame() with column sin_t containing the generated sin wave &apos;&apos;&apos; random.seed(0) df = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=[&quot;t&quot;]) df[&quot;sin_t&quot;] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)+ random.uniform(-1.0, +1.0) * random_factor)) df[&quot;sin_t_clean&quot;] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle))) print(&quot;create period-{} sin wave with {} cycles&quot;.format(steps_per_cycle,number_of_cycles)) print(&quot;In total, the sin wave time series length is {}&quot;.format(steps_per_cycle*number_of_cycles+1)) return(df) steps_per_cycle = 10 df = noisy_sin(steps_per_cycle=steps_per_cycle, random_factor = 0) n_plot = 8 df[[&quot;sin_t&quot;]].head(steps_per_cycle * n_plot).plot( title=&quot;Generated first {} cycles&quot;.format(n_plot), figsize=(15,3)) ``` create period-10 sin wave with 500 cycles In total, the sin wave time series length is 5001 ![png](/old/rnn_sin/output_3_2.png) Create a training and testing data. Here, the controversial &quot;length of time series&quot; parameter comes into play. For now, we set this parameter to 2. ```python def _load_data(data, n_prev = 100): &quot;&quot;&quot; data should be pd.DataFrame() &quot;&quot;&quot; docX, docY = [], [] for i in range(len(data)-n_prev): docX.append(data.iloc[i:i+n_prev].as_matrix()) docY.append(data.iloc[i+n_prev].as_matrix()) alsX = np.array(docX) alsY = np.array(docY) return alsX, alsY length_of_sequences = 2 test_size = 0.25 ntr = int(len(df) * (1 - test_size)) df_train = df[[&quot;sin_t&quot;]].iloc[:ntr] df_test = df[[&quot;sin_t&quot;]].iloc[ntr:] (X_train, y_train) = _load_data(df_train, n_prev = length_of_sequences) (X_test, y_test) = _load_data(df_test, n_prev = length_of_sequences) print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) ``` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead. if __name__ == &apos;__main__&apos;: (3748, 2, 1) (3748, 1) (1249, 2, 1) (1249, 1) #Simple RNN model As a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit. * The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias * The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for bias In total, there are only 5 weights in this model. Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as: $$ \begin{aligned} h_{t}=\tanh \left(x_{t}^{T} w_{1 x}+h_{t-1}^{T} w_{1 h}+b_{1}\right) \\ x_{t+1}=h_{t}^{T} w_{2}+b_{2} \end{aligned} $$ Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to &quot;remember&quot; the past history of $x_t$ through the hidden unit $h_t$. ##batch_shape needs for BPTT.¶ * Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data. * This means that the each batch is treated as independent. * This batch_shape determines the size of this subset. * Every batch starts will the initial hidden unit $h_0=0$. * As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value. * The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!! Now, we define this model using Keras and show the model summary. ```python from keras.layers import Input from keras.models import Model from keras.layers.core import Dense, Activation from keras.layers.recurrent import SimpleRNN def define_model(length_of_sequences, batch_size = None, stateful = False): in_out_neurons = 1 hidden_neurons = 1 inp = Input(batch_shape=(batch_size, length_of_sequences, in_out_neurons)) rnn = SimpleRNN(hidden_neurons, return_sequences=False, stateful = stateful, name=&quot;RNN&quot;)(inp) dens = Dense(in_out_neurons,name=&quot;dense&quot;)(rnn) model = Model(inputs=[inp],outputs=[dens]) model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;) return(model,(inp,rnn,dens)) ## use the default values for batch_size, stateful model, (inp,rnn,dens) = define_model(length_of_sequences = X_train.shape[1]) model.summary() ``` WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. Model: &quot;model_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 2, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (None, 1) 3 _________________________________________________________________ dense (Dense) (None, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Now we train the model. The script was run without GPU. ```python hist = model.fit(X_train, y_train, batch_size=600, epochs=1000, verbose=False,validation_split=0.05) ``` WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. #Plot of val_loss and loss. The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001. ```python for label in [&quot;loss&quot;,&quot;val_loss&quot;]: plt.plot(hist.history[label],label=label) plt.ylabel(&quot;loss&quot;) plt.xlabel(&quot;epoch&quot;) plt.title(&quot;The final validation loss: {}&quot;.format(hist.history[&quot;val_loss&quot;][-1])) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_11_0.png) #The plot of true and predicted sin waves look nearly identical ```python y_pred = model.predict(X_test) plt.figure(figsize=(19,3)) plt.plot(y_test,label=&quot;true&quot;) plt.plot(y_pred,label=&quot;predicted&quot;) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_13_0.png) #What are the model weights? The best way to understand the RNN model is to create a model from scratch. Let&apos;s extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers. ```python ws = {} for layer in model.layers: ws[layer.name] = layer.get_weights() ws ``` {&apos;RNN&apos;: [array([[-0.43695387]], dtype=float32), array([[-0.64668506]], dtype=float32), array([0.00117508], dtype=float32)], &apos;dense&apos;: [array([[-3.7658346]], dtype=float32), array([-0.00123706], dtype=float32)], &apos;input_1&apos;: []} #What are the predicted values of hidden units? Since we used Keras&apos;s functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest. In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model &quot;rnn_model&quot; shares the weights obtained by the previous model&apos;s optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations. ```python rnn_model = Model(inputs=[inp],outputs=[rnn]) rnn_model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;) hidden_units = rnn_model.predict(X_test).flatten() ``` Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave. ```python upto = 100 predicted_sin_wave = ws[&quot;dense&quot;][0][0][0]*hidden_units + ws[&quot;dense&quot;][1][0] plt.figure(figsize=(19,3)) plt.plot(y_test[:upto],label=&quot;y_pred&quot;) plt.plot(hidden_units[:upto],label=&quot;hidden units&quot;) plt.plot(predicted_sin_wave[:upto],&quot;*&quot;, label=&quot;w2 * hidden units + b2&quot;) plt.legend() plt.show() ``` ![png](/old/rnn_sin/output_19_0.png) #Obtain predicted sin wave at the next time point given the current sin wave by hand We understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as: ```python def RNNmodel(ws,x,h=0): &apos;&apos;&apos; ws: predicted weights x : scalar current sign value h : scalar RNN hidden unit &apos;&apos;&apos; h = np.tanh(x*ws[&quot;RNN&quot;][0][0][0] + h*ws[&quot;RNN&quot;][1][0][0] + ws[&quot;RNN&quot;][2][0]) x = h*ws[&quot;dense&quot;][0][0][0] + ws[&quot;dense&quot;][1][0] return(x,h) ``` Naturally, you can obtain the predicted sin waves $(x_1,x_2,...,x_t)$ by looping around RNNmodel as: $x^∗_{t+1},h_{t+1} = $ RNNmodel$(x_t,h_t)$ Here $x^∗_t$ indicates the estimated value of $x$ at time point $t$. As our model is not so complicated, we can readily implement this algorithm as: ```python upto = 50 ## predict the first sin values xstars, hs_hand = [], [] for i, x in enumerate(df_test.values): if i == 0: h = 0 ## initial hidden layer value is zero xstar = x print(&quot;initial value of sin x_0 = {}, h_0 = {}&quot;.format(x,h)) hs_hand.append(h) xstars.append(xstar[0]) xstar, h = RNNmodel(ws,x, h) assert len(df_test.values) == len(xstars) ``` initial value of sin x_0 = [-1.27375647e-13], h_0 = 0 In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_test ```python plt.figure(figsize=(18,3)) plt.plot(df_test.values[:upto],label=&quot;true&quot;,alpha=0.3,linewidth=5) plt.plot(xstars[:upto],label=&quot;sin prediction (xstar)&quot;) plt.plot(hs_hand[:upto],label=&quot;hidden state (xstar)&quot;) plt.legend() ``` ![png](/old/rnn_sin/output_25_1.png) You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on? ```python &quot;validation loss {:3.2f}&quot;.format(np.mean((np.array(xstars) - df_test[&quot;sin_t&quot;].values)**2)) ``` &apos;validation loss 0.08&apos; Let&apos;s predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as: x1,x2x2,x3x3,x4... ```python y_test_from_keras = model.predict(X_test).flatten() ``` Notice that this predicted values are exactly the same as the ones calculated before. ```python np.all(predicted_sin_wave == y_test_from_keras) ``` True As the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars. ```python y_test_from_keras = [np.NaN, np.NaN] + list(y_test_from_keras.flatten()) h_test_from_keras = [np.NaN, np.NaN] + list(hidden_units.flatten()) ``` The plot shows that Keras&apos;s predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras&apos;s prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output? ```python plt.figure(figsize=(18,3)) plt.plot(df_test.values[:upto],label=&quot;true&quot;,alpha=0.3,linewidth=5) plt.plot(xstars[:upto],label=&quot;sin prediction (xstar)&quot;) plt.plot(hs_hand[:upto],label=&quot;hidden state (xstar)&quot;) plt.plot(y_test_from_keras[:upto],label=&quot;sin prediction (keras)&quot;) plt.plot(h_test_from_keras[:upto],label=&quot;hidden state (keras)&quot;) plt.legend() print(&quot;validation loss {:6.5f}&quot;.format(np.nanmean((np.array(y_test_from_keras) - df_test[&quot;sin_t&quot;].values)**2))) ``` validation loss 0.00021 ![png](/old/rnn_sin/output_35_1.png) Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role. As the BPTT algorithm only passed back 2 steps, the model assumes that: the hidden units are initialized to zero every 2 steps. the prediction of the next sin value (xt+1) is based on the hidden unit (ht) which is created by updating the hidden units twice in the past assuming that ht−1=0. x∗t,ht=RNNmodel(xt−1,0)xt+1,−=RNNmodel(xt,ht) Note that the intermediate predicted sin x∗t based on ht−1=0 should not be used as the predicted sin value. This is because the x∗t was not directly used to evaluate the loss function. Finally, obtain the Keras&apos;s predicted sin wave at the next time point given the current sin wave by hand. ```python def myRNNpredict(ws,X): X = X.flatten() h = 0 for i in range(len(X)): x,h = RNNmodel(ws,X[i],h) return(x,h) xs, hs = [], [] for i in range(X_test.shape[0]): x, h = myRNNpredict(ws,X_test[i,:,:]) xs.append(x) hs.append(h) ``` ```python print(&quot;All sin estimates agree with ones from Keras = {}&quot;.format( np.all(np.abs( np.array(xs) - np.array(y_test_from_keras[2:]) ) &amp;lt; 1E-5))) print(&quot;All hidden state estmiates agree with ones fome Keras = {}&quot;.format( np.all(np.abs( np.array(hs) - np.array(h_test_from_keras[2:]) ) &amp;lt; 1E-5)) ) ``` All sin estimates agree with ones from Keras = True All hidden state estmiates agree with ones fome Keras = True Now we understand how Keras is predicting the sin wave. In fact, Keras has a way to return xstar as predicted values, using &quot;stateful&quot; flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself. In order to predict in &quot;stateful&quot; mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1. ```python model_stateful,_ = define_model(length_of_sequences = 1, batch_size=1, stateful = True) model_stateful.summary() ``` Model: &quot;model_3&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (1, 1, 1) 0 _________________________________________________________________ RNN (SimpleRNN) (1, 1) 3 _________________________________________________________________ dense (Dense) (1, 1) 2 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ Assign the trained weights into the stateful model. ```python for layer in model.layers: for layer_predict in model_stateful.layers: if (layer_predict.name == layer.name): layer_predict.set_weights(layer.get_weights()) break ``` Now we predict in stateful mode. Here it is very important to reset_state() before the prediction so that h0=0. ```python pred = df_test.values[0][0] stateful_sin = [] model_stateful.reset_states() for i in range(df_test.shape[0]): stateful_sin.append(pred) pred = model_stateful.predict(df_test.values[i].reshape(1,1,1))[0][0] stateful_sin = np.array(stateful_sin) ``` ```python print(&quot;All predicted sin values with stateful model agree to xstars = {}&quot;.format( np.all(np.abs(np.array(stateful_sin) - np.array(xstars))&amp;lt; 1E-5))) ``` All predicted sin values with stateful model agree to xstars = True Now we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example. However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say K? Do we always have to go back all the K time steps, set ht−K=0 and then feed forward K steps in order to predict at the time point t? This may be computationally intense.">
<meta name="twitter:image" content="https://colab.research.google.com/assets/colab-badge.svg">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://localhost:4000/"/>





  <title>               | NexT      </title>
  
















</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NexT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
<div id="posts" class="posts-expand">
<header class="post-header">

	<h1 class="post-title" itemprop="name headline">
    
      
    
  </h1>



</header>

  
  
    
<p><a href="https://colab.research.google.com/github/allenlu2009/tensorflow2/blob/master/rnn_sin.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="k">print</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span>
<span class="k">print</span><span class="p">(</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="k">print</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
</code></pre></div></div>

<p style="color: red;">
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br />
We recommend you <a href="https://www.tensorflow.org/guide/migrate" target="_blank">upgrade</a> now 
or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:
<a href="https://colab.research.google.com/notebooks/tensorflow_version.ipynb" target="_blank">more info</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.15.0
2.2.5


Using TensorFlow backend.
</code></pre></div></div>

<p>#Generate a sin wave with no noise
First, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves!</p>

<p>Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">noisy_sin</span><span class="p">(</span><span class="n">steps_per_cycle</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
              <span class="n">number_of_cycles</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
              <span class="n">random_factor</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">):</span>
    <span class="s">'''
    number_of_cycles : The number of steps required for one cycle
    
    Return : 
    pd.DataFrame() with column sin_t containing the generated sin wave 
    '''</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">steps_per_cycle</span> <span class="o">*</span> <span class="n">number_of_cycles</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"t"</span><span class="p">])</span>
    <span class="n">df</span><span class="p">[</span><span class="s">"sin_t"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">steps_per_cycle</span><span class="p">)</span><span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">+</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">random_factor</span><span class="p">))</span>
    <span class="n">df</span><span class="p">[</span><span class="s">"sin_t_clean"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="n">steps_per_cycle</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"create period-{} sin wave with {} cycles"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">steps_per_cycle</span><span class="p">,</span><span class="n">number_of_cycles</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"In total, the sin wave time series length is {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">steps_per_cycle</span><span class="o">*</span><span class="n">number_of_cycles</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>



<span class="n">steps_per_cycle</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">noisy_sin</span><span class="p">(</span><span class="n">steps_per_cycle</span><span class="o">=</span><span class="n">steps_per_cycle</span><span class="p">,</span>
              <span class="n">random_factor</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">n_plot</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">df</span><span class="p">[[</span><span class="s">"sin_t"</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">steps_per_cycle</span> <span class="o">*</span> <span class="n">n_plot</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
      <span class="n">title</span><span class="o">=</span><span class="s">"Generated first {} cycles"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">n_plot</span><span class="p">),</span>
      <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create period-10 sin wave with 500 cycles
In total, the sin wave time series length is 5001





&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4922f362e8&gt;
</code></pre></div></div>

<p><img src="/old/rnn_sin/output_3_2.png" alt="png" /></p>

<p>Create a training and testing data. Here, the controversial “length of time series” parameter comes into play. For now, we set this parameter to 2.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_load_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_prev</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>  
    <span class="s">"""
    data should be pd.DataFrame()
    """</span>

    <span class="n">docX</span><span class="p">,</span> <span class="n">docY</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="n">n_prev</span><span class="p">):</span>
        <span class="n">docX</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">n_prev</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">())</span>
        <span class="n">docY</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">n_prev</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">())</span>
    <span class="n">alsX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">docX</span><span class="p">)</span>
    <span class="n">alsY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">docY</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">alsX</span><span class="p">,</span> <span class="n">alsY</span>

<span class="n">length_of_sequences</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">ntr</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">test_size</span><span class="p">))</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">"sin_t"</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">ntr</span><span class="p">]</span>
<span class="n">df_test</span>  <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">"sin_t"</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">ntr</span><span class="p">:]</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">=</span> <span class="n">_load_data</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span> <span class="n">n_prev</span> <span class="o">=</span> <span class="n">length_of_sequences</span><span class="p">)</span>
<span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>   <span class="o">=</span> <span class="n">_load_data</span><span class="p">(</span><span class="n">df_test</span><span class="p">,</span> <span class="n">n_prev</span> <span class="o">=</span> <span class="n">length_of_sequences</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  if __name__ == '__main__':


(3748, 2, 1) (3748, 1) (1249, 2, 1) (1249, 1)
</code></pre></div></div>

<p>#Simple RNN model</p>

<p>As a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit.</p>

<ul>
  <li>The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias</li>
  <li>The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for bias
In total, there are only 5 weights in this model.</li>
</ul>

<p>Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as:
<script type="math/tex">\begin{aligned}
h_{t}=\tanh \left(x_{t}^{T} w_{1 x}+h_{t-1}^{T} w_{1 h}+b_{1}\right) \\
x_{t+1}=h_{t}^{T} w_{2}+b_{2}
\end{aligned}</script></p>

<p>Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to “remember” the past history of $x_t$ through the hidden unit $h_t$.</p>

<p>##batch_shape needs for BPTT.¶</p>

<ul>
  <li>Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data.</li>
  <li>This means that the each batch is treated as independent.</li>
  <li>This batch_shape determines the size of this subset.</li>
  <li>Every batch starts will the initial hidden unit $h_0=0$.</li>
  <li>As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value.</li>
  <li>The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!!</li>
</ul>

<p>Now, we define this model using Keras and show the model summary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span> 
<span class="kn">from</span> <span class="nn">keras.layers.recurrent</span> <span class="kn">import</span> <span class="n">SimpleRNN</span>



<span class="k">def</span> <span class="nf">define_model</span><span class="p">(</span><span class="n">length_of_sequences</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">stateful</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
    <span class="n">in_out_neurons</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">hidden_neurons</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">batch_shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> 
                <span class="n">length_of_sequences</span><span class="p">,</span> 
                <span class="n">in_out_neurons</span><span class="p">))</span>  

    <span class="n">rnn</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">hidden_neurons</span><span class="p">,</span> 
                    <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                    <span class="n">stateful</span> <span class="o">=</span> <span class="n">stateful</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="s">"RNN"</span><span class="p">)(</span><span class="n">inp</span><span class="p">)</span>

    <span class="n">dens</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">in_out_neurons</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">"dense"</span><span class="p">)(</span><span class="n">rnn</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inp</span><span class="p">],</span><span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">dens</span><span class="p">])</span>
    
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mean_squared_error"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"rmsprop"</span><span class="p">)</span>

    
    <span class="k">return</span><span class="p">(</span><span class="n">model</span><span class="p">,(</span><span class="n">inp</span><span class="p">,</span><span class="n">rnn</span><span class="p">,</span><span class="n">dens</span><span class="p">))</span>
<span class="c1">## use the default values for batch_size, stateful
</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span><span class="n">rnn</span><span class="p">,</span><span class="n">dens</span><span class="p">)</span> <span class="o">=</span> <span class="n">define_model</span><span class="p">(</span><span class="n">length_of_sequences</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 2, 1)              0         
_________________________________________________________________
RNN (SimpleRNN)              (None, 1)                 3         
_________________________________________________________________
dense (Dense)                (None, 1)                 2         
=================================================================
Total params: 5
Trainable params: 5
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<p>Now we train the model. The script was run without GPU.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
                 <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.
</code></pre></div></div>

<p>#Plot of val_loss and loss.</p>

<p>The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"loss"</span><span class="p">,</span><span class="s">"val_loss"</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="n">label</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"The final validation loss: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">"val_loss"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/old/rnn_sin/output_11_0.png" alt="png" /></p>

<p>#The plot of true and predicted sin waves look nearly identical</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">19</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">"true"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s">"predicted"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/old/rnn_sin/output_13_0.png" alt="png" /></p>

<p>#What are the model weights?</p>

<p>The best way to understand the RNN model is to create a model from scratch. Let’s extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ws</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="n">ws</span><span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="n">ws</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'RNN': [array([[-0.43695387]], dtype=float32),
  array([[-0.64668506]], dtype=float32),
  array([0.00117508], dtype=float32)],
 'dense': [array([[-3.7658346]], dtype=float32),
  array([-0.00123706], dtype=float32)],
 'input_1': []}
</code></pre></div></div>

<p>#What are the predicted values of hidden units?</p>

<p>Since we used Keras’s functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest.</p>

<p>In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model “rnn_model” shares the weights obtained by the previous model’s optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inp</span><span class="p">],</span><span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">rnn</span><span class="p">])</span>
<span class="n">rnn_model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mean_squared_error"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">"rmsprop"</span><span class="p">)</span>
<span class="n">hidden_units</span> <span class="o">=</span> <span class="n">rnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></div>

<p>Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">upto</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">predicted_sin_wave</span> <span class="o">=</span> <span class="n">ws</span><span class="p">[</span><span class="s">"dense"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">hidden_units</span> <span class="o">+</span> <span class="n">ws</span><span class="p">[</span><span class="s">"dense"</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">19</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"y_pred"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"hidden units"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predicted_sin_wave</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="s">"*"</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s">"w2 * hidden units + b2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/old/rnn_sin/output_19_0.png" alt="png" /></p>

<p>#Obtain predicted sin wave at the next time point given the current sin wave by hand</p>

<p>We understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">RNNmodel</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="s">'''
    ws: predicted weights 
    x : scalar current sign value
    h : scalar RNN hidden unit 
    '''</span>
           
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">ws</span><span class="p">[</span><span class="s">"RNN"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">h</span><span class="o">*</span><span class="n">ws</span><span class="p">[</span><span class="s">"RNN"</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ws</span><span class="p">[</span><span class="s">"RNN"</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="o">*</span><span class="n">ws</span><span class="p">[</span><span class="s">"dense"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ws</span><span class="p">[</span><span class="s">"dense"</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">return</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<p>Naturally, you can obtain the predicted sin waves $(x_1,x_2,…,x_t)$ by looping around RNNmodel as:</p>

<p>$x^∗<em>{t+1},h</em>{t+1} = $ RNNmodel$(x_t,h_t)$</p>

<p>Here $x^∗_t$ indicates the estimated value of $x$ at time point $t$. As our model is not so complicated, we can readily implement this algorithm as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">upto</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1">## predict the first  sin values
</span><span class="n">xstars</span><span class="p">,</span> <span class="n">hs_hand</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">## initial hidden layer value is zero
</span>        <span class="n">xstar</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"initial value of sin x_0 = {}, h_0 = {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">))</span>
    <span class="n">hs_hand</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">xstars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xstar</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">xstar</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">RNNmodel</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">xstars</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>initial value of sin x_0 = [-1.27375647e-13], h_0 = 0
</code></pre></div></div>

<p>In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_test</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"true"</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xstars</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"sin prediction (xstar)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hs_hand</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"hidden state (xstar)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7f490d2fe908&gt;
</code></pre></div></div>

<p><img src="/old/rnn_sin/output_25_1.png" alt="png" /></p>

<p>You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"validation loss {:3.2f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xstars</span><span class="p">)</span> <span class="o">-</span> <span class="n">df_test</span><span class="p">[</span><span class="s">"sin_t"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'validation loss 0.08'
</code></pre></div></div>

<p>Let’s predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as:</p>

<p>x1,x2x2,x3x3,x4…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_from_keras</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></div>

<p>Notice that this predicted values are exactly the same as the ones calculated before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">predicted_sin_wave</span> <span class="o">==</span> <span class="n">y_test_from_keras</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<p>As the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_from_keras</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">NaN</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">NaN</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">y_test_from_keras</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="n">h_test_from_keras</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">NaN</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">NaN</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</code></pre></div></div>

<p>The plot shows that Keras’s predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras’s prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"true"</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xstars</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"sin prediction (xstar)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hs_hand</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"hidden state (xstar)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test_from_keras</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"sin prediction (keras)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h_test_from_keras</span><span class="p">[:</span><span class="n">upto</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">"hidden state (keras)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"validation loss {:6.5f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test_from_keras</span><span class="p">)</span> <span class="o">-</span> <span class="n">df_test</span><span class="p">[</span><span class="s">"sin_t"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>validation loss 0.00021
</code></pre></div></div>

<p><img src="/old/rnn_sin/output_35_1.png" alt="png" /></p>

<p>Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role.</p>

<p>As the BPTT algorithm only passed back 2 steps, the model assumes that:</p>

<p>the hidden units are initialized to zero every 2 steps.
the prediction of the next sin value (xt+1) is based on the hidden unit (ht) which is created by updating the hidden units twice in the past assuming that ht−1=0.
x∗t,ht=RNNmodel(xt−1,0)xt+1,−=RNNmodel(xt,ht)
Note that the intermediate predicted sin x∗t based on ht−1=0 should not be used as the predicted sin value. This is because the x∗t was not directly used to evaluate the loss function.</p>

<p>Finally, obtain the Keras’s predicted sin wave at the next time point given the current sin wave by hand.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">myRNNpredict</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">RNNmodel</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">h</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>


<span class="n">xs</span><span class="p">,</span> <span class="n">hs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">myRNNpredict</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:])</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">hs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"All sin estimates agree with ones from Keras = {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test_from_keras</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span> <span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1E-5</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"All hidden state estmiates agree with ones fome Keras = {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hs</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">h_test_from_keras</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span> <span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1E-5</span><span class="p">))</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>All sin estimates agree with ones from Keras = True
All hidden state estmiates agree with ones fome Keras = True
</code></pre></div></div>

<p>Now we understand how Keras is predicting the sin wave.</p>

<p>In fact, Keras has a way to return xstar as predicted values, using “stateful” flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself.</p>

<p>In order to predict in “stateful” mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_stateful</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">define_model</span><span class="p">(</span><span class="n">length_of_sequences</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> 
                              <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                              <span class="n">stateful</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">model_stateful</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (1, 1, 1)                 0         
_________________________________________________________________
RNN (SimpleRNN)              (1, 1)                    3         
_________________________________________________________________
dense (Dense)                (1, 1)                    2         
=================================================================
Total params: 5
Trainable params: 5
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<p>Assign the trained weights into the stateful model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>        
    <span class="k">for</span> <span class="n">layer_predict</span> <span class="ow">in</span> <span class="n">model_stateful</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">layer_predict</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
            <span class="n">layer_predict</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
            <span class="k">break</span>
</code></pre></div></div>

<p>Now we predict in stateful mode. Here it is very important to reset_state() before the prediction so that h0=0.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pred</span> <span class="o">=</span> <span class="n">df_test</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">stateful_sin</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">model_stateful</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">stateful_sin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model_stateful</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df_test</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
<span class="n">stateful_sin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stateful_sin</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"All predicted sin values with stateful model agree to xstars = {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stateful_sin</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xstars</span><span class="p">))</span><span class="o">&lt;</span> <span class="mf">1E-5</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>All predicted sin values with stateful model agree to xstars = True
</code></pre></div></div>

<p>Now we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example.</p>

<p>However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say K? Do we always have to go back all the K time steps, set ht−K=0 and then feed forward K steps in order to predict at the time point t? This may be computationally intense.</p>

  
</div>


          </div>
          


          

        </div>
        
          

  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/assets/images/avatar.gif"
               alt="Allen Lu (from John Doe)" />
          <p class="site-author-name" itemprop="name">Allen Lu (from John Doe)</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">30</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
        
        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>

        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Allen Lu (from John Doe)</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://jekyllrb.com">Jekyll</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  











  




  

    

  







  






  

  

  
  


  

  

  

</body>
</html>

