<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[Math AI - From EM to Variational Bayesian Inference]]></title>
      <url>/ai/2021/08/05/Math_AI_Baysian_variational/</url>
      <content type="text"><![CDATA[Major Reference  [@poczosCllusteringEM2015]  [@matasExpectationMaximization2018] good reference  [@choyExpectationMaximization2017]  [@tzikasVariationalApproximation2008] excellent introductory paperEM AlgorithmEM 可以視為 MLE 的 extension to hidden state / data.Let’s start with EM algorithm此時可以用 $\eqref{eqQ}$ 定義 EM algorithm一般 $\eqref{eqQ}$ 的 joint distribution $p\left(\mathbf{x}, \mathbf{z} ; \boldsymbol{\theta}\right)$ 包含完整的 data，容易計算或有 analytical solution.大多的問題是 $\eqref{eqE}$ conditional or posterior distribution 是否容易計算，是否有 analytical solution.Variational EM Framework最簡單的話就是 hidden variable z = z1, z2, .., zM.  and p(z) = p(z1)…p(zM).什麼時候會有這種 distribution product?  後面會說明。where$\eqref{eqVarELBO}$ 是 (variational, 因為有 KL divergence) lower bound, KL divergence 必大於 0, 負號後必小於 0.  第二項加上負號是 self-entropy 必大於 0.直觀看出讓 KL 為 0，就是 $q_j(z_j) = \tilde{p}(x, z_j; \theta)$, 似乎就是最大值 (how about the self-entropy?).也就是 optimal distribution $q_j^* (z_j)$ 是上面的 const 可以由 distribution normalization 得到。所以我們可以得到一組 consistency conditions $\eqref{eqVarJ2}$ for the maximum of variational lower bound subject to $\eqref{eqFactor}$.$\eqref{eqVarJ2}$ 顯然不會有 explicit solution, 因為 $q_j$ factors 之間是相互 dependent.  A consistent solution 需要 cycling through these factors.  我們定義 Variational EM algorithmExamples例一： Linear Regression (filter/estimate a noisy signal)我很喜歡這個例子。從簡單的 least-square error filter 進步到 Kalman filter.  類似的應用：deconvolution/equalization, channel estimation, speech recognition, frequency estimation, time series prediction, andsystem identification.問題描述考慮一個未知信號 $y(x) \in R, x \in \Omega ⊆ R^N$, i.e. $R^N \to R$. 我們想要 predict its value $t_* = y(x_)$ at an arbitrary location $x_ \in \Omega$.我們用 vector 表示 $(t_1, \cdots, t_N)$ using a vector t = (t1,…, tN)T of N noisy observations tn = y(xn) + εn, at locations x = (x1,…, xN)T, xn ∈ , n = 1,…, N. The additive noise εn is commonly assumed to be independent, zeromean, Gaussian distributed:注意 $y(x)$ 不是真正的 observables, 而是加上 noise 之後的 t 才是 observations.  我們的目標就是用 $\mathbf{t}$ 來 estimate $\mathbf{w}$.The likelihood function三種解法圖式以下我們用三種 methodologies 用 $\mathbf{t}$ 來 estimate $\mathbf{w}$ (i.e. signal) and $\beta$ (i.e. noise if needed).Method 1: ML Estimation 如果 number of parameters (w) is the same as the number of observations (t), the ML estimates are very sensitive to the model noise.  我們可以用 DAG (Directed Acyclic Graphic) 說明，如下圖 (a).  雙圓框 t 代表 observed random variable. 方框 (W, beta) 代表 parameter to be estimated.  單圓框（e.g. (b) W）代表 hidden random variable.Method 2: 假設 weight W 是 random variable with imposed prior. 我們先用 a simple Bayesian model with stationary Gaussian prior on weight, 如下圖 (b).  以這個 model 而言，我們用 EM algorithm performs Bayesian inference.  結果 robust to noise, 類似 Kalman filter?Method 3: method 2 的一個缺點是假設 stationary Gaussian noise (i.e. $\beta$, a fixed value to be estimated, 無法 capture the local signal properties.  我們可以引入更複雜 spatially/temporally varying hierarchical model which is based on a non-stationary Gaussian prior for the weight, W and a hyperprior, $\beta$, 如下圖 (c).這麼複雜的 DAG 顯然無法用 EM algorithm 解，必須用本文的 “Variational EM Framework” infer values of the unknowns.Method 1, ML for Vanilla Linear Regression始於 likelihood function假設 $\mathbf{w}, \beta$ 為 constant parameters (to be estimated).  Maximize the likelihood or log-likelihood 等價於 minimize $|\mathbf{t}-\Phi \mathbf{w}|^{2}$.  因此**maximal likelihood (ML) estimate of w 等價 least squares (LS) estimate.很多情況 $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)$ 可能是 “ill-conditioned” and difficult to invert.  意味如果 observation t 包含 noise $\varepsilon$, noise 會嚴重干擾 $\mathbf{w}_{L S}$ estimation.例一：Communication equalization/deconvolutionAssuming a lowpass channel $\Phi = 1 + 0.9 z^{-1}$.  The equalizer $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{T}$ 變成 highpass filter; zero-forcing equalizer (ZFE).  如果 noise $\varepsilon$ 是 broadband noise, high frequency noise 會被放大。In the case of ML, 我們必須小心選 basis functions to ensure matrix $\left(\boldsymbol{\Phi}^{T} \boldsymbol{\Phi}\right)$ can be inverted and avoid “ill-condition”.  通常使用 sparse model with few basis functions.Method 2, EM algorithm for Bayesian Linear RegressionMethod 2 放寬 $w$ 從定值 fixed value 變成 distribution (random variable). Voila，這就是 Bayesian 精神！A Bayesian treatment of the linear model begins by assigning a prior distribution to the weights of the model. This introduces bias in the estimation but also greatly reduces its variance, which is a major problem of the ML estimate.此處我們用 common choice of independent, zero-mean, Gaussian prior distribution for the weights of the linear model:當然假設 zero-mean 聽起來有點奇怪，有可能引入 bias, 但好處是有 regularization 的效果，儘量讓 $w_m$ 不要太大。Bayesian inference 接下來是計算 posterior distribution of the hidden variable$\eqref{eqMAP}$ 分母部分進一步展開：$\eqref{eqMAP}$，posterior of the hidden variable，可以寫成：where可以證明，$\alpha, \beta$ 可以用以下的 maximum likelihood estimate.直接計算 $\eqref{eqab}$ 非常困難。除了 $\eqref{eqab}$ 微分非常複雜。$\alpha, \beta \ge 0$ 是一個 constrained optimization 問題。 EM algorithm 提供一個有效的方法解 $\alpha, \beta$ and infer $\mathbf{w}$E-step Compute the Q function三角括號是對 $p(\mathbf{w} \mid \mathbf{t} ; \alpha^{(t)}, \beta^{(t)})$ 的期望值。代入 $\eqref{eqPost}$ 得到where $\boldsymbol{\mu}^{(t)}$ and $\boldsymbol{\Sigma}^{(t)}$ are computed using the current estimates of the parameters $\alpha^{(t)}$ and $\beta^{(t)}$ :M-step Maximize $Q^{(t)}(\mathbf{t}, \mathbf{w} ; \alpha, \beta)$ with respect to $\alpha, \beta$.結果很簡單$\eqref{eqa}$ 和 $\eqref{eqb}$ 同時保證 $\alpha, \beta$ 永遠為正值。幾個重點：  EM algorithm 有可能收斂到 local minimum; initial condition 很重要  注意 $\mathbf{w}$ 不是一個值，而是 distribution.  Inference of $\mathbf{w}$ 就是 posterior distribution $\eqref{eqPost}$.  Posterior distribution 的 mean $\eqref{eqMean}$ 稱為 Bayesian linear minimum mean squire error (LMMSE) inference for $\mathbf{w}$.Method 3, Variational EM-based Bayesian Linear Regression因為非常複雜，可以直接參考 [].例二：filtering例二： Bayesian GMMwhere $\boldsymbol{\pi} = { \pi_j }$ 代表 weights or mixing coefficients.  $\boldsymbol{\mu} = { \boldsymbol{\mu}{j} }$ 是 means of Gaussian distribution.  $\mathbf{T} = { \mathbf{T}{j} }$ 是 precision (inverse covariance) matrices.  在 Bayesian GMM 我們更常用 precision matrix.Bayesian GMM 和一般 GMM 有什麼不同？ 最大的差別就是 $\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T}$ 不再是 parameters for estimation, 而是 random variables. 這有什麼好處？我們可以 impose or embedded our priors on $\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T}$, 通常是 conjugate priors (i.e. no informative priors) 1.Bayesian GMM 的 graph model 如下。Hidden random variables 包含 $h = (\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{T})$. Bayesian 的目標是找出 $p(h\mid x)$, 顯然不會有 analytic solution.因此我們 divide-and-conquer 利用 $\eqref{eqVarJ2}$假設 mean-field approximation看起來還是很複雜，不過 [@tzikasVariationalApproximation2008] 的 reference [27] 有詳細的公式。可以用“簡單” iterative update procedure 得到 optimal approximation $q(h)$ to the true posterior $p(h\mid x)$, 這就是 variational E-step.  下一步就是 variation M-step, 不贅述。Bayesian-GMM 比起 EM-GMM 到底有什麼好處。前面提到可以 impose priors. 如果沒有 prior information (i.e. use conjugate prior), 還有好處嗎？[@tzikasVariationalApproximation2008] 的說法是 Bayesian-GMM 不會有 singular solution, i.e. single data point Gaussian.  然而在 EM-GMM 常常會發生，如下圖 20 Gaussian components。一般 EM-GMM 解決的方法就是多跑幾次 randomize initial conditions to avoid it.另一個好處是可以直接用 Bayesian GMM 決定 Gaussian component number, 而不需要用其他方法 (e.g. cross-validation)。實作如下圖。(a) 初始是 20 component Gaussians; (b), (c) model evolution; (d) 最終解只剩下 5 個 Gaussian components, 其餘 15 個 Gaussian components weight 為 0。注意收斂的過程中都沒有 singularity.這聽起來比較 significant, 不過有一個 catch, 就是 Dirichlet prior 不允許 component mixing weight 為 0.  因此如果要用 Bayesian-GMM 決定 Gaussian component number, 必須 remove $\boldsymbol{\pi} = { \pi_j }$ from priors.  也就是把 $\boldsymbol{\pi} = { \pi_j }$ 視為 parameter to be estimated.Bayesian GMM 的 graph model 如下。注意此時的 $\pi$ 變成方框，代表 parameter to be estimated.  Hidden random variables 包含 $h = (\mathbf{Z}, \boldsymbol{\mu}, \mathbf{T})$.同樣經過一番計算 variational E-step and M-step (此處省略)，可以得到在 iteration 過程中，有一些 mixing coefficients ${\pi_j}$ 收斂到 0. 定性來說，variational bound 可以視為兩項之和：第一項是 likelihood function, 第二項是 prior 造成的 penalty term to penalizes complex models.            Dirichlet for $\boldsymbol{\pi}$.  Gauss-Wishart for ($\boldsymbol{\mu}, \mathbf{T})$ &#8617;      ]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> softmax </tag>
        
          <tag> EM </tag>
        
          <tag> Bayesian </tag>
        
          <tag> Variational </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[English]]></title>
      <url>/2021/08/03/English/</url>
      <content type="text"><![CDATA[語音語調和節奏語音：pronunciation (word)語調：intonation (sentence)節奏：rhymes: biggest problem!!! for ChineseIsochrone:  Chinese word is unit time!! syllable-timedstress-timed language: englishmora-timed language: japanesehttps://www.youtube.com/watch?v=VMDhdaMkeBU]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Math AI - Maximum Likelihood Estimation Evolve To EM Algorithm For Incomplete/Hidden Data To Variational Bayesian Inference]]></title>
      <url>/ai/2021/06/30/MLE_to_EM/</url>
      <content type="text"><![CDATA[Major Reference  [@poczosCllusteringEM2015]  [@matasExpectationMaximization2018] good reference  [@choyExpectationMaximization2017]  [@tzikasVariationalApproximation2008] excellent introductory paperMaximum Likelihood Estimation (MLE) 和應用Maximum likelihood estimation (MLE) 最大概似估計是一種估計模型參數的方法。適用時機在於手邊有模型，但是模型參數有無限多種，透過真實觀察到的樣本資訊，想辦法導出最有可能產生這些樣本結果的模型參數，也就是挑選使其概似性(Likelihood)最高的一組模型參數，這系列找參數的過程稱為最大概似估計法。Bernoulli distribution：投擲硬幣正面的機率 $\theta$, 反面的機率 $1-\theta$. 連續投擲的正面/反面的次數分別是 H/T.  Likelihood function 為MLE 在無限個 $\theta$ 中，找到一個使概似性最大的 $\theta$, i.e. $\widehat{\theta}_{\mathrm{MLE}} =\arg \max _{\theta} {\theta^{H}(1-\theta)^{T}}$只要 likelihood function 一次微分，可以得到就是平均值，推導出來的模型參數符合直覺。Normal distribution： 假設 mean unknown, variance known, 我們可以用 maximum log-likelihood function微分的結果告訴我們，樣本的平均值，其實就是母體平均值 $\mu$ 最好的估計！又是一個相當符合直覺的答案，似乎 MLE 只是用來驗證直覺的工具。這是一個錯覺，常見的 distribution (e.g. Bernoulli, normal distribution) 都是 exponential families.  可以證明 maximum log-likelihood functions of exponential families 都是 concave function, 沒有 local minimum. 非常容易用數值方法找到最佳解，而且大多有 analytical solution.但只要 distribution function 更複雜一點，例如兩個 normal/Gaussian distribution weighted sum to 1, MLE 就非常難解。稱為 Gaussian mixture model (GMM) with 2 groups, GMM(2).另一種情況：MLE 雖然直接明瞭，但現實常常會遇到 missing data 或是 hidden data/state (state 也視為 data). 此時就需要 Expectation Maximization (EM) algorithm.例如 GMM(2) 可以視為有一個 hidden state $z$ with binary value, $p(x) = p(x\mid z=0) p(z=0) + p(x\mid z=1) p(z=1)$. $p(x\mid z=0)$ 和 $p(x\mid z=1)$ 分別是不同 normal distributions.以下先 Q&amp;A maximum likelihood estimation (MLE) vs. expectation maximization (EM) 兩種算法。其實是視 EM 為 MLE 的推廣。 接著用四個簡單例子 (toy example) 說明 MLE 如何推廣到 EM.Q&amp;A of MLE Versus EMQ: Why EM is a special case of MLE?  If the problem can be formulated as MLE parameter estimation of incomplete/hidden data.  Then EM algorithm 的 E-step is guessing incomplete/hidden data; M-step 就對應 MLE parameter estimation with modification (見本文後段)。  EM M-Step is essentially a MLE parameter estimation with modification.  EM can be seen as an iterative MLE.  EM may converge at local minimum during iteration.Q: How EM can be used for to parameter estimation and incomplete/hidden data estimation?  For Bayesian, 兩者可以視為同一類。Unknown parameters 亦可以視為 missing data with distribution.  此時 EM algorithm 相當于 2D coordinate descent (energy) optimization [@wikiCoordinateDescent2021], different from gradient descent.  EM 的 E-step 對應 (conditional) distribution coordinate descent; M-step 對應 parameter coordinate descent.  For Frequentist (古典統計), E-step is guessing incomplete/hidden data; M-step 就對應 MLE parameter estimation.Toy Example [@matasExpectationMaximization2018]前提摘要一個簡單例子觀察 temperature and amount of snow (溫度和雪量, both are binary input) 的 joint probability depending on two “scalar factors” $a$ and $b$ as $p(t, s | a, b)$                   $s_0$      $s_1$                  $t_0$      $a$      $5a$              $t_1$      $3b$      $b$      注意 $a$ and $b$ are parameters, 不是 conditional probability.另外因為機率和為 1 做為一個 constraint: $6a + 4b = 1$例一: MLE一個 ski-center 觀察 $N$ 天的溫度和雪量得到以下的統計，$N_{ij} \in \mathbf{I}$, 如何估計 $a$ and $b$?                   $s_0$      $s_1$                  $t_0$      $N_{00}$      $N_{01}$              $t_1$      $N_{10}$      $N_{11}$      Likelihood function (就是 joint pdf of $N$ repeat experiments)where $C = (\Sigma N_{ij})! / \Pi (N_{ij}!)$ 是 MLE 無關的常數問題改成 maximum log-likelihood with constraint and $C’ = \ln C$上述方程式的解為結果很直觀。其實就是利用大數法則： $a\cdot N \sim N_{00}; 5a\cdot N\sim N_{01}; 3b\cdot N\sim N_{10}; b\cdot N\sim N_{11}$再來大數法則 (a+5a)N~N00+N01; (3b+b)N~N10+N11 =&gt; a = .. ; b = …例二 incomplete/hidden Data假設我們無法觀察到完整的”溫度和雪量“；而是“溫度或雪量”，有時“溫度”，有時“雪量”，但不是同時。對應的不是 joint pdf, 而是 marginal pdf 如下：觀察如下：The Lagrangian (log-likelihood with constraint)此時的方程式比起之前複雜的多，不一定有 close-form solution:如果用大數法則：  $6a \cdot(T_0+T_1) \sim T0; \, 4b\cdot(T_0+T_1) \sim T_1$  $(a+3b) \cdot (S_0+S_1)\sim S_0; \, (5a+b)\cdot(S_0+S_1) \sim S_1$ 注意不論 1. or 2. 都滿足 $6a+4b = 1$ constraint, 可以用來估計 $a$ and $b$.問題是我們要用那一組 $(a, b)$?  單獨用一組都會損失一些 information, 應該要 combine 1 and 2 的 information, how?思路一 平均 (a, b) from 1 and 2.  但這不是好的策略，因為平均 (a,b) 不一定滿足 constraint. 在這個 case 因為 linear constraint, 所以平均 (a,b) 仍然滿足 constraint.  但對於更複雜 constraint, 平均並非好的方法。更重要的是平均並無法代表 maximum likelihood in the above equation.  我們的目標是 maximum likelihood, 平均 (a, b) 完全無法保證會得到更好的 likelihood value!或者把 (a,b) from 1 or 2 代入上述 likelihood function 取大值。顯然這也不是最好的策略。因為一半的資訊被捨棄了。思路二 比較好的方法是想辦法用迭代法解微分後的 Lagrange multiplier 聯立方程式。 (a, b) from 1. or 2. 只作為 initial solution, 想辦法從聯立方程式找出 iterative formula.  這似乎是對的方向，問題是 Lagrange multiplier optimization 是解聯立(level 1)微分方程式。不一定有 close form as in this example.  同時也無法保證收斂。另外如何找出 iterative formula 似乎是 case-by-case, 沒有一致的方式。=&gt; iterative solution is one of the key, but NOT on Lagrange multiplier (level 1)思路三 既然是 missing data, 我們是否可以假設 $(a, b) \to$  fill missing data $\to$ update $(a, b) \to$  update missing data $\cdots$ 具體做法 $N_{00} = T_0 \cdot \frac{1}{6} + S_0 \cdot \frac{a}{a+3b}$$N_{01} = T_0 \cdot \frac{5}{6} + S_1 \cdot \frac{5a}{5a+b}$$N_{10} = T_1 \cdot \frac{3}{4} + S_0 \cdot \frac{3b}{a+3b}$$N_{11} = T_1 \cdot \frac{1}{4} + S_1 \cdot \frac{b}{5a+b}$有了 $N_{00},N_{01},N_{10},N_{11}$ 可以重新估計 $(a, b)$ using joint pdfQ: 如何證明這個方法是最佳或是對應 complete data MLE or incomplete/hidden data MLE? 甚至會收斂？EM algorithm 邏輯前提摘要GMM 特例：Estimate Means of Two Gaussian Distributions (known variance and ratio; unknown means)We measure lengths of vehicles. The observation space is two-dimensional, with $x$ capturing vehicle type (binary) and $y$ capturing length (Gaussian).$p(x, y)$  $x\in$ {car, truck},  $y \in \mathbb{R}$where $\pi_c + \pi_t = 1$例三 Complete Data (Easy case)Log-likelihood很容易用 MLE 估計 $\mu_1, \mu_2$直觀上很容易理解。如果 observations 已經分組，求 mean 只要做 sample 的平均即可。以這個例子，ratio $\pi_c, \pi_t$ 不論已知或未知，都不影響結果。例四 incomplete/hidden DataLog-likelihood不用微分也知道非常難解 MLE. 我們必須用另外的方法，就是 EM 算法。不過我們還是微分一下，得到更多的 insights.上兩式非常有物理意義。基本是 easy case 的延伸：已知分類的平均值，加上未知分類的機率平均值。一個簡單的方法是只取前面已知的部分平均，不過這不是最佳，因為丟失部分的資訊。Missing Values, EM Approach重新 summarize optimality conditions如果 $p(\text {truck} \mid y_{i}^{\bullet}, \mu_c, \mu_t)$ 和 $p(\text {car} \mid y_{i}^{\bullet}, \mu_c, \mu_t)$ 已知，上式非常容易解 $\mu_c$ and $\mu_t$。實際這是一個雞生蛋、蛋生雞的問題，因為這兩個機率又和 $\mu_c$ and $\mu_t$ 相關。EM algorithm 剛好用來打破這個迴圈。  Let $z_i \,(i=1, 2, \cdots, M), z_i \in \text{{car, truck}}$ denote the missing data.  Define $q\left(z_{i}\right)=p\left(z_{i} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)$  上述 optimality equations 可以得到EM Algorithm 可以用以下四步驟表示  Initialize $\mu_c$, $\mu_t$  Compute $q\left(z_{i}\right)=p\left(z_{i} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)$ for $i = 1, 2, \cdots, M$  Recompute $\mu_c$, $\mu_t$ according to the above equations.  If termination condition is met, finish.  Otherwise, goto 2.上述步驟 2 稱為 Expectation (E) Step, 步驟 3 稱為 Maximization (M) Step.  統稱為 EM algorithm.Q. Why Step 2 稱為 Expectation? not clear.  Maximization 比較容易理解，因為 optimality condition 就是 maximization (微分為 0).In summary, EM algorithm 的一個關鍵點是：讓 incomplete/hidden data 變成 complete (Expectation?).  有了完整的 data, 就容易用 MLE 找到 maximal likelihood estimation ($\mu_c$ and $\mu_t$ in this case).Clustering: Soft Assignment Vs. Hard Assignment (K-means)EM Algorithm DerivationEM algorithm 如果只是 heuristic algorithm, 可能有用度減半。這裡討論數學上的 justification.  先定義 terminologies  $\mathbf{o}$: all observed values  $\mathbf{z}$: all unobserved values (i.e. hidden variable)  $\mathbf{\theta}$: model parameters to be estimated目標：Find $\theta^*$ using the Maximum Likelihood approach思路：假設解下列完整 data 很容易解 (例如例一和例三)我們的想法是把 $\eqref{eqMLE}$ 先變形成上式 $\eqref{eqMLE2}$，再想辦法優化這裡引入看似任意 probability distribution $q(\mathbf{z})$ with $\sum_{\mathbf{z}} q(\mathbf{z})=1$. 後面會說明如何選 $q(\mathbf{z})$.Log-Likelihood with Hidden Variable Lower Bound上式 $\eqref{eqMLE3}$ 利用 Jensen’s inequality 可以導出我們定義 $\ln p(\mathbf{o} \mid \boldsymbol{\theta})$ 的 lower bound or ELBO (Evidence Lower BOund) 為 $\mathcal{L}(q, \boldsymbol{\theta})$, for any distribution $q$.這已經非常接近思路！我們的思路修正成把有 hidden data 的 MLE 變成用完整 data 的 MLE 做為 lower bound.  再通過 $q(\mathbf{z})$ 提高 lower bound 逼近原來的目標。Maximizing $\mathcal{L}(q, \boldsymbol{\theta})$ by choosing $q(z)$ 就可以 push the log likelihood $\ln p(\mathbf{o} \mid \boldsymbol{\theta})$ upwards.反過來我們可以計算和 lower bound 之間的 gap.這個 gap $\eqref{eqKL}$ 就是 KL divergence between $q(\mathbf{z})$ and $p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})$, 永遠大於 0. 這和 Jensen Inequality 的結論一致！以下是關鍵：  如果能找到 $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})$ 的 analytical solution，就可以讓 gap 變成 0.  Lower bound $\eqref{eqELBO}$ 就是我們要 maximize 目標，voila!          例如例四 GMM 的 $p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})$ 就是 softmax function.        即使 $q(\mathbf{z})$ 有 analytical solution, e.g. softmax, 不代表容易解 maximum 以及對應的 parameter.  EM algorithm 就是用來處理這個問題，見下文。  假如 $q(\mathbf{z})$ 本身就非常複雜，還有另一個方法：variational approximation; 稱為 Bayesian inference. 本文不討論。EM Algorithm Push the Lower Bound UpwardsLog likelihood function 可以分為兩個部分： lower bound + gap從 Jensen’s inequality 得到 $\mathcal{L}(q, \boldsymbol{\theta})$ 是 lower bound.  同樣從 KL divergence $\ge$ 0 再度驗證。如果 $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})$, the bound is tight.接下來看兩個極端的 examples.Trivial Case:  Hidden variable $\mathbf{z}$ does NOT provide any information of $\mathbf{o}$如果 $\mathbf{o}$ 和 $\mathbf{z}$ 完全無關，$p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta}) = p(\mathbf{z} \mid \boldsymbol{\theta})$.  We can make $q(\mathbf{z}) = p(\mathbf{z} \mid \boldsymbol{\theta})$such that $D_{\mathrm{KL}}(q(\mathbf{z}) | p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})) = 0$, 也就是 gap = 0.Lower bound 就變成原來的 log-likelihood function, trivial case.Case 2: 如果  $p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})$ 有 analytical solution, let $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{o}, \boldsymbol{\theta})$其實這就是 EM algorithm 的精髓EM 具體步驟Recap EM algorithm:  Gap 可以視為從 observables 推論出 unobservables, i.e. incomplete/hidden data, 對應 EM algorithm 的 E-Step.  Lower bound 其實可以視為 MLE of complete data， 對應 EM algorithm 的 M-Step.Recap lower bound $\eqref{eqELBO}$ 包含兩個部分：(i) $q(\mathbf{z})$ distribution and (ii) log-likelihood of complete data, $\ln p(\mathbf{o}, \mathbf{z} \mid \boldsymbol{\theta})$.這兩個部分剛好對應 EM algorithm 的 E-step (i) and M-step (ii).  Initialize $\boldsymbol{\theta}=\boldsymbol{\theta}^{(0)}$  E-step (Expectation):  M-step (Maximization):M-step: $q^{(t+1)}$ is fixed我們先看 M-step $\eqref{eqMstep}$, 因為這和 MLE estimate $\theta$ 非常相似。注意 M-Step 和完整 data 的 MLE 思路如下非常接近，只加了對 $q(\mathbf{z})$ 的 weighted sum.上式微分等於 0 就可以解 $\theta^{t+1}$。上面例四以及例二就是很好的例子。另一個常見的寫法注意 M-Step 是 maximize lower bound, 並不等於 maximize 不完整 data 的 MLE，因為還差了一個 gap function (i.e. KL divergence).  E-Step 的目標才是縮小 gap function, which is also $\boldsymbol{\theta}$ dependent.E-step: $\boldsymbol{\theta}^{(t)}$ is fixed以上 KL divergence 大於等於 0，所以 maximize lower bound 就要讓 要選擇 $q(z)$ 儘量縮小 gap  (i.e. KL divergence) 到 0.  Gap 等於 0 的條件就是同樣 E-Step 深具物理意義，就是猜 incomplete/hidden data distribution based on 已知的 observables 和 iterative $\theta$.例如例四 E-Step 就是計算 $q\left(z_{i}\right)=p\left(z_{i} \mid y_{i}^{\bullet}, \mu_{\mathrm{c}}, \mu_{\mathrm{t}}\right)$ for $i = 1, 2, \cdots, M$.  結果是 softmax function.Conditional Vs. Joint Distribution我們可以把 conditional distribution 改成 joint distribution 如下。兩者都可以用來解 E-Step.EM 精髓: 結合 E-Step and M-Step如果 E-Step $\eqref{eqEstep2}$ 有 analytic solution, 可以代入 M-Step $\eqref{eqMstep2}$ 得到New EM algorithm with fixed $\boldsymbol{\theta}^{t}$Free Energy Interpretation [@poczosCllusteringEM2015]搞 machine learning 很多是物理學家 (e.g. Max Welling), 習慣用物理觀念套用於 machine learning.  常見的例子是 training 的 momentum method.  另一個是 energy/entropy loss function.  此處我們看的是類似 energy loss function.我們從 gap 開始where H(q) is the entropy of q,  第一項是負的，第二項和第三項是正的。我們用一個例子來驗證q = {0 or 1} with 50% chance, =&gt; H(q) = 1 (bit) or ln (?) &gt; 0Eq(z) ln p(o, z) = -(0.5 (o-u1)^2 + 0.5 (o-u2)^2 ) / sqrt(2pi) &lt; 0此處我們 switch to [@poczosCllusteringEM2015] notation.  Observed data: $D = {x_1, \cdots, x_n}$  Unobserved/hidden variable: $z = {z_1, \cdots, z_n}$  Parameter: $\theta = [\mu_1, \cdots, \mu_K, \pi_1, \cdots, \pi_K, \Sigma_1, \cdots, \Sigma_K]$  Goal: $\boldsymbol{\theta}^{*}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ln p(D \mid \theta)$重寫上式：$F_{\theta^t} (q(\cdot), D)$ 稱為 free energy, 包含 joint distribution expectation 和 self-entropy.  或是稱為 ELBO (Evidence lower bound)如果 $p(z\mid x; \theta)$ is analytically available (e.g. GMM, this is just a softmax!).  The ELBO becomes a Q(theta, theta^old) function + H(q)The EM algorithm can be summzied as argmax Q!!  E-step: $p(z\mid x)$  M-step; argmax …We can prove  log likelihood is always increasing!          Use multiple, randomized initialization in practice.Variational Expectation MaximizationEM algorithm 一個問題是對於複雜的問題沒有 close from p(z|x), then toast!Appendix例二的 Conditional Vs. Joint Distribution 解法我們之前的 E-Step 是猜 joint distribution, $p(t, s | a, b)$.                   $s_0$      $s_1$                  $t_0$      a      5a              $t_1$      3b      b      如果用上述的 conditional distribution 可以細膩的看每一個 data.      對於所有 $(\bullet, s_0)$         對於所有 $(\bullet, s_1)$         對於所有 $(t_0, \bullet)$         對於所有 $(t_1, \bullet)$   再來是例二的 M-Step最後再把所有 dataset 的 weighted sum $(t_i, s_j)$ 統計出來，例如$S_0$ 個 $(\bullet, s_0) \to \frac{a}{a+3b}S_0$ 個 $(t_0, s_0)$ 和 $\frac{3b}{a+3b}S_0$ 個 $(t_1, s_0)$$S_1$ 個 $(\bullet, s_1) \to \frac{5a}{5a+b}S_1$ 個 $(t_0, s_1)$ 和 $\frac{b}{5a+b}S_1$ 個 $(t_1, s_1)$$T_0$ 個 $(t_0, \bullet) \to \frac{1}{6}T_0$ 個 $(t_0, s_0)$ 和 $\frac{5}{6}T_0$ 個 $(t_0, s_1)$$T_1$ 個 $(t_1, \bullet) \to \frac{3}{4}T_1$ 個 $(t_1, s_0)$ 和 $\frac{1}{4}T_1$ 個 $(t_1, s_1)$$(t_0, s_0)$ 個數 $\to N_{00} = \frac{1}{6}T_0+\frac{a}{a+3b}S_0$$(t_0, s_1)$ 個數 $\to N_{01} = \frac{5}{6}T_0+\frac{5a}{5a+b}S_1$$(t_1, s_0)$ 個數 $\to N_{10} = \frac{3}{4}T_1+\frac{3b}{a+3b}S_0$$(t_1, s_1)$ 個數 $\to N_{11} = \frac{1}{4}T_1+\frac{b}{5a+b}S_1$因此可以使用完整 data 的 MLE estimation:To Do NextGo through GMM example.下一步 go through HMM model or simplest z -&gt; o graph model.What is the mutual information of $o$ and $z$ in this case?假設可以有一個 close from Q function, e.g. GMMIn summary:  M-Step maximize the lower bound;  E-Step close the gap E-StepM-Step ]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> softmax </tag>
        
          <tag> EM </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Jekyll Memo for Github Blog]]></title>
      <url>/language/2021/06/30/Jekyll-Memo/</url>
      <content type="text"><![CDATA[幾個重點Header      title line:  no other :,  wrong example:  title: Math AI : xxx =&gt; the second : to be removed!        tags: [xxx, xxx, xxx]  Table  目前 Jekyll + Next theme 造成 table column width 非常寬。 I don’t know the exact reason.  I changed the xxx/xxx.github.io/_sass/_common/scaffolding/tables.scss          width: 300px;      table-layout: auto;      Equation      $$ math equation $$ =&gt; leave empty lines “before” and “after” $$ $$! 也就是上下各要空一行！        ${{ }}$  =&gt; ${ \{ \}}$.  如果要打 {, 一定要加 \{.        Equation number:  必須先加上 header 如下。Reference: https://jdhao.github.io/2018/01/25/hexo-mathjax-equation-number/  &lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config({  TeX: { equationNumbers: { autoNumber: "AMS" } }});&lt;/script&gt;Equation 本體$$\begin{equation}E=mc^2\end{equation}\label{eq1}$$Equation citation use $\eqref{eq1}$Image  resize image 似乎有問題，需要另外的 plug-in]]></content>
      <categories>
        
          <category> Language </category>
        
      </categories>
      <tags>
        
          <tag> Jekyll </tag>
        
          <tag> Github </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Edge AI- Object Detection History]]></title>
      <url>/ai/2021/02/16/Typora-Mermaid/</url>
      <content type="text"><![CDATA[Edge AI: Object Detection History: 2-Pass Vs. 1-Pass; Anchor Vs. Anchor-lesssequenceDiagram    participant Alice    participant Bob    Alice-&gt;John: Hello John, how are you?    loop Healthcheck        John-&gt;John: Fight against hypochondria    end    Note right of John: Rational thoughts &lt;br/&gt;prevail...    John--&gt;Alice: Great!    John-&gt;Bob: How about you?    Bob--&gt;John: Jolly good!graph LRA[方形] --&gt;B(圆角)    B --&gt; C{条件a}    C --&gt;|a=1| D[结果1]    C --&gt;|a=2| E[结果2]    F[横向流程图]]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> softmax </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Math ML - Modified Softmax w/ Margin]]></title>
      <url>/ai/2021/01/16/softmax/</url>
      <content type="text"><![CDATA[Math ML - Modified Softmax w/ Margin[@rashadAdditiveMargin2020] and [@liuLargeMarginSoftmax2017]Softmax classification 是陳年技術，可還是有人在老幹上長出新枝。其中一類是在 softmax 加上 maximum margin 概念 (sometimes refers to metric learning), 另一類是在 softmax 所有 dataset 中找出 “supporting vectors” 減少 computation 卻不失準確率。實際做法都是從修改 loss function 著手。本文聚焦在第一類增加 margin 的 算法。Softmax in DL or ML RecapSoftmax 最常用於 DL (i.e. deep layers) 神經網絡最後一層(幾層)的 multi-class classification 如下圖。and  Input vector, $\mathbf{x}$, dimension $n\times 1$.  Weight matrix, $\mathbf[w_1’, w_2’, .., w_K’]’$, dimension $K\times n$  Output vector, $\mathbf{z}$, dimension $K\times 1$.  Softmax output vector, $0\le\sigma(j)\le 1, j=[1:K]$, dimension $K\times 1$.  注意 bias 如果是一個 fixed number, $b$, softmax 分子分母會抵銷。bias 如果不同 $b_1, b_2, …, b_n$，可以擴展 $\mathbf{x’ = [x, }1]$ and $\mathbf{w’_j = [w_j}, b_j]$, 同樣如前適用。Softmax 也常用於 ML (i.e. shallow layers) 的 multi-class classification, 常和 SVM 一起比較。為了處理 nonlinear dataset or decision boundary, Softmax + kernel method 是一個選項。Softmax 另外用於 attention network, TBD.Parameter Notation and Range for ML and DL  $N$: number of data points.  100 to 10,000 for ML, &gt; 1M for DL.  $n$: input vector dimension. maybe from 1~ to 100~ for ML, 1000-4000 for DL.  $K$ or $m$ or $C$: output vector dimension, number of classes, maybe from 1 (binary) to 100 (Imaginet)  $k$: kernel feature space dimension, maybe from 10s’ - $\infty$ for ML.  Usually not use for DL.Summarize the result in table.                   N      n      k      K                  ML      100-10,000      1s’- 100s’      10s’- $\infty$      1s’-10s’              DL      &gt; 1M      1000-4000      NA      10-100      Softmax w/ Margin Via Training根據前文討論，$w_i$ vectors 代表和 class i data 的相似性。普通的 softmax classification 如下圖左所示。Decision boundary 是 data point 和 $w_1$ and $w_2$ 的機率一樣。因為 softmax (or logistic regression) 只要求 $\sigma_1(x) &gt; \sigma_2(x)$ or vice versa to classify $x \in$ class 1 (or class 2).  這裡完全沒有 margin 的觀念。推廣到 multiple class 更是如此，如下圖。因為是取 $\sigma(j)$ 的最大值。除了 $\sigma(j) &gt; 0.5$ 有明顯的歸類。但在三不管地帶，很可能雜錯在一起。因爲 training 是基於 loss function, 解法是在 loss function 加入 margin term 做為 driving force (check the back-prop gradient!), 讓 training process 竭盡所能 “擠出” margin, 如上圖右。如何在 softmax 加入 margin for trainingSVM 是從 decision boundary 的平行線距離著手（margin = 1/|w|, minimize |w| ~ maximum margin)。本文討論 Softmax 加上 margin 有三種方式，都是從角度 $\theta$ 著手，概念如圖二右 (平面角度)，或是下圖右 (球面角度)。maximize $\theta$ 剛好和 minimize |w| 正交 (orthogonal). 這是巧合嗎？我們先看 Softmax 的 loss function 如下圖。先是 softmax function, inference/test 只要 再來通過 cross-entropy loss.  Cross-entropy loss 對應 log likelihood. where $f_{y_{i}}=\boldsymbol{W}{y{i}}^{T} \boldsymbol{x}{i}$ 代表 data $x_i$ 和 $W{y_i}$ 的相似性。三種用角度增加 SoftMax inter-class margin  L-Softmax (Large Margin Softmax) [@liuLargeMarginSoftmax2017]  A-Softmax (Angular Softmax) [@liuSphereFaceDeep2018]  AM-Softmax (Additive Margin Softmax)L-Softmax (Large Margin Softmax): $\cos \theta \to \cos (m\theta)$因為 $f_{j}=\left| \boldsymbol{W_j} \right|\left| \boldsymbol{x_i} \right|\cos\left(\theta_{j}\right)$.  如何在 $x_i$ 和 $W_j$ 加上 margin？  一個方法就是把 $\cos \theta$ 改成 $\cos m\theta$, why?從相似性來看，$\cos(m\theta)$ 在同樣的角度”相似性”掉的比較快。因此在 training 時會強迫把同一 feature 的 data 擠壓在一起, reduce the intra-class distance. 達到增加 inter-class margin 的目的。另外可以從 decision boundary 理解。Softmax 的 decision boundary,$x\in$ Class 1:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right)&gt;\left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$$x\in$ Class 2:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right) &lt; \left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$and $\theta_1 + \theta_2 = \theta$ which is the angle between $W_1$ and $W_2$如果把 $\cos \theta \to \cos (m\theta)$,$x\in$ Class 1:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( m\theta_{1}\right)&gt;\left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$. Assuming $|W_1| = |W_2| \to \theta_1 &lt; \theta_2/m$, 因為 $\cos\theta$ 是遞減函數。$x\in$ Class 2:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right) &lt; \left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(m\theta_{2}\right)$.Assuming $|W_1| = |W_2| \to \theta_1/m &gt; \theta_2$.此時我們有兩個 decision boundaries, 兩個 boundaries 之間可以視為 decision margin, 如下圖。In summary, 就是在 labelled $c$ class 的 data 時，就把對應的 $\cos\theta_c$ 改成 $\cos (m\theta_c)$. $m$ 愈大，margin 就愈大。但過之猶如不及，如果 $m$ 太大，可能無法正確 capture features (TBC)? $m$ 應該有一個 optimal value.為什麼會有 $D(\theta)$？ 原因是要維持 $\psi(\theta)$ 的遞減性，連續性，和可微分性 over $[0, \pi]$.  一旦定義出 $\psi(\theta)$ over $[0, \pi]$. 左右 flip (y 軸對稱) 得到 $\theta\in[-\pi, 0]$. 其他的 $\theta$ 都可以移到 $[-\pi, \pi]$.舉一個例子如下式，$\psi(\theta)$ 的 curve 如下圖。A-Softmax (Angular Softmax): $\cos \theta \to \cos (m\theta)$ and $|W|=1$在 L-Softmax 可以同時調整 $|W|$ and $\theta$, 在 A-Softmax 進一步限制 $|W|=1$, 其他都和 L-Softmax 相同。A-Soft 的 Loss function 如下， 後來有再修正 $\psi(\theta)$, 多加一個 hyper-parameter $\lambda$, angle similarity curve 如下圖。注意 A-Softmax 的 $\psi(0)=1.$因為 $|W|=1$, A-Softmax 一個用途是 hyper-sphere explanation 如下圖。理論上 L-Softmax 包含 A-Softmax, 但在某一些情況下，A-Softmax 似乎效果更好，less is more? (同一作者，2017 L-SoftMax; 2018 A-Softmax).AM-Softmax (Additive Margin Softmax): $\cos \theta \to \cos \theta -m$AM-Softmax 非常有趣，它把 $\cos\theta \to \cos(m\theta) \to \cos\theta -m$, 也就是，AM-Softmax 的 loss function, 但多了一個 hyper-parameter $s$(?)這有很多好處：  不用再分段算 $\psi(\theta)$, forward and backward 計算變成很容易。  $m$ 是 continuous variable, 不是 discrete variable in A-Softmax. $m$ 可以 fine-grain optimized hyper-parameter. 而且是 differentiable, 我認為可以是 trainable variable.  AM-Softmax 同時 push angle and magnitude?Q&amp;AQ. Data 不是固定的嗎？為什麼會隨 loss function 改變？A. 此處是假設 CNN network 的最後一層是 Softmax, 因此 input data 對應的 feature extraction 並非固定而且會隨 loss function 改變如下圖。如果 input data 直接進入 Softmax with or without margin, the input data 顯然不會改變，但是 decision boundary may change? (next Q)Q. 在 inference/test 時，以上的公式 (check class $c$) 加起來不等於 1？ 如何解決？A: 以上的公式只用於 training 增加 margin? 在 inference/test 時，仍然用原來的 softmax 公式，因此機率仍然為 1.Q. 以上 $cos(m \theta)$ 的 $m$ 一定要整數嗎？A. 整數可以定義 continuous and differentiable loss function in $0-\pi$ 角度。上上圖的角度顯示 $0-\pi/2$ 角度，$\pi/2 - \pi$ 是 $0-\pi/2$ 的左右 flip curve.  如果 $m$ 不是整數，在 $\pi/2$ is non-differentiable.  另外也讓 loss function 的分段比較麻煩。不過我認為這都不是什麼問題。重點是 $m$ 不是整數有沒有用？ 我認為有用，可以視為另一個 hyper-parameter, or trainable parameter for optimization!  $m$ 太小沒有 margin, $m$ 太大會 filter out some features (under-fit)?策略：同時使用角度 maximize $\theta$ and Magnitude minimize $|w|$！Magnitude margin: 增加 inter-class margin?Angle margin: compress intra-class?先 push 角度，再 push w, 再角度, ….角度 m, make it differentiable!To Do  check the SVM, check the logistic regression, check import vector  Use binary classification as an example  Pro and Con of the three types.  Most importantly, try to use both amplitude and angle for learning!!  TBDReferenceLiu, Weiyang, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and LeSong. 2018. “SphereFace: Deep Hypersphere Embedding for FaceRecognition.” January 29, 2018. http://arxiv.org/abs/1704.08063.Liu, Weiyang, Yandong Wen, Zhiding Yu, and Meng Yang. 2017.“Large-Margin Softmax Loss for Convolutional Neural Networks.” November17, 2017. http://arxiv.org/abs/1612.02295.Rashad, Fathy. n.d. “Additive Margin Softmax Loss (AM-Softmax).” Medium.Accessed December 27, 2020.https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b.Wang, Feng, Weiyang Liu, Haijun Liu, and Jian Cheng. 2018. “AdditiveMargin Softmax for Face Verification.” May 30, 2018.https://doi.org/10.1109/LSP.2018.2822810.]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> softmax </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Math AI - G-CNN (Group + CNN)]]></title>
      <url>/ai/2020/05/08/G-CNN/</url>
      <content type="text"><![CDATA[Math AI - G-CNN (Group + CNN)Where is group theory (G-CNN) + Curved Space (Spherical CNN)      Manifold learning 是機器學習的分支，屬於淺層學習 (shallow learning).  Manifold learning 的技巧 (kernel PCA?, Laplacian Eigenmap, etc.) 是否能用於深度學習？ Yes, via kernel!   PCA =&gt; CNN kernel;  LE etc. =&gt; geometric kernel?    Why 結合深度學習和 manifold learning?          深度學習 based on CNN kernel =&gt; translation covariant (not invariant, invariant 是指純量 independent of coordinate system, e.g. Lagrangian, action, or $ds^2$.  Covariant means coordinate …) on 2D Euclidean plane,  Need based on ??? kernel  =&gt; translation/rotation covariant on manifold  =&gt; 結合深度學習和 manifold learning      可以減少 training set!  因為 manifold learning 自帶 translation/rotation covariant, 甚至可以 extend to manifold deformation (e.g.姿體移動?)  可以結合 prior information? (姿體移動，蛋白質移動,旋轉,鏡像 …)      Can this resist adversarial attack?        translation equivariant - CNN, plus rotation/mirror equivariant - g-CNN  then sphere equivariant - sphere CNN (non-flat); finally ??  How about scale invariant or equivariant?終於了解 G-CNN 的意義，就是把 kernel 2D convolution (Z2 commutative group) expand to a 4D G-convolution (p4m: Di4 non-commutative group).  只有 input image 是 (x, y) base, 經過 layer-1 G-convolution 轉為 p4m g space.  所有之後的 layers’ convolution 都是在 g space 做, i.e. input and output activation 都是在 {4D g space + 1D Depth=5D} space instead of {2D (x,y) + 1D depth = 3D} space.  到了最後 fully connected 再變成分類網路。  這真是 particle physicist 才會有的高維思維！一般人還是習慣每一層 input output activation 老老實實在 2D (x,y) space.  (example: https://arxiv.org/pdf/1807.11156.pdf).  I like this idea: Go high dimension all the way!  In some sense, channel or depth dimension 也是一個人造的 dimension!  More parameters?  Should be.  Still can find the (x,y) for location?  Yes, it is a superset!  Use 1D convolution with mirror group as an example.  How about broken symmetry? 或是 miss some kernel?再推廣 Group Equivariance [@estevesPOLARTRANSFORMER2018]Equivariant representations are highly sought after as they encode both class and deformation information in a predictable way. Let $G$ be a transformation group and $L_g I$ be the group action applied to an image $I$. A mapping $\Phi : E \to F$ is said to be equivariant to the group action $L_g$, $g \in G$ ifwhere $L_g$ and $L’g$ correspond to application of $g$ to $E$ and $F$ respectively and satisfy $L{gh} = L_g L_h$ and $L’_{gh} = L’_g L’_h$.  Invariance is a special case of equivariance where $L’_g = I$.  Another special case is $L_g = L’_g$.  Image classification and CNN, $g \in G$ can be thought of as an image deformation and $\Phi$ a mapping from the image to a feature map.Next step:  Image $I$ is a function of coordinate, x, $I = f(x)$ at first layer.  Group operation on f(x) is  $L_g f(x) = f(g^{-1}x)$.  原因很簡單，就是在 $x = gx’$ 會得到原來的 $f(x’)$.  2D discrete convolution, $L_g f(x) = f\circ \phi $ 定義如下。$x, y \in Z^2$  CNN 3D convolution, $L_g f(x) = f\circ \phi $ 定義如下。$x, y \in Z^2$; $k$ and $i$ 分別代表 input/output channel depth  推廣到 2D group convolution.  $g, h \in G$  推廣到 3D Group CNN or G-CNN.  $g, h \in G$, $k$ and $i$ 分別代表 input/output channel depthConvolution and CNN 具有 translational equivariance and independent of kernel $\phi$.  直觀而言，就是把 input image (or feature map) 和 kernel filter 的 symmetry group (e.g. translation, rotation, reflection) 做 similarity (inner product), 但保留印記 (coordinate (x,y), reflection (m=1, -1), rotation (r=0, 1, 2, 3)) 到 output feature map.      2D convolution or 3D CNN 的 $g^{-1} h = y-x$  and $h^{-1} g = (g^{-1} h)^{-1} = x-y$ 是 $Z^2$ 反元素。    其中 layer1 的 input image 因為只有 2D coordinate (x,y) + 1D depth (c=3, e.g. RGB) = 3D tensor, 但是 output feature map 變成 2D coordinate (x,y) + 1D reflection(m) + 1D rotation(r) + 1D depth (c) = 5D tensor.  其他 layers 的 input and output 都是 5D tensors.Group Equivariant Operation參考 [@prismGroupEquivariant2019] and [@cohenGroupEquivariant2019].在這篇文章中，作者以初學者的角度，從最基本的概念開始，解釋對稱性並通俗地引入群論的理論框架。所謂對稱性，就是目標在經歷一定變換以後保持不變的性質。而這裡用到的對稱性群（symmetry group），可理解為一系列滿足某些限制條件的對稱性變換的集合。下面是文中對對稱性群的定義：而在卷積網絡裡面涉及到的，最簡單的例子就是二維整數平移操作所組成的群 $\mathbb{Z}^2$。接下來，我們簡單回顧一下傳統卷積網絡的等變（Equivariance）性質。平移等變性質是CNN對目標的響應能夠不受目標在圖像中的位置影響的基礎。《深度學習》花書裡面是這樣描述等變性質：  如果一個函數滿足，輸入改變而輸出也以同樣的方式改變的這一性質，我們就說它是等變的。簡單的例子，就是當目標出現在輸入圖片中的不同位置，輸出的feature map應該是只是進行了平移變換。而從數學上，從算符対易性的角度，等變性質可以這樣定義：對於群對稱 $g \in G$ ，其算符 $L_g$ 和函數 $f(x)$，有 $f(L_g x) = L_g(f(x))$ ，也就是 $f$ 與 $L_g$ 対易，則稱$f(x)$ 對於變換 $g$ 有等變性。在深度學習當中，我們更希望卷積網絡具有等變性，而不是不變性（Invariance）:在畢卡索的這幅畫中，臉部五官都在，但是顯然被解構和調整。如果神經網絡對目標的響應具有「不變性」，顯然仍然會認為這就是一張普通人臉。接下來作者引入一個結論：這個公式的含義是：要得到經過 [公式] 變換的feature map [公式] 在 [公式] 處的值，可以通過計算在 [公式] 位置上面 [公式] 的值。舉例來說，如果 [公式] 是平移操作t，則 [公式] ，那我們只需計算在 [公式] 這一點feature的值便可得到。這個公式將在推到等變性的時候用到。對於傳統卷積網絡， [公式] 則對應平移操作 [公式] 。也就是說，由於平移操作雖然會對卷積操作的輸出產生改變，但是這種改變是線性的，可以預測的。反之，不等變的操作則會對輸出帶來非線性的影響。為了證明傳統卷積網絡裡面，平移與卷積操作対易，首先明確定義傳統卷積操作和互相關操作：在這裡，filter對輸入層的滑動掃描被看做對其平移操作。需要注意的是在傳統的卷積網絡裡面，前向過程事實上用的是互相關操作卻被泛泛稱為「卷積操作」。然後文章中很容易證明瞭互相關操作( [公式] )和卷積（ [公式] ）操作都與平移操作 [公式] 対易（commute）:[公式][公式]由這兩個操作対易，從而得出結論：卷積是平移操作的等變映射。另外一方面，作者發現旋轉操作與卷積操作是不対易的，「correlation is not an equivariant map for the rotation group」，但是feature map的堆疊卻可能是等變的。也正是因為旋轉操作不是卷積的等變映射，往傳統的CNN裡面輸入旋轉了的圖像，圖像識別的效果則會大打折扣。為瞭解決這個問題，最傳統直接的方法是數據增強，直接把圖像旋轉再輸入網絡進行訓練，但是這種方法顯然不是最優的。為了改進網絡本身來解決這個問題，考慮一個簡單的具有四重旋轉對稱軸的對稱性群 [公式] (wiki). 對於這個群，有四種對稱性操作：平移，旋轉90°，旋轉180°，旋轉270°。我們要設計一個新的CNN結構，使得當輸入圖像有以上變換時，網絡仍然具有等變性質。為了這個目的，仿照(2)(3)，根據(1)的結論，作者提出的 G-correlation，其定義為：對於第一層G-CNN（first-layer G-correlation）， [公式] 和[公式] 定義在平面 [公式] 上：[公式]對於接下來的G-CNN層（full G-correlation）， [公式] 和[公式] 定義在群G上：[公式]由此帶來的改變是，作者很容易證明瞭G-CNN對於群G的變換操作是等變的（「G-correlation is an equivariant map for the translation group」）: [公式]詳細推導見文章。值得注意的是， 經師弟提醒，對第一層G-CNN的等變性推到，需要把 [公式] 和 [公式] 拓展到群 [公式] 上，否則將無法推導（因為 [公式] 顯然不再屬於群 [公式] ）。也就是說，G-CNN推廣了對feature map的變換操作，從傳統的只有平移變換的群 [公式] 到某個對稱性群 [公式] 。而且推廣以後，G-CNN卷積層對於該群的對稱性變換操作具有等變性質。雖然作者在文中沒有提及，不難看到，G-CNN可以自然退化到傳統的CNN。當對稱性群G只有平移 [公式] 一種對稱性操作，也就是 [公式] 時，則G-CNN也就是傳統的CNN。總而言之，當輸入圖像是按照特定角度旋轉的，G-CNN網絡的輸出結果應該是按照預定規律變化的。因此，G-CNN具備了更強的旋轉輸入圖像特徵提取的能力。可以完全從 math 角度來看深度學習。CNN 的核心是 convolution, math 抽象來看是 Euclidean translation invariance (Z^2).  更進一步的是 Euclidean rotation invariance (U(1), SO(2) group?).  或者 manifold (sphere) translation/rotation invariance.Gauge Convolutional Networks[@xinzhiyuanGeometricalDeep2020] and [@pavlusIdeaPhysics2020]https://kknews.cc/tech/gpkgx3e.htmlMath Formulation前面說的都是描述性的語言，再來是干貨。先澄清一些無關的 ideas.Symmetric Group Group theory 中的 symmetric group 有明確的定義，就是 n symbol 所有 permutation (i.e. self bijections) 形成的 group, 稱為 $S_n$, with $n!$ group element. 下圖是 $S_4$ 的 Cayley graph, total 4! = 24 elements. 所有的 finite group 可以證明都是某個 symmetric group 的子群。不過這裡的 symmetric group 和本文無關。Symmetry Brings Conservation (Noether Theorem) (check 廣義相對論 article)A physic law is invariant of different observer.  For example the Lagrangian is invariant (or covariant?) under certain coordinate transformation (different observers).  We called these coordinate transformation as symmetry operation.  These symmetry operation corresponds to a specific conservation law.再來進入主題。Equivariance Math Formulation of Neural Network$y = \Phi(x)$ where $\Phi$ represents (part of) the neural network. $y$ is network output feature tensor; x is input image tensor.  Tensor can be viewed as a high dimension matrix.$\Phi$ 可以是一個複雜的 cascaded nonlinear function (with CNN, ReLU, Pooling, etc.) or a simple linear function with tensor input and tensor output.$\Phi$ 可以是 injective/bijective or non-injective.  例如，input image tensor 是 WxHxCin, 如果 output tensor 是 WxHxCout (stride=1) and Cout $\ge$ Cin, 一般是 injective or bijective.  如果 stride &gt; 1 or Cout &lt; Cin, 則是 non-injective, 也就是存在 $x’\ne x$, and $\Phi(x’) = \Phi(x)$.$x’ = T x$ where $T$ is a linear transformation (a multi-dimension matrix) corresponding to a new observer (coordinate).  此處 T 是 bijective transform, or full rank transformation, 例如 translation, rotation, affine transformation.The new observer obtains the new output feature tensor $y’ = \Phi(x’) = \Phi(T x)$Our goal is to explore the relationship between $\Phi(T x)$ and $\Phi(x)$.In general, $\Phi(T x)$ 和 $\Phi(x)$ 可能有各種不同的關係。  如果 $\Phi(T x) = \Phi(x) \; \forall x$, 滿足的所有 $T$ 稱為 $\Phi$ 的 invariant group.          Ex. $\Phi$ is norm of x, $T_g$ 是所有 metric-perserve transformation (translation, rotation, mirror, etc.)      It losses all T information, all completely independent of coordinate.      Usually for scalar.        如果 $\Phi(T x) = T \Phi(x) \; \forall x$, 滿足的所有 $T$ 稱為 $\Phi$ 的 equivariant group.          Keep spatial information      Equivariant Group: $\Phi$ is Linear and Bijective (full rank)If $\Phi$ is a linear network, 可視為一個 matrix $\Phi$, i.e. $\Phi(T x) = \Phi T x$.  為了簡化，假設 $\Phi$ and $T$ 是 2D matrix.現在需要找到 given $\Phi$, 什麼 $T$ 可以得到 $\Phi(T x) = \Phi T x = T \Phi x = T \Phi(x)$ for $\forall x$$\Rightarrow \Phi T = T \Phi$, 也就是 $\Phi$ and $T$ commute, 因此變成 commuting matrices problem, 可以參考 [@wikiCommutingMatrices2019].One sufficient (not a necessary) condition: $\Phi$ and $T$ are simultaneously diagonalizable, i.e. $\Phi = P^{-1} D P$ and $T = P^{-1} Q P$ where $D$ and $Q$ 都是 diagonal matrix. $\Phi T = P^{-1} D Q P = P^{-1} Q D P = T \Phi$也就是只要 $T = P^{-1} Q P$ where P comes from eigenvectors of $\Phi$,  $\Phi T x = T \Phi x \; \forall x$.  Commuting matrices preserve each other’s eigenspaces.這樣的 $T$ form a commuting (Abelian) group $T_g$ (assuming T is full rank, exclude 0 in the eigenvalues of T and Q), 因為 $T_1  T_2 = P^{-1} Q_1 P P^{-1} Q_2 P = P^{-1} (Q_1 Q_2) P = P^{-1} (Q_2 Q_1) P = T_2 T_1 = T_3$ (multiplication closure and commuting), 並且每一個 $T$ 都存在唯一的反元素 $P^{-1} Q^{-1} P$ (inverse closure).In summary, given a linear and bijective network $\Phi$, 可以定義一個 equivarient commutative group $T_g$ such that $\Phi(T x) = T \Phi(x) \; \forall x$.  這個群的元素(matrix) 的 eigenvectors 都和 $\Phi$ eigenvectors 一致。  也可以把 $\Phi$ 視為這個 group, $T_g$ 的一個 element.Simple $\Phi$: 2D Matrix Equivariant Group ExampleEx1: $\Phi$ = [1, 0; 0, 2]  $\Rightarrow T_g =[k_1, 0; 0, k_2]$. 所有 unequal scaling 都是 equivariant group.Ex2: $\Phi$ = [2, 1; 1, 2]  $\Rightarrow T_g =[c, s; s, c]$.  所有 hyperbolic rotation 都是 equivariant group (with a normalization constant).Ex3: $\Phi$ = [2, -1; 1, 2]  $\Rightarrow T_g =[c, -s; s, c]$.  所有 rotation 都是 equivariant group (with a normalization constant).Ex4: Horizontal shear 也是一個 equivariant group.Proof: $[1, k_1; 0, 1] \times [1, k_2; 0, 1] = [1, k_1+k_2; 0, 1] \to$ multiplication closure and 反元素是 $[1, -k; 0, 1] \to$ inverse closure.Ex5: Uniform scaling 也是一個 (trivial) equivariant group.下圖摘自 [@wikiEigenvaluesEigenvectors2020].Discrete Convolution: [@wikiToeplitzMatrix2020]Discrete convolution (離散卷積) 廣泛用於數位訊號處理和深度學習 for audio and video.  Discrete convolution 基本是 linear bijective operation, 同樣適用 equivariant group 的結論。我們用 1D discrete convolution 為例如下：$y = h * x = \Phi x$ where $\Phi$ is a $n\times n$ matrix, 就是把 m-tap kernel filter $[h_1, h_2, …, h_m]$ shift (平移) n 次造出的 matrix, 稱為 Toeplitz matrix. 一般 n » m, 因此是 “band matrix” with high sparsity. 後面會看到 $\Phi$ 的 equivariant group $T_g$ 和這個操作直接相關。下一步是要找出 $\Phi$ 的 eigenvectors 以及構成的 commutative group. 可以參考 [@grayToeplitzCirculant1971], excel article about Toeplitz matrix.有一個 “trick” 就是用 Circulant matrix 取代 Toeplitz matrix by using cyclic shift to replace regular shift!  因為 n » m, 實務上Toeplitz 和 Circulant matrix 得到的 $y$ 差異很小。但 Circulant matrix 好求解而且具有物理意義。Follow [@grayToeplitzCirculant1971] 的 notation on p.31, 我們用 $C$ 代替 $\Phi$.A $n\times n$ circulant matrix $C$ has the form Circulant matrix eigenvalues and eigenvectorsThe eigenvalues $\psi_m$ and the eigenvectors $y^{(m)}$ are the solution of我們引入一個 variable $\rho$, which is one of the n distinct complex root of unity ($\rho_m = e^{-2\pi i m/n}$, $m = 0, … n-1$), we have the eigenvalue and eigenvector and 帶入 $\rho_m$, we have eigenvalue $(m = 0, … n)$!!注意：$\psi_{m}$ is the DFT of $c_k$, i.e. $\psi = DFT(c)$.  反之，$c = IDFT(\psi)$$\psi_{m}$ 對應的 (column) eigenvector 檢查幾個 eigenvalue. First, $m=0$ is the DC component of $c_k$對應的 (column) eigenvector帶入驗證  $ C y^{(0)} = \psi_{0} y^{(0)}  $.Next $m=1$ is the 1st fundamental component of $c_k$對應的 (column) eigenvector Next $m=2$ is the 2nd fundamental component of $c_k$對應的 (column) eigenvector 可以驗證  $ C y^{(m)} = \psi_{m} y^{(m)}  $.我們用 one equation to summarize the results. 其實就是 $C$ 的 eigenvalue decomposition 如下。$\Psi$ 是 diagonal matrix with eigenvalues, 剛好就是 $C$ matrix 第一列 (row 1) 的 DFT 結果。  wherewith $\omega = e^{-2 \pi i / n}$ and $\bar{\omega} = \omega^{*} = e^{+2 \pi i / n}$Complex conjugate frequency sequence另一種的順序是 complex conjugate (Nyquist) frequency sequence, 就是 [DC, +f, -f, +2f, -2f, …, AC]  如果 n 是偶數，AC = [1, -1, 1, -1…].  如果 n 是奇數，….Equivariant: $\Phi$ is Circulant Matrix for Discrete ConvolutionGiven $\Phi = C$, a circulant matrix, 現在需要找到 equivariant group $T$ to make $\Phi(T x) = \Phi T x = T \Phi x = T \Phi(x)$.  答案是  $T_g= U Q U^{}$ where $U$ and $U^{}$ 就是以上的 matrices (n 點分圓函數) and $Q$ is a diagonal matrix.注意 $U$ and $U^{}$ 是 complex matrix, Q in general 也是 complex matrix.  但實際應用會限制 $T_g = U Q U^{}$ 必須是 real matrix.  因此會要求 Q matrix 滿足一些特性。因為 Q matrix 其實是另一個 circulant matrix 的 row 1 FFT 結果。In summary, circulant matrix 本身 forms a commutative group, i.e. $A B = B A = C$ (multiplication closure and commuting) is circulant matrix, $A^{-1}$ 也是 circulant matrix, 甚至 $A + B$ 也是 circulant matrix [@wikiCirculantMatrix2020].整理一下：  $y = \Phi(x) = h * x$ performs discrete convolution (i.e. 1D CNN) where $x$ and $y$ are input and output signals of n-dimension.  $h$ is the kernel filter of m dimension. 一般 n » m.  可以用 $n\times n$ Circulant matrix multiplication 近似 discrete convolution by zero padding, i.e. $y = C x$.  $C$ 是把 $h$ 放在 $C$ 的 row1, 再 cyclic right shift by 1 放在 row 2, and so on.  In summary, discrete convolution is equivalent to Circulant matrix multiplication.  $n \times n$ Circulant matrices form a commutative group, $T_g$, i.e. $\Phi(T_g x) = T_g \Phi(x)$ as long as $T_g$ is a $n\times n$ Circulant matrix.  Actually, $\Phi \in T_g$.  $T_g$ is equivariant operation.  Circulant group 的 generating element is $g$ = [0, 1, 0…, 0; 0, 0, 1, …,0,; ….; 1, 0, 0, …, 0]‘ 代表 right cyclic shift by 1; $g^2 = gg, g^k = gg..g$. Therefore, $I, g^2, g^3, ..g^{n-1}$ 構成 basis for 所有 $n\times n$ Circulant matrix.  For any Circulant matrix by $[a_0, a_1, …, a_{n-1}] = a_0 I + a_1 g^2 + …, + a_{n-1} g^{n-1}$.  也就是說，Circulant matrix can be decomposed to translation matrix superposition.  Discrete convolution is therefore translation multiplication commutable =&gt; translation equivariant, i.e. $\Phi ( T_g x) = T_g (\Phi x)$A discrete convolution example in appendix A.Equivariant: $\Phi$ is Circulant Matrix for 2D Discrete Convolution$y(t) = h(t) * x(t)$ 可以直接推廣到 2D,  $y(u, v) = h(u, v) * x(u, v)$  因為 $u$ and $v$ are independent on the Cartesian coordinate.  注意這並不代表 $y, h, x$ are $u, v$ separable.  Circulant group 是兩個 Circulant group 的 direct sum.  Generator 是 $g_u$ and $g_v$.  The DFT core is exp(-2piinu/.) exp(-2pimv/.)  How about eigenvalue and eigenvectors?How about other equivariant for 1D signal processing?Mirror, scale equivariant?  $\Phi(x)$ condition of Q?Use Cohen’s paper notation and concept以上的推導太狹隘，接下來採用 Cohen’s paper notation and ideas.The group $p4m$ (non-commutative group)以上是 2D Cartesian coordinate (+1D depth) generates to 4D symmetry G space (+1D depth).  分為兩種 case: (1) input 仍然是 3D tensor, but output is converted to 5D tensor.  僅用於神經網絡的第一層。之後就轉換成 (2) both input/output 都是 5D tensors.  原文有簡化版 p4 (no mirror reflection) and 2D translation only.此處考慮更簡單的 case, 1D translation and 1D translation + mirror reflection.Next step:  Function f(x)  Group operation on f(x) is  $L_g f(x) = f(g^{-1}x)$.  原因很簡單，就是在 $x‘=gx$ 會得到原來的函數。      考慮 CNN convolution 函數，定義如下。$x, y \in Z^2$    推廣到 G-CNN convolution.  $g, h \in G$上式是 forward pass 的 convolution ($\ast$).  下式是 backward pass 的 correlation ($\star$).      Combine 2 and 3, $L_u f \star \psi = f \star \psi \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}((u^{-1}g)^{-1}h) \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(g^{-1}uh)   \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(u^{-1}h) \psi_{k}^{i}(g^{-1}h) \  = [L_u f] \star \psi$        Combine 2 and 3, $L_u f * \psi = f * \psi \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1} (u^{-1}g)) \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1}u^{-1}g)   \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(u^{-1}h) \psi_{k}^{i}(h^{-1}g) \  = [L_u f] * \psi$  我們用一個 1D convolution 來驗證。 Example: g = [(-1)^m, u; 0, 1]   g^-1 = [(-1)^m, -u*(-1)^m; 0 , 1]Does it make sense?   If $\psi$ is an odd function, $f \star \psi^{i} =0$?No, g = (x, m) =&gt; m should be kept instead of disappear after summation!!Let’s look at another example, polar transform. [@estevesPOLARTRANSFORMER2018]Polar CoordinateA similarity transformation, i.e. conformal mapping, 旋轉(R)+scaling(s)+平移(t), $\rho \in $ SIM(2), acts on a point in $x \in R^2$ bywhere SO(2) is the rotation group.Equivariance to SIM(2) is achieved by (1) learning the center of the dilated rotation, (2) shifting the original image accordingly then (3) transforming the image to canonical coordinates.Q1: How to find the center of rotation? Need an origin predictor.Transformation of the image $L_t I = I(t-t_o)$ reduces the SIM(2) deformation to a dilated-rotation if $t_o$ is the true translation. After centering, we perform $SO(2) \times R^+$ convolutions on the new image $I_o = I(x-t_o)$.Layer 1 convolution 變成：In summary,本文 (polar transformation) 比較像是 coordinate transformation instead of adding group dimension.  No. 從原始的 $t \in R^2$, 多了 rotation and scale dimension $SO(2) \times R^+$.  But yes, 就 convolution 而言，feature extraction 已經不是 (x,y) convolution, 而是 $\epsilon, \theta$  location information 仍然存在，但用 origin predictor 取代 (x,y) convolution learning.Equivariant: $\Phi$ is CNN and Bijective (reversible, stride=1, ignore boundary)(Translation) Equivariant:= T’y = T’f(x) where T’ is another coordinate which could be different from T because of scaling, etc.   But both T and T’ are linear operators. This orange part is the crucial step assuming translation equivariant!!   However, T is translation equivariant, but NOT rotational equivariant. y’ = T’y = T’ f(x) = T’ f(T^-1 x’)  assuming linear inversible operation.Use [@cohenGroupEquivariant2019] notation $f \to \Phi$ and $T \to T_g$ Original output feature is $\Phi(x)$, where $\Phi$ can be a nonlinear (complicated) mapping, such as convolution + pooling + ReLU.Given input image x is transformed by $T_g$ operator/transform, new output feature is $\Phi(T_g x)$.如果具有 translation equivariant =&gt; $\Phi(T_g x) = T’_g \Phi(x)$ where T’_g 是同樣的 translation operator/transform, but may have different scaling factor (stride &gt; 1).所以 $T_g$ and $T’_g$ 需要有什麼特性？只需要 linear, i.e. $T(gh) = T(g)T(h)$.如果 $T’_g = I$ for all g, 是 special case, 稱為 invariant.  這和一般物理定義的 invariant 似乎不同?  對於深度學習 invariant 會失去 spatial information, $T’_g$ 而變得無用, equivariance 是更有用。另一個極端是沒有 equivariant, 也就是 $\Phi(T_g x)$ 和 $\Phi(x)$ 沒有簡單的 linear mapping, 例如 Multi-layer Perceptron (MLP).Paper 另外一段話如下，似乎和 invariant 相抵觸? No, 是擴充到 non-injective (降維) network.A network $\Phi$ can be non-injective, meaning that non-identical vectors $x$ and $y$ in the input space become identical in the output space.  (for example, two instances of a face may be mapped onto a single vector indicating the presence of any face, e.g. 人臉偵測而非識別，兩個不同的人臉對應到相同的 feature map or bounding box).  If $\Phi$ is equivariant, then the G-transformed inputs $T_g x$ and $T_g y$ must also mapped to the same output.  Their “sameness” is preserved under symmetry transformations.數學表示：Non-injective network: $\Phi(x) = \Phi(y)$ with $x \ne y$ If $\Phi$ is equivariant, then the G-transform (symmetry transform) has:$\Phi(T_g x)  = T’_g \Phi(x) = T’_g \Phi(y) = \Phi(T_g y)$ with $x \ne y$g represents general group, in the paper considering three groups: Z2, p4, p4m.  Conclusion.  On MNIST and CIFAR, G-CNN performs better than CNN at about same parameter number.  G-CNN also benefit from data augment.  Step 1: G-CNN to include translation, rotation, mirror on grid  Step 2: G-CNN on hexagon grid  Step 3: On 3D sphere and use G-FFT to compute sphere convolution for 3D application.  Step 4: Gauge CNN?CNN, pooling, ReLU are translation equivariant (up to edge effect); but MLP is NOT translation equivariant.Translation Equivariant:  There is a function (e.g. CNN)1D =&gt; 2D convolution =&gt; high dimension tensor convolutionStep 1: Define the network operator $\Phi$ Step 2: Find the commuting operator $T$, actually, a commutative group $T_g$.  $\Phi$ 可以視為 $T_g$ 的一個 element. Step 3: Find the group generator for the commutative group.What is the fundamental element of a group? =&gt; generator &lt;g, ..&gt;!所有的 group element 都可以從 generator &lt;g, ..&gt; 產生。All Abelian group is isomorphic to direct sum of primed cycle group =&gt; generator g, gg, ggg, …Group ExamplesReferenceBronstein, Michael M., Joan Bruna, Yann LeCun, Arthur Szlam, and PierreVandergheynst. 2017. “Geometric Deep Learning: Going Beyond EuclideanData.” IEEE Signal Processing Magazine 34 (4): 18–42.https://doi.org/10.1109/MSP.2017.2693418.Cohen, Taco S, T S Cohen, and Uva Nl. 2019. “Group Equivariant Convolutional Networks,” 10.Cohen, Taco S., Maurice Weiler, Berkay Kicanaoglu, and Max Welling.  “Gauge Equivariant Convolutional Networks and the IcosahedralCNN,” May. http://arxiv.org/abs/1902.04615.Pavlus, John. 2020. “An Idea from Physics Helps AI See in HigherDimensions.” Quanta Magazine. January 9, 2020.https://www.quantamagazine.org/an-idea-from-physics-helps-ai-see-in-higher-dimensions-20200109/.prism. 2019. 「Group Equivariant CNN to Spherical CNNs: 從群等變卷積網絡到球面卷積網絡.」 知乎專欄. 2019.https://zhuanlan.zhihu.com/p/34042888.Winkels, Marysia, and Taco S. Cohen. 2018. 「3D G-CNNs for PulmonaryNodule Detection,」 April. http://arxiv.org/abs/1804.04656.XinZhiYuan. 2020. 「Geometrical Deep Learning 受愛因斯坦啟示：讓AI擺脫平面看到更高的維度.」 2020. https://kknews.cc/tech/gpkgx3e.html.]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> quantization </tag>
        
          <tag> model compression </tag>
        
          <tag> pruning </tag>
        
          <tag> distillation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[增進工程師效率 Julia Linear Algebra]]></title>
      <url>/2020/04/21/matrix-julia/</url>
      <content type="text"><![CDATA[Use Julia for Linear Algebrausing LinearAlgebraA = [1 2 3; 2 3 4; 4 5 6]3×3 Array{Int64,2}: 1  2  3 2  3  4 4  5  6eigvals(A)3-element Array{Float64,1}: 10.830951894845311      -0.8309518948453025      1.0148608166285778e-16det(A)0.0x = range(0, 10, length=1000)0.0:0.01001001001001001:10.0using PyPlotgrid()plot(x, sin.(x))1-element Array{PyCall.PyObject,1}: PyObject &lt;matplotlib.lines.Line2D object at 0x140288630&gt;Discrete Convolution Using Circulant Matrix$y[t] = h[t] * x[t]$  where $x[t] = [1, 2, 3, 0, -3, -1, 1, -2]$, $h[t] = [1, 3, 1]$Use Julia LinearAlgebra for matrix/vector operation.  Use two space for new line.Use DSP.conv to perform discrete convolution.  x: length=8; h: length=3; y: length=8+3-1=10 (padding two 0’s at x)import Pkg; Pkg.add("SpecialMatrices")using LinearAlgebrausing SpecialMatricesusing DSPusing FFTW[32m[1m Resolving[22m[39m package versions...[32m[1m  Updating[22m[39m `~/.julia/environments/v1.1/Project.toml`[90m [no changes][39m[32m[1m  Updating[22m[39m `~/.julia/environments/v1.1/Manifest.toml`[90m [no changes][39mx = [1, 2, 3, 0,  -3, -1,  1, -2, 0, 0];h = [1, 3, 1];y = conv(x, h);y'1×12 Adjoint{Int64,Array{Int64,1}}: 1  5  10  11  0  -10  -5  0  -5  -2  0  0Circulant Matrix Multiplication Approximates Dicrete ConvolutionFirst extend $h[t]$ by padding seven 0’s (10-3=7).Use SpecialMatrices.Cirlulant to cyclic shift $h[t]$ and form a 10x10 Circulant matrix $\Phi$.Use SpecialMatrices.Matrix to convert special matrix type to normal Array$y = \Phi x$Φ = Matrix(Circulant([1,3,1,0,0,0,0,0,0,0]))10×10 Array{Int64,2}: 1  0  0  0  0  0  0  0  1  3 3  1  0  0  0  0  0  0  0  1 1  3  1  0  0  0  0  0  0  0 0  1  3  1  0  0  0  0  0  0 0  0  1  3  1  0  0  0  0  0 0  0  0  1  3  1  0  0  0  0 0  0  0  0  1  3  1  0  0  0 0  0  0  0  0  1  3  1  0  0 0  0  0  0  0  0  1  3  1  0 0  0  0  0  0  0  0  1  3  1y = Φ * x;y'1×10 Adjoint{Int64,Array{Int64,1}}: 1  5  10  11  0  -10  -5  0  -5  -2Find eigenvalues and eigenvectors of $\Phi$$\Phi$ is a Circulant matrix, its eigenvalue array s[n] is “equivalent” to DFT($h[t]$), sort of,up to frequency sequence difference.For example, DFT frequency sequence is always defined counter clockwise on the unit circle (0,1,2,..,9) for n=10.The eigenvalue/eigenvector decomposition: $\Phi = U P U^{*}$ In this eigvals implementation frequency is defined as conjugate first on the unit circle (0,1,9,2,8…,5)The first eigenvalue of $\Phi$ corresponds to Nyquist frequency = 0 (DC: 1+1+3=5)The last eigenvalue of $\Phi$ corresponds to Nyquist frequency = $\pi$ (highest AC: 1+1-3=-1)Its eigenvector array P cosists of eigenvectors in column sequences.The first column corresponds to DC eigenvector: [1, 1, …, 1]’.The last column corresponds to DC eigenvector: [1, -1, …, -1]’.s = eigvals(Φ)10-element Array{Complex{Float64},1}:                 5.0 + 0.0im                 3.7360679774997863 + 2.7144122731725697im  3.7360679774997863 - 2.7144122731725697im   1.118033988749894 + 3.440954801177931im    1.118033988749894 - 3.440954801177931im  -0.7360679774997894 + 2.265384296592988im  -0.7360679774997894 - 2.265384296592988im  -1.1180339887498945 + 0.8122992405822655im -1.1180339887498945 - 0.8122992405822655im -1.0000000000000002 + 0.0im               fft([1 3 1 0 0 0 0 0 0 0])'10×1 Adjoint{Complex{Float64},Array{Complex{Float64},2}}:                 5.0 - 0.0im                   3.73606797749979 + 2.714412273172573im    1.118033988749895 + 3.4409548011779334im -0.7360679774997898 + 2.2653842965929876im  -1.118033988749895 + 0.8122992405822659im                -1.0 - 0.0im                 -1.118033988749895 - 0.8122992405822659im -0.7360679774997898 - 2.2653842965929876im   1.118033988749895 - 3.4409548011779334im    3.73606797749979 - 2.714412273172573im P = Diagonal(s)10×10 Diagonal{Complex{Float64},Array{Complex{Float64},1}}: 5.0+0.0im          ⋅          …           ⋅                ⋅         ⋅      3.73607+2.71441im              ⋅                ⋅         ⋅              ⋅                      ⋅                ⋅         ⋅              ⋅                      ⋅                ⋅         ⋅              ⋅                      ⋅                ⋅         ⋅              ⋅          …           ⋅                ⋅         ⋅              ⋅                      ⋅                ⋅         ⋅              ⋅                      ⋅                ⋅         ⋅              ⋅             -1.11803-0.812299im       ⋅         ⋅              ⋅                      ⋅           -1.0+0.0imU = eigvecs(Φ)(round.(U*1000*sqrt(10)))/100010×10 Array{Complex{Float64},2}: 1.0+0.0im   0.809+0.588im   0.809-0.588im  …  -0.809-0.588im   1.0+0.0im 1.0+0.0im     1.0+0.0im       1.0-0.0im          1.0-0.0im    -1.0+0.0im 1.0+0.0im   0.809-0.588im   0.809+0.588im     -0.809+0.588im   1.0+0.0im 1.0+0.0im   0.309-0.951im   0.309+0.951im      0.309-0.951im  -1.0+0.0im 1.0+0.0im  -0.309-0.951im  -0.309+0.951im      0.309+0.951im   1.0+0.0im 1.0+0.0im  -0.809-0.588im  -0.809+0.588im  …  -0.809-0.588im  -1.0+0.0im 1.0+0.0im    -1.0-0.0im      -1.0+0.0im          1.0-0.0im     1.0+0.0im 1.0+0.0im  -0.809+0.588im  -0.809-0.588im     -0.809+0.588im  -1.0+0.0im 1.0+0.0im  -0.309+0.951im  -0.309-0.951im      0.309-0.951im   1.0+0.0im 1.0+0.0im   0.309+0.951im   0.309-0.951im      0.309+0.951im  -1.0+0.0imU_b = inv(U)(round.(U_b*1000*sqrt(10)))/100010×10 Array{Complex{Float64},2}:    1.0+0.0im       1.0-0.0im    …     1.0+0.0im       1.0-0.0im    0.809-0.588im     1.0-0.0im       -0.309-0.951im   0.309-0.951im  0.809+0.588im     1.0+0.0im       -0.309+0.951im   0.309+0.951im -0.809-0.588im   0.309-0.951im      0.309+0.951im  -0.809+0.588im -0.809+0.588im   0.309+0.951im      0.309-0.951im  -0.809-0.588im -0.809+0.588im  -0.309-0.951im  …   0.309-0.951im   0.809+0.588im -0.809-0.588im  -0.309+0.951im      0.309+0.951im   0.809-0.588im -0.809-0.588im     1.0+0.0im        0.309-0.951im   0.309+0.951im -0.809+0.588im     1.0-0.0im        0.309+0.951im   0.309-0.951im    1.0-0.0im      -1.0+0.0im          1.0+0.0im      -1.0-0.0im  (round.((U_b - U')*1000*sqrt(10)))/1000 # U_b is the same as conjugate transpose10×10 Array{Complex{Float64},2}:  0.0+0.0im   0.0-0.0im   0.0-0.0im  …  -0.0-0.0im  -0.0+0.0im   0.0-0.0im  0.0+0.0im  -0.0-0.0im   0.0-0.0im      0.0+0.0im   0.0+0.0im   0.0+0.0im  0.0-0.0im  -0.0+0.0im   0.0+0.0im     -0.0+0.0im   0.0-0.0im   0.0-0.0im  0.0+0.0im  -0.0+0.0im  -0.0-0.0im      0.0-0.0im  -0.0+0.0im  -0.0+0.0im  0.0-0.0im  -0.0-0.0im  -0.0+0.0im     -0.0+0.0im  -0.0-0.0im  -0.0-0.0im  0.0-0.0im   0.0+0.0im   0.0+0.0im  …   0.0-0.0im  -0.0-0.0im   0.0+0.0im  0.0+0.0im   0.0-0.0im   0.0-0.0im     -0.0+0.0im  -0.0+0.0im   0.0-0.0im  0.0+0.0im  -0.0+0.0im   0.0-0.0im      0.0-0.0im   0.0-0.0im  -0.0+0.0im  0.0+0.0im  -0.0-0.0im   0.0+0.0im      0.0+0.0im  -0.0+0.0im   0.0-0.0im -0.0-0.0im   0.0+0.0im  -0.0+0.0im     -0.0-0.0im   0.0+0.0im   0.0-0.0imPhi = U * P * U_b  # verify U*P*U_b is the eigen value decompostion of Φreal.(round.(Phi*1000))/100010×10 Array{Float64,2}:  1.0  -0.0   0.0   0.0   0.0   0.0   0.0  -0.0   1.0   3.0  3.0   1.0   0.0   0.0  -0.0  -0.0  -0.0   0.0  -0.0   1.0  1.0   3.0   1.0   0.0   0.0   0.0   0.0   0.0  -0.0  -0.0  0.0   1.0   3.0   1.0   0.0   0.0   0.0  -0.0  -0.0   0.0  0.0   0.0   1.0   3.0   1.0   0.0   0.0  -0.0  -0.0   0.0  0.0   0.0   0.0   1.0   3.0   1.0  -0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0   1.0   3.0   1.0   0.0   0.0   0.0  0.0   0.0   0.0  -0.0  -0.0   1.0   3.0   1.0  -0.0   0.0 -0.0   0.0   0.0  -0.0   0.0   0.0   1.0   3.0   1.0  -0.0  0.0   0.0  -0.0   0.0   0.0   0.0   0.0   1.0   3.0   1.0Commutative Group - Translation Equivariant  Discrete convolution is equivalent to Circulant matrix multiplication.  Circulant matrix is itself commutative/Abelian group.  All Cirulant matrix multiplication can be decomposed into translation matrix multiplication’s superposition.  Discrete convolution is therefore translation multiplication commutable =&gt; translation equivariantR = rand(10, 10)Φ * R - R * Φ   # Random matrix multiplication does NOT commutate with Circulant matrix10×10 Array{Float64,2}: -1.58559   -0.237127  -0.129967  …   0.066688  -1.72504    -1.40459   1.04033    1.08089    0.34026      -0.782441  -1.91891    -1.63573   3.35632    0.831891   0.805718     -0.849477   1.18107     0.183805  0.815998  -1.54383   -0.38105      -1.00838   -0.141359   -0.176457 -0.915783   0.225814  -1.14518       0.5001     0.128517    1.68327  -2.07181    1.04074   -2.12622   …   1.62098    0.348925    1.07887   0.313094   1.84738   -0.894152      0.282517  -2.32041    -0.039078  1.11406   -0.71119   -1.059        -0.981632  -0.0110641   0.999691  1.42449   -1.50675   -1.53478       1.06594    0.590891    2.41234  -2.2159    -2.33044   -1.29612       0.626618  -0.119957    0.743334Tg = Circulant([1,2,3,4,5,6,7,8,9,10]) # Verify Circulant matrix multiplication is a commutative group10×10 Circulant{Int64}:  1  10   9   8   7   6   5   4   3   2  2   1  10   9   8   7   6   5   4   3  3   2   1  10   9   8   7   6   5   4  4   3   2   1  10   9   8   7   6   5  5   4   3   2   1  10   9   8   7   6  6   5   4   3   2   1  10   9   8   7  7   6   5   4   3   2   1  10   9   8  8   7   6   5   4   3   2   1  10   9  9   8   7   6   5   4   3   2   1  10 10   9   8   7   6   5   4   3   2   1Φ * Tg - Tg * Φ   10×10 Array{Int64,2}: 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0g = Circulant([0,1,0,0,0,0,0,0,0,0])  # Circulant matrix group generator: right cyclic shift by 110×10 Circulant{Int64}: 0  0  0  0  0  0  0  0  0  1 1  0  0  0  0  0  0  0  0  0 0  1  0  0  0  0  0  0  0  0 0  0  1  0  0  0  0  0  0  0 0  0  0  1  0  0  0  0  0  0 0  0  0  0  1  0  0  0  0  0 0  0  0  0  0  1  0  0  0  0 0  0  0  0  0  0  1  0  0  0 0  0  0  0  0  0  0  1  0  0 0  0  0  0  0  0  0  0  1  0g*g  # right cyclic shift by 210×10 Array{Int64,2}: 0  0  0  0  0  0  0  0  1  0 0  0  0  0  0  0  0  0  0  1 1  0  0  0  0  0  0  0  0  0 0  1  0  0  0  0  0  0  0  0 0  0  1  0  0  0  0  0  0  0 0  0  0  1  0  0  0  0  0  0 0  0  0  0  1  0  0  0  0  0 0  0  0  0  0  1  0  0  0  0 0  0  0  0  0  0  1  0  0  0 0  0  0  0  0  0  0  1  0  0eye = 1.0*Matrix(I, 10, 10)   # Verify Circulant Tg is decomposed into group generator superposition1*eye+2*g+3*g*g+4*g*g*g+5*g*g*g*g+6*g*g*g*g*g+7*g*g*g*g*g*g+8*g*g*g*g*g*g*g+9*g*g*g*g*g*g*g*g+10*g*g*g*g*g*g*g*g*g10×10 Array{Float64,2}:  1.0  10.0   9.0   8.0   7.0   6.0   5.0   4.0   3.0   2.0  2.0   1.0  10.0   9.0   8.0   7.0   6.0   5.0   4.0   3.0  3.0   2.0   1.0  10.0   9.0   8.0   7.0   6.0   5.0   4.0  4.0   3.0   2.0   1.0  10.0   9.0   8.0   7.0   6.0   5.0  5.0   4.0   3.0   2.0   1.0  10.0   9.0   8.0   7.0   6.0  6.0   5.0   4.0   3.0   2.0   1.0  10.0   9.0   8.0   7.0  7.0   6.0   5.0   4.0   3.0   2.0   1.0  10.0   9.0   8.0  8.0   7.0   6.0   5.0   4.0   3.0   2.0   1.0  10.0   9.0  9.0   8.0   7.0   6.0   5.0   4.0   3.0   2.0   1.0  10.0 10.0   9.0   8.0   7.0   6.0   5.0   4.0   3.0   2.0   1.0(Φ*(Tg*x) - Tg*(Φ*x) )'    # Φ and Tg are commutative on input signal x as expected1×10 Adjoint{Int64,Array{Int64,1}}: 0  0  0  0  0  0  0  0  0  0(Φ*x)'   # normal discrete convolution1×10 Adjoint{Int64,Array{Int64,1}}: 1  5  10  11  0  -10  -5  0  -5  -2(g*(Φ*x))'   # group generator causes discrete convolution right cyclic translation1×10 Adjoint{Int64,Array{Int64,1}}: -2  1  5  10  11  0  -10  -5  0  -5(g*x)'    # group generator's action on x[t] is to right cyclic shift by 11×10 Adjoint{Int64,Array{Int64,1}}: 0  1  2  3  0  -3  -1  1  -2  0(Φ*(g*x))'  # discrete convolution of the right cyclic shift signal1×10 Adjoint{Int64,Array{Int64,1}}: -2  1  5  10  11  0  -10  -5  0  -5(Φ*(g*x) - g*(Φ*x) )'   # discrete convolution is translation (i.e. g or g*g or g*g*g ...) eqivaraint (commutative)1×10 Adjoint{Int64,Array{Int64,1}}: 0  0  0  0  0  0  0  0  0  0]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Edge AI Trilogy III - Model Compression]]></title>
      <url>/ai/2020/04/05/EdgeAI-Compression/</url>
      <content type="text"><![CDATA[Edge AI Trilogy III - Model CompressionEdge AI 三部曲的最終篇是 model compression.  之後還會有番外篇 on advance topics such as NAS, etc.  為什麼 model compression 放在最終篇？可以用 Han Song 的 deep compression [@hanDeepCompression2016; @hanLearningBoth2015] 為例。下圖架構正好對應三部曲：pruning, quantization, and (parameter) compression.基本上 parameter compression 可以收割之前 sparsity, quantization, weight sharing 帶來的 storage and memory bandwidth reduction (35x-49x) 的好處。當然隨著 parameter reduction, 額外還有 computation and energy reduction 的好處，例如 zero-skipping for sparsity 和 low bitwidth MAC computation for quantization and weight sharing.Model compression 包含 parameter compression 以及其他的技巧減少 parameter, 甚至改變 model structure。最後的 model size (in MB) and MAC (in GFlop) 就是 “moment of the truth”.  就像 SNR or BER 是通訊系統的整體檢驗。除了 parameter pruning (sparsity) and sharing (clustering and quantizing) 之外，[@chengSurveyModel2019] 把 model compression 作法分為四類：            Theme Name      Description      Applications      Details                  Parameter pruning and sharing      Reducing redundant parameters not sensitive to the performance      CONV and FC layer      Robust to various settings, can achieve good performance, support both train from scratch and pre-trained model              Low-rank factorization      Using matrix/tensor decomposition to estimate the informative parameters      CONV and FC layer      Standardized pipeline, easily to implement, support both train from scratch and pre-trained model              Transferred/compact convolutional filters      Designing special structural convolutional filters to save parameters      CONV layer only      Algorithms are dependent on applications, usually achieve good performance, only support train from scratch              Knowledge distillation      Training a compact neural network with distilled knowledge of a large model      CONV and FC layer      Model performances are sensitive to applications and network structure only support train from scratch      Another paper 提出的 model compression 分類 [@kuzminTaxonomyEvaluation2019].  SVD-based methods (low rank)  Tensor decomposition-based methods (low rank)  Pruning methods  Compression ratio selection method  Loss-aware compression  Probabilistic compression  Efficient architecture designAnother good review paper from Purdue. [@goelSurveyMethods2020]            Technique      Description      Advantages      Disadvantages                  Quantization and Pruning      Reduces precision/completely removes the redundant parameters and connections from a DNN.      Negligible accuracy loss with small model size. Highly efficient arithmetic operations.      Difficult to implement on CPUs and GPUs because of matrix sparsity. High training costs.              Filter Compression and Matrix Factorization      Decreases the size of DNN filters and layers to improve efficiency.      High accuracy. Compatible with other optimization techniques.      Compact convolutions can be memory inefficient. Matrix factorization is computationally expensive.              Network Architecture Search      Automatically finds a DNN architecture that meets performance and accuracy requirements on a target device.      State-of-the-art accuracy with low energy consumption.      Prohibitively high training costs.              Knowledge Distillation      Trains a small DNN with the knowledge of a larger DNN to reduce model size.      Low computation cost with few DNN parameters.      Strict assumptions on DNN structure. Only compatible with softmax outputs.      Model =&gt; Data =&gt; Memory?? (from Huawei’s talk)My classification:Level 1:  Compression Without changing the network layer and architecture, i.e. weight compression including pruning (weight = 0), quantization (reduce weight bitwidth), weight sharing, etc.  可以到達 10x-50x compression for large network (e.g. resnet, alexnet, etc.)Level 2:  Modify network architecture based on some basic rules (matrix/tensor decomposition), network fusion, etc.Level 3: Change the network architecture completely.  Knowledge transfer (KT) or knowledge distillation (KD) belongs to Level 3 or Level 4?Level 4: Network architecture search (NAS) to explore a big search space and based on the constraints of edge device capability.  嚴格來說，已經不是 model compression, 而是 model search or exploration.Level 1: Quantization and Pruning and Huffman EncodeDetails 可以參考之前兩篇文章。In summary, quantization from FP32 to INT8 可以 save up to 75% (4x) storage/bandwidth/computation, 而不損失 accuracy or increase error. 這也代表更低的功耗。如果使用更少 bitwidth (6/4/2/1), 可以省更多，但是可能 trade-off accuracy/error 或是限制應用的範圍。下圖是不同 quantized bitwidth 對應的 energy vs. test error.Pruning 是另一個更深奧更有空間的方式。可以視為 quantization 的 special case (weight and activation = 0), 但更進一步是一個 subset network 可以完全 represent the original network (lottery conjecture).  pruning 對於一些 “fat network” 可以達到 10x 的 saving.  對於一些 “lean network”, e.g. MobileNet 就比較少 saving.Parameter compression 也是常用的技巧。包含 weight compression and activation compression.Parameter compression 省最多是 data 包含很多的冗余或是 regular structure，例如大量的 0, 從 information theory 就是 low entropy facilitate compression.  Huffman encoding 是一個有效的方法 for parameter compression.pruning, quantization, compression 可以合在一起得到最佳的效果 at a cost of higher training time.Level 2: Matrix and Tensor Decomposition分為兩個部分: CONV layer 和 FC layer.  當然廣義來說，FC layer 也是一種 CONV layer with kernel size WxHxC.  此處 CONV layer 的 kernel size 一般指 1x1, 3x3, …, 11x11, etc.對於 CONV layer, 越大 kernel filter 的 parameters and MACs 越大，較小的 kernel filter 的 parameters and MACs 越小。但如果把所有大的 kernel filter 都替換成小的 kernel filter, 會影響 DNN 的平移不變性，這會降低 DNN model 的精度。因此一些策略是識別冗余的 kernel filter, 並用較小 kernel filter 取代。例如 VGG 把所有大於 3x3 (e.g. 5x5, 7x7, etc.) 都用 3x3 filter 取代。SqueezeNet and MobileNet 甚至用 1x1 取代部分的 3x3 filter.Convolutional Filter Compression (CONV layer only)SqueezeNet use 1x1 kernel to replace 3x3 kernel filter. MobileNet use depth-wise + point-wise (1x1) network to replace original kernel to reduce computation by 1/8-1/9 for 3x3 kernel.MobileNet v3 可以達到不錯的精度 (75%)，但是 parameter and MAC 比起 AlexNet 少非常多。比起 ResNet50 (parameter 25M, MAC 4G) 也好不少。Matrix Factorization/Decomposition (CONV or FC layer)通過將張量或矩陣分解為合積形式（sum-product form），將多維張量分解為更小的矩陣，從而可以消除冗余計算。一些因子分解方法可以將DNN模型加速4 倍以上，因為它們能夠將矩陣分解為更密集的參數矩陣，且能夠避免非結構化稀疏乘法的局部性問題。目前 Matrix decomposition/factorization 並非主流。原因：  關於矩陣分解，有多種技術。Kolda等人證明，大多數因子分解技術都可以用來做DNN模型的加速，但這些技術在精度和計算複雜度之間不一定能夠取得最佳的平衡。  由於缺乏理論解釋，因此很難解釋為什麼一些分解（例如CPD、BMD）能夠獲得較高的精度，而其他分解卻不能。  與矩陣分解相關的計算常常與模型獲得的性能增益相當，造成收益與損耗抵消。  矩陣分解很難在大型DNN模型中實現，因為隨著深度增加分解超參會呈指數增長，訓練時間主要耗費在尋找正確的分解超參。更多更 detailed description 可以參考 [@kuzminTaxonomyEvaluation2019].Level 3: Knowledge Transfer or Distillationhttps://www.leiphone.com/news/202003/cggvDDPFIVTjydxS.html大模型比小模型更準確，因為參數越多，允許學習的函數就可以越複雜。那麼能否用小的模型也學習到這樣複雜的函數呢？一種方式便是知識遷移（KT, Knowledge Transfer），通過將大的DNN模型獲得的知識遷移到小的DNN模型上。為了學習複雜函數，小的 DNN 模型會在大的 DNN 模型標記處的數據上進行訓練。其背後的思想是，大的 DNN 標記的數據會包含大量對小的DNN有用的信息。例如大的 DNN 模型對一個輸入圖像在一些類標籤上輸出中高機率，那麼這可能意味著這些類共享一些共同的視覺特徵；對於小的 DNN模型，如果去模擬這些機率，相比於直接從數據中學習，要能夠學到更多。具體的作法之一是 Hinton 在 2014年 提出的知識蒸餾 (KD, Knowledge Distillation)，這種方法的訓練過程相比於知識遷移 (KT) 要簡單得多。在知識蒸餾中，小的 DNN 模型使用學生-教師模式進行訓練，其中小的 DNN 模型是學生，一組專門的 DNN 模型是教師；通過訓練學生，讓它模仿教師的 softmax 輸出，小的DNN 模型可以完成整體的任務。但在 Hinton 的工作中，小的 DNN 模型的準確度卻相應有些下降。 Li 等人利用最小化教師與學生之間特徵向量的歐氏距離，進一步提高的小的 DNN 模型的精度。類似的，FitNet 讓學生模型中的每一層都來模仿教師的特徵圖。但以上兩種方法都要求對學生模型的結構做出嚴格的假設，其泛化性較差。Knowledge transfer/distillation is very interesting and similar to proxy AI I thought before!Knowledge transfer or distillation 是個非常有趣而且實用的技術。例如 teacher model 可以是雲端的大 DNN 模型，有比較好的精度以及泛化性。但在 edge or device 可以 deploy student model, i.e. 小 DNN 模型。雖然精度和泛化性比較差，但是 quick response, 以及 edge and device 不一定需要非常強的泛化性 (e.g. local voice recognition, or local face detection).優點：基於知識遷移和知識蒸餾的技術可以顯著降低大型預訓練模型的計算成本。有研究表明，知識蒸餾的方法不僅可以在計算機視覺中應用，還能用到許多例如半監督學習、域自適應等任務中。缺點及改進方向：知識蒸餾通常對學生和教師的結構和規模有嚴格的假設，因此很難推廣到所有的應用中。此外目前的知識蒸餾技術嚴重依賴於 softmax 輸出，不能與不同的輸出層協同工作。作為改進方向，學生可以學習教師模型的神經元激活序列，而不是僅僅模仿教師的神經元/層輸出，這能夠消除對學生和教師結構的限制（提高泛化能力），並減少對softmax輸出層的依賴。Transfer learning is different from knowledge transfer/distillationThe objective of transfer learning and knowledge distillation are quite different. In transfer learning, the weights are transferred from a pre-trained network to a new network and the pre-trained network should exactly match the new network architecture.  What this means is that the new network is essentially as deep and complex as the pre-trained network.the objective of knowledge distillation is different. The aim is not to transfer weights but to transfer the generalizations of a complex model to a much lighter model.  如何 transfer generalizations?  使用 student-teach model 是一種方式。還有其他的方式可以參考 [@kompellaTapDark2018].Level 4: Network Architecture Search (NAS)在設計低功耗計算機視覺程序時，針對不同的任務可能需要不同的DNN模型架構。但由於存在許多這種結構上的可能性，通過手工去設計一個最佳DNN模型往往是困難的。最好的辦法就是將這個過程自動化，即網絡架構搜索技術（Network Architecture Search）。NAS使用一個遞歸神經網絡(RNN)作為控制器，並使用增強學習來構建候選的DNN架構。對這些候選DNN架構進行訓練，然後使用驗證集進行測試，測試結果作為獎勵函數，用於優化控制器的下一個候選架構。NASNet 和AmoebaNet 證明瞭NAS的有效性，它們通過架構搜索獲得DNN模型能夠獲得SOTA性能。為了獲得針對移動設備有效的DNN模型，Tan等人提出了MNasNet，這個模型在控制器中使用了一個多目標獎勵函數。在實驗中，MNasNet 要比NASNet快2.3倍，參數減少4.8倍，操作減少10倍。此外，MNasNet也比NASNet更準確。不過，儘管NAS方法的效果顯著，但大多數NAS算法的計算量都非常大。例如，MNasNet需要50,000個GPU 時才能在ImageNet數據集上找到一個高效的DNN架構。為了減少與NAS相關的計算成本，一些研究人員建議基於代理任務和獎勵來搜索候選架構。例如在上面的例子中，我們不選用ImageNet，而用更小的數據集CIFAR-10。FBNet正是這樣來處理的，其速度是MNasNet的420倍。但Cai等人表明，在代理任務上優化的DNN架構並不能保證在目標任務上是最優的，為了克服基於代理的NAS解決方案所帶來的局限性，他們提出了Proxyless-NAS，這種方法會使用路徑級剪枝來減少候選架構的數量，並使用基於梯度的方法來處理延遲等目標。他們在300個GPU時內便找到了一個有效的架構。此外，一種稱為單路徑NAS（Single-Path NAS）的方法可以將架構搜索時間壓縮到 4 個GPU時內，不過這種加速是以降低精度為代價的。優點：NAS通過在所有可能的架構空間中進行搜索，而不需要任何人工干預，自動平衡準確性、內存和延遲之間的權衡。NAS能夠在許多移動設備上實現準確性、能耗的最佳性能。缺點及改進方向：計算量太大，導致很難去搜索大型數據集上任務的架構。另外，要想找到滿足性能需求的架構，必須對每個候選架構進行訓練，並在目標設備上運行來生成獎勵函數，這會導致較高的計算成本。其實，可以將候選DNN在數據的不同子集上進行並行訓練，從而減少訓練時間；從不同數據子集得到的梯度可以合併成一個經過訓練的DNN。不過這種並行訓練方法可能會導致較低的準確性。另一方面，在保持高收斂率的同時，利用自適應學習率可以提高準確性。Model Compression ExamplesEx1: Deep Compression by Han (Level 1)參考 fig.1 使用 pruning, quantization, and parameter compression.  整體的效益如下。精度和 AlexNet 差不多 (TBC)。  Step 1: Pruning (9x-13x)  Step 2: Quantizing clustered weights for weight sharing (32bit -&gt; 5bit) (~4x)  Step 3: Compression: encode weights/index for weight compression; Huffman encoding sparsity/zero and weight sharing compression.  Total: 35x-49xAccuracy result (TBA)Ex2: Model compression via distillation and quantization (Level 1+3)This excellent paper [@polinoModelCompression2018] 1 proposes two new compression methods, which jointly leverage weight quantization and distillation of larger networks, called “teachers,” into compressed “student” networks.  簡單說 FP32 model 是 teacher model; quantized model 是 student model. Github code 2.The first method is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher network, into the training of a smaller student network whose weights are quantized to a limited set of levels. teacher model 使用 FP32 deep model; student model 則是 uniformly quantized and shallow model.  藉著 distillation loss that teacher model can train student model.The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. 使用和 student model 同樣的小模型。重點是 linear but non-uniform quantization. 但沒有 distillation loss; 而是用一般的 cross-entropy loss to train this model and optimize the non-uniform quantization.  當然也可以使用 non-uniform quantization for student model.  可能 computation 會太複雜。其他細節請參考[@polinoModelCompression2018].  這裏直接討論結果。CIFAR10 accuracy.  Teacher model: 5.3M param of FP32, 21MB, accuracy 89.71%.  有三種 student models, 分別為 1M/0.3M/0.1M param 如下表左第一欄。      第一欄 (A) 都是 FP32 full precision training. 例如 A1 代表 student model 1, 1M param, FP32：4MB.  兩個 accuracy 對應 cross-entropy loss and distillation loss.  Accuracy 84.5% 對應 normal training (cross-entropy loss).  Accuracy 88.8% 對應 teacher-student training (distillation loss).        第二欄 (B) 都是 quantized training.  PM (post-mortem) Quant. 只是把 FP32 teacher-student training 的 weight 直接 post training uniform quantization without any additional operation. 有兩種 PM Quant., 一是 global scaling (no bucket), 另一個是 local scaling (with bucket size = 256).  所以 (1) PM Quant. accuracy 一定差於 FP32 accuracy (88.8%).  Quantized Distill. 使用 distillation loss back propagation, 因此 accuracy is better than PM Quant.  In summary, FP Distill. &gt; Quantized Distill. &gt; PM (with bucket) &gt; PM (no bucket)        Differentiable Quant. 使用 cross-entropy loss training.  從另一個角度, non-uniform quantization, approach this problem.  在 4-bits quantization 的表現不輸於 uniform distillation loss training.  但在 2-bits quantization distillation loss training 還是比較好。合理推論，differentiable non-uniform quantization with distillation loss 應該會有最佳的 accuracy.    Differentiable quantization 在 computation 不容易在 edge AI 實現，因為 quantized values 不會落在 linear grid 上，很難用 finite bitwidth 表示，也很難做 MAC 計算。比較接近的解法是用 k-mean clustering algorithm to cluster weights and adopt the centroids as quantization points. Han 稱為 weight sharing.  最佳的結果 assuming accuracy loss &lt; 2% compared with baseline (89.71%) is student model 1 (1M param) of 4-bit with accuracy 88%. The total size of best student model 1 is: 1M param x 0.5 = 0.5MB.  A factor of 21MB/0.5MB = 42x saving in memory/bandwidth/computation etc.!!!更好的結果是用比較 deeper student model, 但是用 4-bit (5.8M x 0.5 = 2.9MB), 而且 accuracy 還比較好 (92.3% vs. 89.71%).  A factor of 21MB/2.9MB = 7.2x saving in memory/bandwidth/computation etc.!!!CIFAR100 accuracy.  Teacher model: 36.5M param of FP32, 146MB, accuracy 77.21%.  Student model is 17.2M param, about 1/2 of teacher model.  4-bit model is 8.2MB with accuracy 76.31%.  A factor of 146MB/8.2MB = 17.8x saving.  Differential quantization seems to perform better, but with more params and more complicated computation.Imagenet accuracy.  Teacher model: ResNet34 or ResNet50  Student model: 2xResNet18 QD 4 bit and 2xResNet34 QD 4 bit  Student model of 4-bit 可以得到和 teacher model 類似 accuracy.  但 size 比起 teacher FP32 model 少了 2x-4x.Distillation + Quantization 結論：  FP32 to INT8 已經有很多 post-training quantization or quantization aware training 可以達到同樣的 accuracy.  因此 4x saving 已經很普通。  Teacher-student models 在小 dataset (CIFAR10), model compression 效果比較突出: (1) 8x from FP32 to 4-bit;  (2) student model 可以比 teacher model gain 4x-5x.  但對於大 dataset, CIFAR100 or ImageNet, student model param 已經接近甚至超過 teacher model param.  此時只有 FP32 to 4-bit gain.  對於大 dataset, knowledge distillation 的結果並不突出。Summary  Level 1 + Level 2 compression 可以同時使用。增加壓縮的倍率。  Level 1 + Level 3 compression 可以同時使用。對於大 dataset 效果有限。但是小 dataset 似乎不錯。  Level 4 NAS 雖然看起來很好，但是需要太多的 computation resource/time to search.  需要更好的方式用於 edge AI.ReferenceCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2019. “A Survey of ModelCompression and Acceleration for Deep Neural Networks,” September.https://arxiv.org/abs/1710.09282v8.Goel, Abhinav, Caleb Tung, Yung-Hsiang Lu, and George K. Thiruvathukal. 2020. “A Survey of Methods for Low-Power Deep Learning and Computer Vision,” March. http://arxiv.org/abs/2003.11066.Han, Song, Huizi Mao, and William J. Dally. 2016. “Deep Compression:Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,” February. http://arxiv.org/abs/1510.00149.Han, Song, Jeff Pool, John Tran, and William J. Dally. 2015. “Learning Both Weights and Connections for Efficient Neural Networks,” October. http://arxiv.org/abs/1506.02626.Kompella, Ravindra. n.d. “Tap into the Dark Knowledge Using Neural Nets Distillation.” Medium. Accessed April 2, 2020.https://towardsdatascience.com/knowledge-distillation-and-the-concept-of-dark-knowledge-8b7aed8014ac.Kuzmin, Andrey, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, TijmenBlankevoort, and Max Welling. 2019. “Taxonomy and Evaluation ofStructured Compression of Convolutional Neural Networks,” December.http://arxiv.org/abs/1912.09802.Polino, Antonio, Razvan Pascanu, and Dan Alistarh. 2018. “ModelCompression via Distillation and Quantization,” February.http://arxiv.org/abs/1802.05668.            Published in ICLR 2018 &#8617;              https://github.com/antspy/quantized_distillation &#8617;      ]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> quantization </tag>
        
          <tag> model compression </tag>
        
          <tag> pruning </tag>
        
          <tag> distillation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[增進工程師效率 Python DataFrame - CSV & Plot]]></title>
      <url>/language/2018/12/22/dataframe/</url>
      <content type="text"><![CDATA[Download the code: https://github.com/allenlu2009/colab/blob/master/dataframe_demo.ipynbPython DataFrameCreate DataFrame      Direct input        Use dict: Method 1: 一筆一筆加入。  import pandas as pddict1 = {'Name': 'Allen' , 'Sex': 'male', 'Age': 33}dict2 = {'Name': 'Alice' , 'Sex': 'female', 'Age': 22}dict3 = {'Name': 'Bob' , 'Sex': 'male', 'Age': 11}data = [dict1, dict2, dict3]df = pd.DataFrame(data)df                  Name      Sex      Age                  0      Allen      male      33              1      Alice      female      22              2      Bob      male      11      Method 2: 一次加入所有資料。name = ['Allen', 'Alice', 'Bob']sex = ['male', 'female', 'male']age = [33, 22, 11]all_dict = {    "Name": name,    "Sex": sex,    "Age": age}df = pd.DataFrame(all_dict)df[['Name', 'Age']]                  Name      Age                  0      Allen      33              1      Alice      22              2      Bob      11      Dataframe 的屬性  ndim: 2 for 2D dataframe; axis 0 =&gt; row; axis 1 =&gt; column  shape:  (row no. x column no.) (not including number index)  dtypes: (object or int) of each columndf.ndim2df.shape(3, 3)df.dtypesName    objectSex     objectAge      int64dtype: objectdf.columnsIndex(['Name', 'Sex', 'Age'], dtype='object')df.indexRangeIndex(start=0, stop=3, step=1)Read CSVDonwload a test csv file from https://people.sc.fsu.edu/~jburkardt/data/csv/csv.html  Pick the biostats.csvFor 2, Before read csv, reference Medium article to import google drive  Read csv 使用 read_csv function.  但是要加上 skipinitialspace to strip the leading space!!  Two ways to read_csv: (1) load csv file directly; (2) load from urlimport pandas as pdfrom google.colab import drivedrive.mount('/content/drive')#!ls 'drive/My Drive/Colab Notebooks/'df = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', skipinitialspace=True)dfGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonlyEnter your authorization code:··········Mounted at /content/drive                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      url = "https://people.sc.fsu.edu/~jburkardt/data/csv/biostats.csv"df = pd.read_csv(url, skipinitialspace=True)df                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      print(df.columns); print(df.index)Index(['Name', 'Sex', 'Age', 'Height (in)', 'Weight (lbs)'], dtype='object')RangeIndex(start=0, stop=18, step=1)df.ndim2df.shape(18, 5)df.dtypesName            objectSex             objectAge              int64Height (in)      int64Weight (lbs)     int64dtype: objectBasic Viewing Commanddf.head(3)                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155      df.tail(3)                  Name      Sex      Age      Height (in)      Weight (lbs)                  15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      df.shape(18, 5)df.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 18 entries, 0 to 17Data columns (total 5 columns):Name            18 non-null objectSex             18 non-null objectAge             18 non-null int64Height (in)     18 non-null int64Weight (lbs)    18 non-null int64dtypes: int64(3), object(2)memory usage: 848.0+ bytesdf[7:10]                  Name      Sex      Age      Height (in)      Weight (lbs)                  7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143      df['Name'][7:10]7    Hank8    Ivan9    JakeName: Name, dtype: objectdf[['Name', 'Age', 'Sex']][7:10]                  Name      Age      Sex                  7      Hank      30      M              8      Ivan      53      M              9      Jake      32      M      df.loc[7:10, ['Name', 'Age', 'Sex']] # compare with loc call                  Name      Age      Sex                  7      Hank      30      M              8      Ivan      53      M              9      Jake      32      M              10      Kate      47      F      df.count()Name            18Sex             18Age             18Height (in)     18Weight (lbs)    18dtype: int64Basic Index OperationIndex (索引) is a very useful key for DataFrame.  The default index is the row number starting from 0 to N-1, where N is the number of data.除了用 row number 做為 index, 一般也會使用 unique feature 例如 name, id, or phone number 做為 index.把 column 變成 index  Method 1: 直接在 read_csv 指定 index_col.  可以看到 index number 消失，而被 Name column 取代。df = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', skipinitialspace=True, index_col='Name')df                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131        df.index shows the element in index columndf.indexIndex(['Alex', 'Bert', 'Carl', 'Dave', 'Elly', 'Fran', 'Gwen', 'Hank', 'Ivan',       'Jake', 'Kate', 'Luke', 'Myra', 'Neil', 'Omar', 'Page', 'Quin', 'Ruth'],      dtype='object', name='Name')  使用 reset_index 又會回到 index number.df.reset_index()                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      再看一次 df 並沒有改變。很多 DataFrame 的 function 都是保留原始的 df, create a new object, 也就是 inplace = False.   如果要取代原來的 df, 必須 inplace = True!df                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.reset_index(inplace=True)df                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      如果再 reset_index(）一次，會是什麼結果？此處用 default inplace=False.多了一個 index columndf.reset_index()                  index      Name      Sex      Age      Height (in)      Weight (lbs)                  0      0      Alex      M      41      74      170              1      1      Bert      M      42      68      166              2      2      Carl      M      32      70      155              3      3      Dave      M      39      72      167              4      4      Elly      F      30      66      124              5      5      Fran      F      33      66      115              6      6      Gwen      F      26      64      121              7      7      Hank      M      30      71      158              8      8      Ivan      M      53      72      175              9      9      Jake      M      32      69      143              10      10      Kate      F      47      69      139              11      11      Luke      M      34      72      163              12      12      Myra      F      23      62      98              13      13      Neil      M      36      75      160              14      14      Omar      M      38      70      145              15      15      Page      F      31      67      135              16      16      Quin      M      29      71      176              17      17      Ruth      F      28      65      131      df                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131        Method 2: 使用 set_index()df.set_index('Name', inplace=True)df                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      loc[]使用 loc[] 配合 index label 取出資料非常方便。\如果是 number index, 可以用 df[0], df[3], etc.\但如果是其他 column index, e.g. Name, df[2] 或是 df[“Hank”] are wrong!, 必須用 df.loc[‘Hank’]\或是 df.loc[ [‘Hank’, ‘Ruth’, ‘Page’] ]df.loc['Hank']Sex               MAge              30Height (in)      71Weight (lbs)    158Name: Hank, dtype: objectdf.loc[:, ['Sex', 'Age']]                  Sex      Age              Name                              Alex      M      41              Bert      M      42              Carl      M      32              Dave      M      39              Elly      F      30              Fran      F      33              Gwen      F      26              Hank      M      30              Ivan      M      53              Jake      M      32              Kate      F      47              Luke      M      34              Myra      F      23              Neil      M      36              Omar      M      38              Page      F      31              Quin      M      29              Ruth      F      28      df.loc[ ['Hank', 'Ruth', 'Page'] ]                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Hank      M      30      71      158              Ruth      F      28      65      131              Page      F      31      67      135      loc[] 可以用 row, column 得到對應的 element, 似乎是奇怪的用法df.loc['Hank', 'Age']30iloc[]使用 column index 仍然可以用 iloc[] 配合 index number 取出資料。df.iloc[0]Sex               MAge              41Height (in)      74Weight (lbs)    170Name: Alex, dtype: objectdf.iloc[1:10]                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143      df.iloc[[1, 4, 6]]                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Bert      M      42      68      166              Elly      F      30      66      124              Gwen      F      26      64      121      排序包含兩種排序  sort_index()  sort_value()df.sort_index()                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.sort_values(by = 'Age')                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Myra      F      23      62      98              Gwen      F      26      64      121              Ruth      F      28      65      131              Quin      M      29      71      176              Elly      F      30      66      124              Hank      M      30      71      158              Page      F      31      67      135              Carl      M      32      70      155              Jake      M      32      69      143              Fran      F      33      66      115              Luke      M      34      72      163              Neil      M      36      75      160              Omar      M      38      70      145              Dave      M      39      72      167              Alex      M      41      74      170              Bert      M      42      68      166              Kate      F      47      69      139              Ivan      M      53      72      175      Rename and Drop Column(s) and Index(s)df.rename(columns={"Height (in)": "Height", "Weight (lbs)": "Weight"}, inplace=True)df                  Sex      Age      Height      Weight              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.rename(index={"Alex": "Allen", "Bert": "Bob"}, inplace=True)df                  Sex      Age      Height      Weight              Name                                          Allen      M      41      74      170              Bob      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.drop(labels=['Sex', 'Weight'], axis="columns") # axis=1 eq axis="columns"                  Age      Height              Name                              Allen      41      74              Bob      42      68              Carl      32      70              Dave      39      72              Elly      30      66              Fran      33      66              Gwen      26      64              Hank      30      71              Ivan      53      72              Jake      32      69              Kate      47      69              Luke      34      72              Myra      23      62              Neil      36      75              Omar      38      70              Page      31      67              Quin      29      71              Ruth      28      65      df.drop(labels=['Allen', 'Ruth'], axis="index") # axis=0 eq axis="index"                  Sex      Age      Height      Weight              Name                                          Bob      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176      進階技巧Multiple Index (多重索引)這是非常有用的技巧，使用 set_index with keysdf = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', skipinitialspace=True)df  # show the original dataframe                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      df.set_index(keys = ['Name', 'Sex'])  # Notice "Name" "Sex" columns header is lower than the rest                        Age      Height (in)      Weight (lbs)              Name      Sex                                    Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.set_index(keys = ['Sex', 'Name'], inplace=True)df# Note that key sequence matters; and same index values group# Note that inplace=True replaces the original df # This is useful to display sorted group                        Age      Height (in)      Weight (lbs)              Sex      Name                                    M      Alex      41      74      170              Bert      42      68      166              Carl      32      70      155              Dave      39      72      167              F      Elly      30      66      124              Fran      33      66      115              Gwen      26      64      121              M      Hank      30      71      158              Ivan      53      72      175              Jake      32      69      143              F      Kate      47      69      139              M      Luke      34      72      163              F      Myra      23      62      98              M      Neil      36      75      160              Omar      38      70      145              F      Page      31      67      135              M      Quin      29      71      176              F      Ruth      28      65      131      df.indexMultiIndex([('M', 'Alex'),            ('M', 'Bert'),            ('M', 'Carl'),            ('M', 'Dave'),            ('F', 'Elly'),            ('F', 'Fran'),            ('F', 'Gwen'),            ('M', 'Hank'),            ('M', 'Ivan'),            ('M', 'Jake'),            ('F', 'Kate'),            ('M', 'Luke'),            ('F', 'Myra'),            ('M', 'Neil'),            ('M', 'Omar'),            ('F', 'Page'),            ('M', 'Quin'),            ('F', 'Ruth')],           names=['Sex', 'Name'])df.index.namesFrozenList(['Sex', 'Name'])type(df.index)  # MultiIndexpandas.core.indexes.multi.MultiIndexdf.sort_index(inplace=True)df# sorting is based on "Sex", and then "Name"                        Age      Height (in)      Weight (lbs)              Sex      Name                                    F      Elly      30      66      124              Fran      33      66      115              Gwen      26      64      121              Kate      47      69      139              Myra      23      62      98              Page      31      67      135              Ruth      28      65      131              M      Alex      41      74      170              Bert      42      68      166              Carl      32      70      155              Dave      39      72      167              Hank      30      71      158              Ivan      53      72      175              Jake      32      69      143              Luke      34      72      163              Neil      36      75      160              Omar      38      70      145              Quin      29      71      176      Groupby CommandGroupby 是 SQL 的語法。根據某一項資料做分組方便查找。\The SQL GROUP BY StatementThe GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns.df = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', index_col="Name", skipinitialspace=True)df  # show the dataframe with "Name" index column                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      grpBySex = df.groupby('Sex')  # output is a DataFrameGroupBy objecttype(grpBySex)pandas.core.groupby.generic.DataFrameGroupBygrpBySex.groups  # output is a dict, use get_group() obtains each sub-group{'F': Index(['Elly', 'Fran', 'Gwen', 'Kate', 'Myra', 'Page', 'Ruth'], dtype='object', name='Name'), 'M': Index(['Alex', 'Bert', 'Carl', 'Dave', 'Hank', 'Ivan', 'Jake', 'Luke', 'Neil',        'Omar', 'Quin'],       dtype='object', name='Name')}grpBySex.size()  # size() shows the counts of each groupSexF     7M    11dtype: int64grpBySex.get_group('M')  # get_group() output a DataFrame object                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Luke      M      34      72      163              Neil      M      36      75      160              Omar      M      38      70      145              Quin      M      29      71      176      Groupby Operation分組後可以進行各類運算：sum(), mean(), max(), min()grpBySex.sum()                  Age      Height (in)      Weight (lbs)              Sex                                    F      218      459      863              M      406      784      1778      grpBySex.mean()                  Age      Height (in)      Weight (lbs)              Sex                                    F      31.142857      65.571429      123.285714              M      36.909091      71.272727      161.636364      grpBySex.max()                  Age      Height (in)      Weight (lbs)              Sex                                    F      47      69      139              M      53      75      176      grpBySex.min()                  Age      Height (in)      Weight (lbs)              Sex                                    F      23      62      98              M      29      68      143      Wash Data with NAN判斷 NAN  isnull()  notnull()處理 NAN  dropna()  fillna()import numpy as npimport pandas as pdgroups = ["Modern Web", "DevOps", np.nan, "Big Data", "Security", "自我挑戰組"]ironmen = [59, 9, 19, 14, 6, np.nan]ironmen_dict = {                "groups": groups,                "ironmen": ironmen}# 建立 data frameironmen_df = pd.DataFrame(ironmen_dict)print(ironmen_df.loc[:, "groups"].isnull()) # 判斷哪些組的組名是遺失值print("---") # 分隔線print(ironmen_df.loc[:, "ironmen"].notnull()) # 判斷哪些組的鐵人數不是遺失值ironmen_df_na_dropped = ironmen_df.dropna() # 有遺失值的觀測值都刪除print(ironmen_df_na_dropped)print("---") # 分隔線ironmen_df_na_filled = ironmen_df.fillna(0) # 有遺失值的觀測值填補 0print(ironmen_df_na_filled)print("---") # 分隔線ironmen_df_na_filled = ironmen_df.fillna({"groups": "Cloud", "ironmen": 71}) # 依欄位填補遺失值print(ironmen_df_na_filled)0    False1    False2     True3    False4    False5    FalseName: groups, dtype: bool---0     True1     True2     True3     True4     True5    FalseName: ironmen, dtype: bool       groups  ironmen0  Modern Web     59.01      DevOps      9.03    Big Data     14.04    Security      6.0---       groups  ironmen0  Modern Web     59.01      DevOps      9.02           0     19.03    Big Data     14.04    Security      6.05       自我挑戰組      0.0---       groups  ironmen0  Modern Web     59.01      DevOps      9.02       Cloud     19.03    Big Data     14.04    Security      6.05       自我挑戰組     71.0PlotDataFrame 一個很重要的特性是利用 matplotlib.pyplot 繪圖功能 visuallize data!\有兩種方式：(1) 直接用 df.plot; (2) 用 pyplot 的 plot.\(1) 是一個 quick way to plot \(2) 可以調用 pyplot 所有的功能import matplotlib.pyplot as pltdf = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', index_col="Name", skipinitialspace=True)df.plot(title="Generated Plot", grid=True, figsize=(8,4))&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f952bc52240&gt;df.columnsplt.plot(df[['Age', 'Height (in)']])plt.xlabel('Name')plt.ylabel('Number')plt.title('Generated Plot')plt.grid()]]></content>
      <categories>
        
          <category> Language </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> pandas </tag>
        
          <tag> DataFrame </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RNN]]></title>
      <url>/foo/2018/12/22/rnn/</url>
      <content type="text"><![CDATA[import sysprint(sys.version)import tensorflowprint(tensorflow.__version__)import kerasprint(keras.__version__)import pandas as pdimport numpy as npimport mathimport randomimport matplotlib.pyplot as plt%matplotlib inline3.6.9 (default, Nov  7 2019, 10:44:02) [GCC 8.3.0]The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic:more info.1.15.02.2.5Using TensorFlow backend.Generate a sin wave with no noiseFirst, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves!Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles.def noisy_sin(steps_per_cycle = 50,              number_of_cycles = 500,              random_factor = 0.4):    '''    number_of_cycles : The number of steps required for one cycle        Return :     pd.DataFrame() with column sin_t containing the generated sin wave     '''    random.seed(0)    df = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=["t"])    df["sin_t"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)+ random.uniform(-1.0, +1.0) * random_factor))    df["sin_t_clean"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)))    print("create period-{} sin wave with {} cycles".format(steps_per_cycle,number_of_cycles))    print("In total, the sin wave time series length is {}".format(steps_per_cycle*number_of_cycles+1))    return(df)steps_per_cycle = 10df = noisy_sin(steps_per_cycle=steps_per_cycle,              random_factor = 0)n_plot = 8df[["sin_t"]].head(steps_per_cycle * n_plot).plot(      title="Generated first {} cycles".format(n_plot),      figsize=(15,3))create period-10 sin wave with 500 cyclesIn total, the sin wave time series length is 5001&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4922f362e8&gt;Create a training and testing data. Here, the controversial “length of time series” parameter comes into play. For now, we set this parameter to 2.def _load_data(data, n_prev = 100):      """    data should be pd.DataFrame()    """    docX, docY = [], []    for i in range(len(data)-n_prev):        docX.append(data.iloc[i:i+n_prev].as_matrix())        docY.append(data.iloc[i+n_prev].as_matrix())    alsX = np.array(docX)    alsY = np.array(docY)    return alsX, alsYlength_of_sequences = 2test_size = 0.25ntr = int(len(df) * (1 - test_size))df_train = df[["sin_t"]].iloc[:ntr]df_test  = df[["sin_t"]].iloc[ntr:](X_train, y_train) = _load_data(df_train, n_prev = length_of_sequences)(X_test, y_test)   = _load_data(df_test, n_prev = length_of_sequences)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.  /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.  if __name__ == '__main__':(3748, 2, 1) (3748, 1) (1249, 2, 1) (1249, 1)Simple RNN modelAs a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit.  The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias  The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for biasIn total, there are only 5 weights in this model.Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as:Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to “remember” the past history of $x_t$ through the hidden unit $h_t$.batch_shape needs for BPTT.¶  Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data.  This means that the each batch is treated as independent.  This batch_shape determines the size of this subset.  Every batch starts will the initial hidden unit $h_0=0$.  As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value.  The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!!Now, we define this model using Keras and show the model summary.from keras.layers import Inputfrom keras.models import Modelfrom keras.layers.core import Dense, Activation from keras.layers.recurrent import SimpleRNNdef define_model(length_of_sequences, batch_size = None, stateful = False):    in_out_neurons = 1    hidden_neurons = 1    inp = Input(batch_shape=(batch_size,                 length_of_sequences,                 in_out_neurons))      rnn = SimpleRNN(hidden_neurons,                     return_sequences=False,                    stateful = stateful,                    name="RNN")(inp)    dens = Dense(in_out_neurons,name="dense")(rnn)    model = Model(inputs=[inp],outputs=[dens])        model.compile(loss="mean_squared_error", optimizer="rmsprop")        return(model,(inp,rnn,dens))## use the default values for batch_size, statefulmodel, (inp,rnn,dens) = define_model(length_of_sequences = X_train.shape[1])model.summary()WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 2, 1)              0         _________________________________________________________________RNN (SimpleRNN)              (None, 1)                 3         _________________________________________________________________dense (Dense)                (None, 1)                 2         =================================================================Total params: 5Trainable params: 5Non-trainable params: 0_________________________________________________________________Now we train the model. The script was run without GPU.hist = model.fit(X_train, y_train, batch_size=600, epochs=1000,                  verbose=False,validation_split=0.05)WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.Plot of val_loss and loss.The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001.for label in ["loss","val_loss"]:    plt.plot(hist.history[label],label=label)plt.ylabel("loss")plt.xlabel("epoch")plt.title("The final validation loss: {}".format(hist.history["val_loss"][-1]))plt.legend()plt.show()The plot of true and predicted sin waves look nearly identicaly_pred = model.predict(X_test)plt.figure(figsize=(19,3))plt.plot(y_test,label="true")plt.plot(y_pred,label="predicted")plt.legend()plt.show()What are the model weights?The best way to understand the RNN model is to create a model from scratch. Let’s extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers.ws = {}for layer in model.layers:    ws[layer.name] = layer.get_weights()ws{'RNN': [array([[-0.43695387]], dtype=float32),  array([[-0.64668506]], dtype=float32),  array([0.00117508], dtype=float32)], 'dense': [array([[-3.7658346]], dtype=float32),  array([-0.00123706], dtype=float32)], 'input_1': []}What are the predicted values of hidden units?Since we used Keras’s functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest.In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model “rnn_model” shares the weights obtained by the previous model’s optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations.rnn_model = Model(inputs=[inp],outputs=[rnn])rnn_model.compile(loss="mean_squared_error", optimizer="rmsprop")hidden_units = rnn_model.predict(X_test).flatten()Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave.upto = 100predicted_sin_wave = ws["dense"][0][0][0]*hidden_units + ws["dense"][1][0]plt.figure(figsize=(19,3))plt.plot(y_test[:upto],label="y_pred")plt.plot(hidden_units[:upto],label="hidden units")plt.plot(predicted_sin_wave[:upto],"*",         label="w2 * hidden units + b2")plt.legend()plt.show()Obtain predicted sin wave at the next time point given the current sin wave by handWe understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as:def RNNmodel(ws,x,h=0):    '''    ws: predicted weights     x : scalar current sign value    h : scalar RNN hidden unit     '''               h = np.tanh(x*ws["RNN"][0][0][0] + h*ws["RNN"][1][0][0] + ws["RNN"][2][0])    x = h*ws["dense"][0][0][0] + ws["dense"][1][0]        return(x,h)Naturally, you can obtain the predicted sin waves $(x_1,x_2,…,x_t)$ by looping around RNNmodel as:$x^∗{t+1},h{t+1} = RNNmodel (x_t,h_t)$Here $x^∗_t$ indicates the estimated value of $x$ at time point $t$. As our model is not so complicated, we can readily implement this algorithm as:upto = 50 ## predict the first  sin valuesxstars, hs_hand = [], []for i, x in enumerate(df_test.values):    if i == 0:        h = 0 ## initial hidden layer value is zero        xstar = x        print("initial value of sin x_0 = {}, h_0 = {}".format(x,h))    hs_hand.append(h)    xstars.append(xstar[0])    xstar, h = RNNmodel(ws,x, h)assert len(df_test.values) == len(xstars)initial value of sin x_0 = [-1.27375647e-13], h_0 = 0In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_testplt.figure(figsize=(18,3))plt.plot(df_test.values[:upto],label="true",alpha=0.3,linewidth=5)plt.plot(xstars[:upto],label="sin prediction (xstar)")plt.plot(hs_hand[:upto],label="hidden state (xstar)")plt.legend()&lt;matplotlib.legend.Legend at 0x7f490d2fe908&gt;You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on?"validation loss {:3.2f}".format(np.mean((np.array(xstars) - df_test["sin_t"].values)**2))'validation loss 0.08'Let’s predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as:x1,x2x2,x3x3,x4…y_test_from_keras = model.predict(X_test).flatten()Notice that this predicted values are exactly the same as the ones calculated before.np.all(predicted_sin_wave == y_test_from_keras)TrueAs the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars.y_test_from_keras = [np.NaN, np.NaN] + list(y_test_from_keras.flatten())h_test_from_keras = [np.NaN, np.NaN] + list(hidden_units.flatten())The plot shows that Keras’s predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Keras’s prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output?plt.figure(figsize=(18,3))plt.plot(df_test.values[:upto],label="true",alpha=0.3,linewidth=5)plt.plot(xstars[:upto],label="sin prediction (xstar)")plt.plot(hs_hand[:upto],label="hidden state (xstar)")plt.plot(y_test_from_keras[:upto],label="sin prediction (keras)")plt.plot(h_test_from_keras[:upto],label="hidden state (keras)")plt.legend()print("validation loss {:6.5f}".format(np.nanmean((np.array(y_test_from_keras) - df_test["sin_t"].values)**2)))validation loss 0.00021Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role.As the BPTT algorithm only passed back 2 steps, the model assumes that:the hidden units are initialized to zero every 2 steps.the prediction of the next sin value (xt+1) is based on the hidden unit (ht) which is created by updating the hidden units twice in the past assuming that ht−1=0.x∗t,ht=RNNmodel(xt−1,0)xt+1,−=RNNmodel(xt,ht)Note that the intermediate predicted sin x∗t based on ht−1=0 should not be used as the predicted sin value. This is because the x∗t was not directly used to evaluate the loss function.Finally, obtain the Keras’s predicted sin wave at the next time point given the current sin wave by hand.def myRNNpredict(ws,X):    X = X.flatten()    h = 0    for i in range(len(X)):        x,h = RNNmodel(ws,X[i],h)    return(x,h)xs, hs = [], []for i in range(X_test.shape[0]):    x, h = myRNNpredict(ws,X_test[i,:,:])    xs.append(x)    hs.append(h)print("All sin estimates agree with ones from Keras = {}".format(    np.all(np.abs( np.array(xs) - np.array(y_test_from_keras[2:]) ) &lt; 1E-5)))print("All hidden state estmiates agree with ones fome Keras = {}".format(    np.all(np.abs( np.array(hs) - np.array(h_test_from_keras[2:]) ) &lt; 1E-5)) )All sin estimates agree with ones from Keras = TrueAll hidden state estmiates agree with ones fome Keras = TrueNow we understand how Keras is predicting the sin wave.In fact, Keras has a way to return xstar as predicted values, using “stateful” flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself.In order to predict in “stateful” mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1.model_stateful,_ = define_model(length_of_sequences = 1,                               batch_size=1,                               stateful = True)model_stateful.summary()Model: "model_3"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_2 (InputLayer)         (1, 1, 1)                 0         _________________________________________________________________RNN (SimpleRNN)              (1, 1)                    3         _________________________________________________________________dense (Dense)                (1, 1)                    2         =================================================================Total params: 5Trainable params: 5Non-trainable params: 0_________________________________________________________________Assign the trained weights into the stateful model.for layer in model.layers:            for layer_predict in model_stateful.layers:        if (layer_predict.name == layer.name):            layer_predict.set_weights(layer.get_weights())            breakNow we predict in stateful mode. Here it is very important to reset_state() before the prediction so that h0=0.pred = df_test.values[0][0]stateful_sin = []model_stateful.reset_states()for i in range(df_test.shape[0]):    stateful_sin.append(pred)    pred = model_stateful.predict(df_test.values[i].reshape(1,1,1))[0][0]    stateful_sin = np.array(stateful_sin)print("All predicted sin values with stateful model agree to xstars = {}".format(    np.all(np.abs(np.array(stateful_sin) - np.array(xstars))&lt; 1E-5)))All predicted sin values with stateful model agree to xstars = TrueNow we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example.However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say K? Do we always have to go back all the K time steps, set ht−K=0 and then feed forward K steps in order to predict at the time point t? This may be computationally intense.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Poincare Conjecture/Theorem and Ricci Flow]]></title>
      <url>/foo/2018/12/22/test/</url>
      <content type="text"><![CDATA[Introduction之前學 group theory 和 tensor calculus, 總結到平直空間的量子場論。最簡單的是 QED 的 Lagrangian 如下為純量，具有 U(1) 對稱性，對應各種守恆律。以及不同路徑對時間積分滿足最小作用原理。可以由 QED Lagrangian 推導非量子場論近似解 Maxwell equations. Maxwell equations 可以解釋所有的電磁現象，但無法解釋光量子效應例如光電效應，黑體輻射，雷射等等。就像可以從愛因斯坦場方程式推導近似解牛頓萬有引力定律。把 tensor calculus 從 Euclidean (differential) geometry 推廣到 Riemannian (differential) geometry, 可以連結到廣義相對論。以下是愛因斯坦場方程式：右手 $T_{\mu\nu}$ 是 energy-momentum tensor, 二階張量，代表 mass-energy distribution. 左手 $G_{\mu\nu}$ 是 Einstein tensor, 也是二階張量，代表 space-time curvature, 基本是 $R_{\mu\nu}$ (Ricci curvature tensor) 減去一個修正項。多出的修正項 $ 1/2 R g_{\mu\nu}$ 項：$R$ 是 scalar curvature (trace of Ricci curvature tensor), $g_{\mu\nu}$ is metric tensor.  當初愛因斯坦寫下的場方程式並沒有這一項: (1)違反 local conservation of energy-momentum. 也就是 energy flow is not preserved [@wikiHistoryGeneral2019];（2）無法得到座標系無關形式，違反(馬赫)廣義相對性原理。愛因斯坦求助於 Hilbert.  在 Hilbert 的協助下，找到這個修正項。如果 $T_{\mu\nu}$ 隨時間變化，例如兩個黑洞旋轉合併，會改變時空曲率。時空曲率又會反過來影響質能分佈 and vice versa, 因而產生時空漣漪，一般稱為引力波。如同 Maxwell equation 的電場變化產生磁場 and vice versa, 因而產生電磁波。Tensor Calculus 和 Differential Geometry 能夠用於 Quantum Field Theory and General Relativity 兩大物理學，已經是非常幸福。 更幸福的是可以用於 Topology 的 Poincare conjecture (now theorem proved by Perelman).  這部分我們 follow Hamilton’s direction using Ricci flow. [@hamiltonRichardHamilton]Laplacian Operator and Heat Equation這部分可以參考前文【】。我們從座標無關的張量定義拉普拉斯算子：$\Delta = \nabla\cdot\nabla$, 或是 diverge of gradient of a scalar or vector field. 以上的定義不只用於歐氏幾何，也適用黎曼幾何。熱傳導 (heat diffusion) 上式是 manifold 固定，只是定義在 manifold 上的純量場 (e.g. 勢能場，溫度場) 隨時間和空間變化，但是整體 volume 不變（守恆量），對應一個 flow。Ricci Flow = 愛因斯坦場方程式 + 拉普拉斯熱傳導Hamilton 則是考慮 manifold 本身隨時間變化。1981 引入 Ricci flow. 觀念上非常類似上述的熱傳導。但直接用於 manifold (intrinsic) 而非其上的 (extrinsic) field.  非常開創性而且具物理性直觀性！看了 Hamilton 2006 Youtube 的演講 [@hamiltonRichardHamilton2006], 他也許不是第一個把 PDE (Partial Differential Equation) 用於 topology. 但是第一個引入 Ricci flow, 結合分析和拓墣，對於 topology 非常具體實用 (N-manifold, not only 2 or 3).  拓墣可以大量借用 PDE 的理論，甚至可以用計算機協助。就像笛卡爾引入直角座標系結合代數和幾何。Hamilton 高度評價 Perelman 在 Ricci flow 的貢獻，不像某一些文章暗示 Hamilton 對 Perelman 有心結。Perelman 在拒絕 Fields medal 也高度評價 Hamilton 在 Ricci flow 的創見。兩人在專業領域應該是互相佩服。Hamilton 提出的 Ricci Flow 如下。果然是數學家的公式，非常簡潔。其實就是張量版的熱傳導方程式！$R_{ij}$ 代表 manifold 的 intrinsic curvature, 基本是 Christoffel symbol 的空間一階導數 [@ListFormulas2019]。and Christoffel symbol 是 metric tensor 的空間一階導數因此 $R_{ij}$ 基本是 metric tensor $g_{ij}$ 的空間二階導數。這和拉普拉斯算子的功能一致。等式的右手則是 metric tensor 對時間一階導數。因此 Ricci flow equation 類似拉普拉斯熱傳導公式。隨時間改變 manifold 的 metric tensor, Christoffel tesnor, curvature tensor.熟悉愛因斯坦場方程式者會想到修正項。Yes! 這稱為 normalized Ricci flow.Normalized Ricci flow 的定義如下 [@wikiRicciFlow2019]：where $R_{avg}$ is the average (mean) of the scalar curvature (which is the trace of Ricci tensor), n is the dimension of the manifold.The normalized equation preserves the volume of the metric space.  這一句話就是加上中間這一項才能保持 volume 不變。這是 “(incompressible) flow” 的基本條件。這修正項和愛因斯坦廣義場方程式基本一致 (n=4)，滿足場方程式座標系無關，也就是廣義相對性原理。基本原則是 metric tensor, Christoffel tensor, curvature tensor exponentially decay.  Ricci flow 的負號會讓不穩定的負曲率 (3-manifold 雙曲面) 只會短暫出現。  大的正曲率（非常彎 3-manifold 橢圓曲面）也會很快 decay。  最後由小的正曲率（平緩 3-manifold 橢圓曲面）dominate manifold 的變化。  Ricci flow 變化 manifold 過程中，拓墣特性不變 (invariant)，就是同胚！可以用於證明 Poincare theorem.  Volume (area for 2-manifold) is preserved? Yes for normalized Ricci flow; No for Ricci flow.  A good way to think of the normalized Ricci flow is that it’s the same as Ricci flow but you rescale every time-slice to make the volume constant. Maybe also reparametrize time to make the equation nicer if you feel like it. Of course, isometries are still isometries after a metric gets rescaled.  下圖是一個 2D surface/manifold 的 Ricci flow 變化 surface/manifold 的過程。因為是 Ricci flow, surface area is not preserved.Poincare Conjecture/Theorem回到 Poincare conjecture [@PoincareConjecture2019]. 先從最基本的 2D surface 開始，比較直觀。A compact 2-dimensional surface (2D manifold) without boundary is topologically homeomorphic to a 2-sphere if every loop can be continuously tightened to a point.更簡潔的說法Every simply connected, closed (i.e. no boundary and compact) 2-manifold is homeomorphic to the 2-sphere.基本上如果一個 2D surface 任何一個 loop 可以連續收斂到一個點，2D surface 必定和球面同胚，如上圖。再看 2D torus (環面) 如下圖。沒有 boundary, 存在兩種 loops (red and pink) 都無法收斂到一個點。因此 2D torus 和球面不同胚。任何一個 loop 可以連續收斂到一個點 = 沒有破洞 = 單連通翻譯成中文：任一單連通的、封閉的二維流形與二維球面同胚。The Poincaré conjecture asserts that the same is true for 3-dimensional as follows!Every simply connected, closed (i.e. no boundary and compact) 3-manifold is homeomorphic to the 3-sphere.翻譯成中文：任一單連通、封閉的三維流形與三維球面同胚。###如何想像單連通、封閉的三維流形？對於處於三維歐氏空間的我們，可以看到封閉的二維流形（如各種球面，環面，Klein bottle, etc.）我們可以想像有邊界的三維流形，但是很難想像封閉的三維流形。這需要四維空間的視角才能想像。但對於簡單封閉三維流形，我們可以展開降維到三維歐氏空間。以下用 2D 骰子面（和 2D 球面同胚）來類比。參考數學女孩龐加萊猜想。2D 骰子面是單連通、封閉的二維曲面，和二維球面同胚。為什麼用 2D 骰子面？因為 3D cube (embed 2D 骰子面)可以展開成 6 個 2D 正方形在 2D 歐氏平面。每一個正方形的 4 邊，都和 4 個正方形相鄰。因此一個 2D 曲面的生物 (毛毛蟲)，只要遵循相鄰的規則，可以一直移動不會離開 2D 骰子面。也就是具有封閉性。把 2D 骰子面推廣到 3D 骰子體（和 3D 超球面同胚）。原則上要在 4D 歐氏空間才能想像。可以用下圖左近似 4D hypercube。可以展開成 8 個 3D 立方體 (cube), 每一個 3D cube 的 6 面，都和 6 個（上下左右前後）3D cube 相鄰。因此一個 3D 生物（人），只要遵循相鄰的規則，可以一直移動不會離開 3D 骰子體。也就是具有封閉性。Why Poincare Conjecture is Important？首先聽起來很基本且重要。的確這是拓墣學一個基本問題。事實上，在 2 維和大於等於 4 維流形，本命題都已證明維真。只有在 3 維流形，也就是 Poincare conjecture, 一直到 Perelman 在 2006 才證明 Poincare conjecture.更重要的是 1982 Thurston 提出 geometrization conjecture (now theorem) 猜測所有封閉的三維流形 (3-manifold) 可以分解為 8 種基本幾何結構，3-sphere 是其中之一。[@wikiGeometrizationConjecture2019]類似有 uniformization theorem 適用於二維流形 (2-manifold): 所有單連通的二維流形（球面）一定是 3 種曲面之一（Euclidean, spherical, or hyperbolic).Strategy to Prove Poincare ConjectureHamilton 1981 提出 Ricci flow 的思路：  對於單連通、封閉 3-manifold 作為初始條件, $g_{ij}(0)$, 施加 Ricci flow deforms 3-manifold.  Ricci flow 變化 manifold 過程中，manifold 拓墣特性不變 (invariant)，就是同胚！  Ricci flow 的負號會讓不穩定的負曲率只會短暫出現。大的正曲率也會很快 decay.  最後由小的正曲率 dominate manifold 的變化。最後趨近 3-sphere.  因此證明單連通、封閉 3-manifold 和 3-sphere 同胚，也就是 Poincare conjecture.Hamilton 在 Ricci flow 的貢獻：[@hamiltonRichardHamilton2006]  正曲率的 2/3-manifold 在 finite time 收斂到一點 (singularity with curvature $\to\infty$)。但 normalize (area/volume) 之後收斂到 2/3-sphere，就是 2/3-sphere 同胚。等效於使用 normalized Ricci flow to preserve volume (?).  2-manifold 啞鈴 (1 “neck” with positive and negative curvature) 或是多個 “neck” 如圖一在 finite time 收斂到一點。  因此 2-manifold 可以很容易用 Ricci flow 證明和 2-sphere 同胚。這是簡單的牛刀小試。  3-manifold with neck 就跟複雜，會產生 “neck pinch” singularity.  Hamilton 提出 Ricci flow with surgery to cut off large curvature portion and solve the singularity to converge to 3-sphere.  Hamilton 的父親是真的外科醫生。  但存在 cigar (2-manifold) or other 3-manifold soliton 過程永遠保持形狀不變，無法收斂到 3-sphere.Perelman 解決 Hamilton Ricci-flow 的漏洞。  Improve the surgery to completely solve singularity.  From transport equation to make soliton 無法產生。  Prove geometrization conjecture, Poincare conjecture 基本是一個子定理。ReferenceHamilton, Richard, dir. 2006. Richard Hamilton | the PoincareConjecture | 2006. https://www.youtube.com/watch?v=fymCXcIt20g.Wiki. 2019a. “Ricci Flow.” Wikipedia.https://en.wikipedia.org/w/index.php?title=Ricci_flow&amp;oldid=920777616.———. 2019b. “History of General Relativity.” Wikipedia.https://en.wikipedia.org/w/index.php?title=History_of_general_relativity&amp;oldid=931327622.———. 2019c. “Geometrization Conjecture.” Wikipedia.https://en.wikipedia.org/w/index.php?title=Geometrization_conjecture&amp;oldid=932572904.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Next Theme Tutorial]]></title>
      <url>/tutorial/2017/07/20/next-tutorial/</url>
      <content type="text"><![CDATA[  NexT is a high quality elegant Jekyll theme ported from Hexo Next. It is crafted from scratch, with love.Live PreviewScreenshots      Desktop        Sidebar    Sidebar (Post details page)  MobileInstallationCheck whether you have Ruby 2.1.0 or higher installed:ruby --versionInstall Bundler:gem install bundlerClone Jacman theme:git clone https://github.com/Simpleyyt/jekyll-theme-next.gitcd jekyll-theme-nextInstall Jekyll and other dependencies from the GitHub Pages gem:bundle installRun your Jekyll site locally:bundle exec jekyll serverMore Details：Setting up your GitHub Pages site locally with JekyllFeaturesMultiple languages support, including: English / Russian / French / German / Simplified Chinese / Traditional Chinese.Default language is English.language: en# language: zh-Hans# language: fr-FR# language: zh-hk# language: zh-tw# language: ru# language: deSet language field as following in site _config.yml to change to Chinese.language: zh-HansComment support.NexT has native support for DuoShuo and Disqus comment systems.Add the following snippets to your _config.yml:duoshuo:  enable: true  shortname: your-duoshuo-shortnameORdisqus_shortname: your-disqus-shortnameSocial MediaNexT can automatically add links to your Social Media accounts:social:  GitHub: your-github-url  Twitter: your-twitter-url  Weibo: your-weibo-url  DouBan: your-douban-url  ZhiHu: your-zhihu-urlFeed link.  Show a feed link.Set rss field in theme’s _config.yml, as the following value:  rss: false will totally disable feed link.      rss:   use sites’ feed link. This is the default option.    Follow the installation instruction in the plugin’s README. After the configuration is done for this plugin, the feed link is ready too.    rss: http://your-feed-url set specific feed link.Up to 5 code highlight themes built-in.NexT uses Tomorrow Theme with 5 themes for you to choose from.Next use normal by default. Have a preview about normal and night:Head over to Tomorrow Theme for more details.ConfigurationNexT comes with few configurations.# Menu configuration.menu:  home: /  archives: /archives# Faviconfavicon: /favicon.ico# Avatar (put the image into next/source/images/)# can be any image format supported by web browsers (JPEG,PNG,GIF,SVG,..)avatar: /default_avatar.png# Code highlight theme# available: normal | night | night eighties | night blue | night brighthighlight_theme: normal# Fancybox for image galleryfancybox: true# Specify the date when the site was setupsince: 2013Browser support]]></content>
      <categories>
        
          <category> tutorial </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Highlight Test]]></title>
      <url>/test/2017/07/19/highlight-test/</url>
      <content type="text"><![CDATA[This is a highlight test.Normal blockalert('Hello World!');print 'helloworld'Highlight blockalert( 'Hello, world!' );print 'helloworld'def foo  puts 'foo'enddef foo  puts 'foo'end123def foo  puts 'foo'end#include &lt;iostream&gt;using namespace std;void foo(int arg1, int arg2){}int main(){  string str;  foo(1, 2);  cout &lt;&lt; "Hello World" &lt;&lt; endl;  return 0;}]]></content>
      <categories>
        
          <category> Test </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Emoji Test]]></title>
      <url>/2015/09/19/emoji-test/</url>
      <content type="text"><![CDATA[This is an emoji test. :smile: lol.See emoji cheat sheet for more detail :wink: : https://www.webpagefx.com/tools/emoji-cheat-sheet/.:bowtie::smile::laughing::blush::smiley::relaxed::smirk::heart_eyes::kissing_heart::kissing_closed_eyes::flushed::relieved::satisfied::grin:]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Gallery Post]]></title>
      <url>/photo/2014/11/18/gallery-post/</url>
      <content type="text"><![CDATA[Nunc dignissim volutpat enim, non sollicitudin purus dignissim id. Nam sit amet urna eu velit lacinia eleifend. Proin auctor rhoncus ligula nec aliquet. Donec sodales molestie lacinia. Curabitur dictum faucibus urna at convallis. Aliquam in lectus at urna rutrum porta. In lacus arcu, molestie ut vestibulum ut, rhoncus sed eros. Sed et elit vitae risus pretium consectetur vel in mi. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi tempus turpis quis lectus rhoncus adipiscing. Proin pulvinar placerat suscipit. Maecenas imperdiet, quam vitae varius auctor, enim mauris vulputate sapien, nec laoreet neque diam non quam.Etiam luctus mauris at mi sollicitudin quis malesuada nibh porttitor. Vestibulum non dapibus magna. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Proin feugiat hendrerit viverra. Phasellus sit amet nunc mauris, eu ultricies tellus. Sed a mi tortor, eleifend varius erat. Proin consectetur molestie tortor eu gravida. Cras placerat orci id arcu tristique ut rutrum justo pulvinar. Maecenas lacinia fringilla diam non bibendum. Aenean vel viverra turpis. Integer ut leo nisi. Pellentesque vehicula quam ut sapien convallis consequat. Aliquam ut arcu purus, eget tempor purus. Integer eu tellus quis erat tristique gravida eu vel lorem.]]></content>
      <categories>
        
          <category> Photo </category>
        
      </categories>
      <tags>
        
          <tag> consectetur </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MathJax with Jekyll]]></title>
      <url>/opinion/2014/02/16/Mathjax-with-jekyll/</url>
      <content type="text"><![CDATA[One of the rewards of switching my website to Jekyll is theability to support MathJax, which means I can write LaTeX-like equations that getnicely displayed in a web browser, like this one \( \sqrt{\frac{n!}{k!(n-k)!}} \) orthis one \( x^2 + y^2 = r^2 \).What’s MathJax?If you check MathJax website (www.mathjax.org) you’ll seethat it is an open source JavaScript display engine for mathematics that works in allbrowsers.How to implement MathJax with JekyllI followed the instructions described by Dason Kurkiewicz forusing Jekyll and Mathjax.Here are some important details. I had to modify the Ruby library for Markdown inmy _config.yml file. Now I’m using redcarpet so the corresponding line in theconfiguration file is: markdown: redcarpetTo load the MathJax javascript, I added the following lines in my layout post.html(located in my folder _layouts)&lt;script type="text/javascript"    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;Of course you can choose a different file location in your jekyll layouts.Note that by default, the tex2jax preprocessor defines theLaTeX math delimiters, which are \\(...\\) for in-line math, and \\[...\\] fordisplayed equations. It also defines the TeX delimiters $$...$$ for displayedequations, but it does not define $...$ as in-line math delimiters. To enable in-line math delimiter with $...$, please use the following configuration:&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config({  tex2jax: {    inlineMath: [['$','$'], ['\\(','\\)']],    processEscapes: true  }});&lt;/script&gt;&lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;A Couple of ExamplesHere’s a short list of examples. To know more about the details behind MathJax, you canalways checked the provided documentation available athttp://docs.mathjax.org/en/latest/Let’s try a first example. Here’s a dummy equation:How do you write such expression? Very simple: using double dollar signs$$a^2 + b^2 = c^2$$To display inline math use \\( ... \\) like this \\( sin(x^2) \\) which getsrendered as \( sin(x^2) \)Here’s another example using type \mathsf$$ \mathsf{Data = PCs} \times \mathsf{Loadings} $$which gets displayed asOr even better:\\[ \mathbf{X} = \mathbf{Z} \mathbf{P^\mathsf{T}} \\]is displayed as\[ \mathbf{X} = \mathbf{Z} \mathbf{P^\mathsf{T}} \]If you want to use subscripts like this \( \mathbf{X}_{n,p} \) you need to scape theunderscores with a backslash like so \mathbf{X}\_{n,p}:$$ \mathbf{X}\_{n,p} = \mathbf{A}\_{n,k} \mathbf{B}\_{k,p} $$will be displayed as\[ \mathbf{X}_{n,p} = \mathbf{A}_{n,k} \mathbf{B}_{k,p} \]]]></content>
      <categories>
        
          <category> opinion </category>
        
      </categories>
      <tags>
        
          <tag> resources </tag>
        
          <tag> jekyll </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Images]]></title>
      <url>/2013/12/27/images/</url>
      <content type="text"><![CDATA[This is a image test post.]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Excerpts]]></title>
      <url>/2013/12/25/excerpts/</url>
      <content type="text"><![CDATA[The following contents should be invisible in home/archive page.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce eget urna vitae velit eleifend interdum at ac nisi. In nec ligula lacus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eu cursus erat, ut dapibus quam. Aliquam eleifend dolor vitae libero pharetra adipiscing. Etiam adipiscing dolor a quam tempor, eu convallis nulla varius. Aliquam sollicitudin risus a porta aliquam. Ut nec velit dolor. Proin eget leo lobortis, aliquam est sed, mollis mauris. Fusce vitae leo pretium massa accumsan condimentum. Fusce malesuada gravida lectus vel vulputate. Donec bibendum porta nibh ut aliquam.Sed lorem felis, congue non fringilla eu, aliquam eu eros. Curabitur orci libero, mollis sed semper vitae, adipiscing in lectus. Aenean non egestas odio. Donec sollicitudin nisi quis lorem gravida, in pharetra mauris fringilla. Duis sit amet faucibus dolor, id aliquam neque. In egestas, odio gravida tempor dictum, mauris felis faucibus purus, sit amet commodo lacus diam vitae est. Ut ut quam eget massa semper sodales. Aenean non ipsum cursus, blandit lectus in, ornare odio. Curabitur ultrices porttitor vulputate.]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Block]]></title>
      <url>/foo/2013/12/25/block/</url>
      <content type="text"><![CDATA[This post is used for testing tag plugins. See docs for more info.Block QuoteNormal blockquote  Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna.Code BlockInline code blockThis is a inline code block: python, print 'helloworld'.Normal code blockalert('Hello World!');print "Hello world"Highlight code blockprint "Hello world"def foo  puts 'foo'end123def foo  puts 'foo'endGist]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[日本語テスト]]></title>
      <url>/2013/12/25/%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%83%86%E3%82%B9%E3%83%88/</url>
      <content type="text"><![CDATA[This is a Japanese test post.私は昨日ついにその助力家というのの上よりするたなけれ。最も今をお話団はちょうどこの前後なかろでくらいに困りがいるたをは帰着考えたなかって、そうにもするでうたらない。がたを知っないはずも同時に九月をいよいよたありた。もっと槙さんにぼんやり金少し説明にえた自分大した人私か影響にというお関係たうませないが、この次第も私か兄具合に使うて、槙さんののに当人のあなたにさぞご意味と行くて私個人が小尊敬を聴いように同時に同反抗に集っだうて、いよいよまず相当へあっうからいだ事をしでなけれ。  それでそれでもご時日をしはずはたったいやと突き抜けるますて、その元がは行ったてという獄を尽すていけですた。この中道具の日その学校はあなたごろがすまなりかとネルソンさんの考えるですん、辺の事実ないというご盲従ありたですと、爺さんのためが薬缶が結果までの箸の当時してならて、多少の十月にためからそういう上からとにかくしましないと触れべきものたで、ないうですと多少お人達したのでたた。From すぐ使えるダミーテキスト - 日本語 Lorem ipsum]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[中文測試]]></title>
      <url>/test/test/2013/12/25/%E4%B8%AD%E6%96%87%E6%B8%AC%E8%A9%A6/</url>
      <content type="text"><![CDATA[This is a Chinese test post.善我王上魚、產生資西員合兒臉趣論。畫衣生這著爸毛親可時，安程幾？合學作。觀經而作建。都非子作這！法如言子你關！手師也。以也座論頭室業放。要車時地變此親不老高小是統習直麼調未，行年香一？就竟在，是我童示讓利分和異種百路關母信過明驗有個歷洋中前合著區亮風值新底車有正結，進快保的行戰從：弟除文辦條國備當來際年每小腳識世可的的外的廣下歌洲保輪市果底天影；全氣具些回童但倒影發狀在示，數上學大法很，如要我……月品大供這起服滿老？應學傳者國：山式排只不之然清同關；細車是！停屋常間又，資畫領生，相們制在？公別的人寫教資夠。資再我我！只臉夫藝量不路政吃息緊回力之；兒足灣電空時局我怎初安。意今一子區首者微陸現際安除發連由子由而走學體區園我車當會，經時取頭，嚴了新科同？很夫營動通打，出和導一樂，查旅他。坐是收外子發物北看蘭戰坐車身做可來。道就學務。國新故。  工步他始能詩的，裝進分星海演意學值例道……於財型目古香亮自和這乎？化經溫詩。只賽嚴大一主價世哥受的沒有中年即病行金拉麼河。主小路了種就小為廣不？From 亂數假文產生器 - Chinese Lorem Ipsum]]></content>
      <categories>
        
          <category> test/test </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam justo turpis, tincidunt ac convallis id.]]></title>
      <url>/foo/2013/12/25/long-title/</url>
      <content type="text"><![CDATA[This post has a long title. Make sure the title displayed right.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
          <tag> Foo </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Categories]]></title>
      <url>/foo/bar/baz/2013/12/25/categories/</url>
      <content type="text"><![CDATA[This post contains 3 categories. Make sure your theme can display all of the categories.]]></content>
      <categories>
        
          <category> Foo </category>
        
          <category> Bar </category>
        
          <category> Baz </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Link Post]]></title>
      <url>/foo/2013/12/25/link-post/</url>
      <content type="text"><![CDATA[This is a link post. Clicking on the link should open Google in a new tab or window.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tags]]></title>
      <url>/foo/2013/12/25/tags/</url>
      <content type="text"><![CDATA[This post contains 3 tags. Make sure your theme can display all of the tags.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
          <tag> Foo </tag>
        
          <tag> Bar </tag>
        
          <tag> Baz </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Elements]]></title>
      <url>/foo/2013/12/25/elements/</url>
      <content type="text"><![CDATA[The purpose of this post is to help you make sure all of HTML elements can display properly. If you use CSS reset, don’t forget to redefine the style by yourself.Heading 1Heading 2Heading 3Heading 4Heading 5Heading 6ParagraphLorem ipsum dolor sit amet, test link consectetur adipiscing elit. Strong text pellentesque ligula commodo viverra vehicula. Italic text at ullamcorper enim. Morbi a euismod nibh. Underline text non elit nisl. Deleted text tristique, sem id condimentum tempus, metus lectus venenatis mauris, sit amet semper lorem felis a eros. Fusce egestas nibh at sagittis auctor. Sed ultricies ac arcu quis molestie. Donec dapibus nunc in nibh egestas, vitae volutpat sem iaculis. Curabitur sem tellus, elementum nec quam id, fermentum laoreet mi. Ut mollis ullamcorper turpis, vitae facilisis velit ultricies sit amet. Etiam laoreet dui odio, id tempus justo tincidunt id. Phasellus scelerisque nunc sed nunc ultricies accumsan.Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed erat diam, blandit eget felis aliquam, rhoncus varius urna. Donec tellus sapien, sodales eget ante vitae, feugiat ullamcorper urna. Praesent auctor dui vitae dapibus eleifend. Proin viverra mollis neque, ut ullamcorper elit posuere eget.  Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna.Maecenas ornare arcu at mi suscipit, non molestie tortor ultrices. Aenean convallis, diam et congue ultricies, erat magna tincidunt orci, pulvinar posuere mi sapien ac magna. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Praesent vitae placerat mauris. Nullam laoreet ante posuere tortor blandit auctor. Sed id ligula volutpat leo consequat placerat. Mauris fermentum dolor sed augue malesuada sollicitudin. Vivamus ultrices nunc felis, quis viverra orci eleifend ut. Donec et quam id urna cursus posuere. Donec elementum scelerisque laoreet.List TypesDefinition List (dl)Definition List TitleThis is a definition list division.Ordered List (ol)  List Item 1  List Item 2  List Item 3Unordered List (ul)  List Item 1  List Item 2  List Item 3Table            Table Header 1      Table Header 2      Table Header 3                  Division 1      Division 2      Division 3              Division 1      Division 2      Division 3              Division 1      Division 2      Division 3      Misc Stuff - abbr, acronym, sub, sup, etc.Lorem superscript dolor subscript amet, consectetuer adipiscing elit. Nullam dignissim convallis est. Quisque aliquam. cite. Nunc iaculis suscipit dui. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus. Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. NBA Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus.  AVE]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
