<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[Math ML - Modified Softmax w/ Margin]]></title>
      <url>/ai/2021/01/16/softmax/</url>
      <content type="text"><![CDATA[Math ML - Modified Softmax w/ Margin[@rashadAdditiveMargin2020] and [@liuLargeMarginSoftmax2017]Softmax classification æ˜¯é™³å¹´æŠ€è¡“ï¼Œå¯é‚„æ˜¯æœ‰äººåœ¨è€å¹¹ä¸Šé•·å‡ºæ–°æžã€‚å…¶ä¸­ä¸€é¡žæ˜¯åœ¨ softmax åŠ ä¸Š maximum margin æ¦‚å¿µ (sometimes refers to metric learning), å¦ä¸€é¡žæ˜¯åœ¨ softmax æ‰€æœ‰ dataset ä¸­æ‰¾å‡º â€œsupporting vectorsâ€ æ¸›å°‘ computation å»ä¸å¤±æº–ç¢ºçŽ‡ã€‚å¯¦éš›åšæ³•éƒ½æ˜¯å¾žä¿®æ”¹ loss function è‘—æ‰‹ã€‚æœ¬æ–‡èšç„¦åœ¨ç¬¬ä¸€é¡žå¢žåŠ  margin çš„ ç®—æ³•ã€‚Softmax in DL or ML RecapSoftmax æœ€å¸¸ç”¨æ–¼ DL (i.e. deep layers) ç¥žç¶“ç¶²çµ¡æœ€å¾Œä¸€å±¤(å¹¾å±¤)çš„ multi-class classification å¦‚ä¸‹åœ–ã€‚and  Input vector, $\mathbf{x}$, dimension $n\times 1$.  Weight matrix, $\mathbf[w_1â€™, w_2â€™, .., w_Kâ€™]â€™$, dimension $K\times n$  Output vector, $\mathbf{z}$, dimension $K\times 1$.  Softmax output vector, $0\le\sigma(j)\le 1, j=[1:K]$, dimension $K\times 1$.  æ³¨æ„ bias å¦‚æžœæ˜¯ä¸€å€‹ fixed number, $b$, softmax åˆ†å­åˆ†æ¯æœƒæŠµéŠ·ã€‚bias å¦‚æžœä¸åŒ $b_1, b_2, â€¦, b_n$ï¼Œå¯ä»¥æ“´å±• $\mathbf{xâ€™ = [x, }1]$ and $\mathbf{wâ€™_j = [w_j}, b_j]$, åŒæ¨£å¦‚å‰é©ç”¨ã€‚Softmax ä¹Ÿå¸¸ç”¨æ–¼ ML (i.e. shallow layers) çš„ multi-class classification, å¸¸å’Œ SVM ä¸€èµ·æ¯”è¼ƒã€‚ç‚ºäº†è™•ç† nonlinear dataset or decision boundary, Softmax + kernel method æ˜¯ä¸€å€‹é¸é …ã€‚Softmax å¦å¤–ç”¨æ–¼ attention network, TBD.Parameter Notation and Range for ML and DL  $N$: number of data points.  100 to 10,000 for ML, &gt; 1M for DL.  $n$: input vector dimension. maybe from 1~ to 100~ for ML, 1000-4000 for DL.  $K$ or $m$ or $C$: output vector dimension, number of classes, maybe from 1 (binary) to 100 (Imaginet)  $k$: kernel feature space dimension, maybe from 10sâ€™ - $\infty$ for ML.  Usually not use for DL.Summarize the result in table.            Â       N      n      k      K                  ML      100-10,000      1sâ€™- 100sâ€™      10sâ€™- $\infty$      1sâ€™-10sâ€™              DL      &gt; 1M      1000-4000      NA      10-100      Softmax w/ Margin Via Trainingæ ¹æ“šå‰æ–‡è¨Žè«–ï¼Œ$w_i$ vectors ä»£è¡¨å’Œ class i data çš„ç›¸ä¼¼æ€§ã€‚æ™®é€šçš„ softmax classification å¦‚ä¸‹åœ–å·¦æ‰€ç¤ºã€‚Decision boundary æ˜¯ data point å’Œ $w_1$ and $w_2$ çš„æ©ŸçŽ‡ä¸€æ¨£ã€‚å› ç‚º softmax (or logistic regression) åªè¦æ±‚ $\sigma_1(x) &gt; \sigma_2(x)$ or vice versa to classify $x \in$ class 1 (or class 2).  é€™è£¡å®Œå…¨æ²’æœ‰ margin çš„è§€å¿µã€‚æŽ¨å»£åˆ° multiple class æ›´æ˜¯å¦‚æ­¤ï¼Œå¦‚ä¸‹åœ–ã€‚å› ç‚ºæ˜¯å– $\sigma(j)$ çš„æœ€å¤§å€¼ã€‚é™¤äº† $\sigma(j) &gt; 0.5$ æœ‰æ˜Žé¡¯çš„æ­¸é¡žã€‚ä½†åœ¨ä¸‰ä¸ç®¡åœ°å¸¶ï¼Œå¾ˆå¯èƒ½é›œéŒ¯åœ¨ä¸€èµ·ã€‚å› çˆ² training æ˜¯åŸºæ–¼ loss function, è§£æ³•æ˜¯åœ¨ loss function åŠ å…¥ margin term åšç‚º driving force (check the back-prop gradient!), è®“ training process ç«­ç›¡æ‰€èƒ½ â€œæ“ å‡ºâ€ margin, å¦‚ä¸Šåœ–å³ã€‚å¦‚ä½•åœ¨ softmax åŠ å…¥ margin for trainingSVM æ˜¯å¾ž decision boundary çš„å¹³è¡Œç·šè·é›¢è‘—æ‰‹ï¼ˆmargin = 1/|w|, minimize |w| ~ maximum margin)ã€‚æœ¬æ–‡è¨Žè«– Softmax åŠ ä¸Š margin æœ‰ä¸‰ç¨®æ–¹å¼ï¼Œéƒ½æ˜¯å¾žè§’åº¦ $\theta$ è‘—æ‰‹ï¼Œæ¦‚å¿µå¦‚åœ–äºŒå³ (å¹³é¢è§’åº¦)ï¼Œæˆ–æ˜¯ä¸‹åœ–å³ (çƒé¢è§’åº¦)ã€‚maximize $\theta$ å‰›å¥½å’Œ minimize |w| æ­£äº¤ (orthogonal). é€™æ˜¯å·§åˆå—Žï¼Ÿæˆ‘å€‘å…ˆçœ‹ Softmax çš„ loss function å¦‚ä¸‹åœ–ã€‚å…ˆæ˜¯ softmax function, inference/test åªè¦ å†ä¾†é€šéŽ cross-entropy loss.  Cross-entropy loss å°æ‡‰ log likelihood. where $f_{y_{i}}=\boldsymbol{W}{y{i}}^{T} \boldsymbol{x}{i}$ ä»£è¡¨ data $x_i$ å’Œ $W{y_i}$ çš„ç›¸ä¼¼æ€§ã€‚ä¸‰ç¨®ç”¨è§’åº¦å¢žåŠ  SoftMax inter-class margin  L-Softmax (Large Margin Softmax) [@liuLargeMarginSoftmax2017]  A-Softmax (Angular Softmax) [@liuSphereFaceDeep2018]  AM-Softmax (Additive Margin Softmax)L-Softmax (Large Margin Softmax): $\cos \theta \to \cos (m\theta)$å› ç‚º $f_{j}=\left| \boldsymbol{W_j} \right|\left| \boldsymbol{x_i} \right|\cos\left(\theta_{j}\right)$.  å¦‚ä½•åœ¨ $x_i$ å’Œ $W_j$ åŠ ä¸Š marginï¼Ÿ  ä¸€å€‹æ–¹æ³•å°±æ˜¯æŠŠ $\cos \theta$ æ”¹æˆ $\cos m\theta$, why?å¾žç›¸ä¼¼æ€§ä¾†çœ‹ï¼Œ$\cos(m\theta)$ åœ¨åŒæ¨£çš„è§’åº¦â€ç›¸ä¼¼æ€§â€æŽ‰çš„æ¯”è¼ƒå¿«ã€‚å› æ­¤åœ¨ training æ™‚æœƒå¼·è¿«æŠŠåŒä¸€ feature çš„ data æ“ å£“åœ¨ä¸€èµ·, reduce the intra-class distance. é”åˆ°å¢žåŠ  inter-class margin çš„ç›®çš„ã€‚å¦å¤–å¯ä»¥å¾ž decision boundary ç†è§£ã€‚Softmax çš„ decision boundary,$x\in$ Class 1:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right)&gt;\left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$$x\in$ Class 2:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right) &lt; \left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$and $\theta_1 + \theta_2 = \theta$ which is the angle between $W_1$ and $W_2$å¦‚æžœæŠŠ $\cos \theta \to \cos (m\theta)$,$x\in$ Class 1:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( m\theta_{1}\right)&gt;\left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(\theta_{2}\right)$.â€¨Assuming $|W_1| = |W_2| \to \theta_1 &lt; \theta_2/m$, å› ç‚º $\cos\theta$ æ˜¯éžæ¸›å‡½æ•¸ã€‚$x\in$ Class 2:  $\left|\boldsymbol{W_1}\right||\boldsymbol{x}| \cos \left( \theta_{1}\right) &lt; \left|\boldsymbol{W_2}\right||\boldsymbol{x}| \cos \left(m\theta_{2}\right)$.Assuming $|W_1| = |W_2| \to \theta_1/m &gt; \theta_2$.æ­¤æ™‚æˆ‘å€‘æœ‰å…©å€‹ decision boundaries, å…©å€‹ boundaries ä¹‹é–“å¯ä»¥è¦–ç‚º decision margin, å¦‚ä¸‹åœ–ã€‚In summary, å°±æ˜¯åœ¨ labelled $c$ class çš„ data æ™‚ï¼Œå°±æŠŠå°æ‡‰çš„ $\cos\theta_c$ æ”¹æˆ $\cos (m\theta_c)$. $m$ æ„ˆå¤§ï¼Œmargin å°±æ„ˆå¤§ã€‚ä½†éŽä¹‹çŒ¶å¦‚ä¸åŠï¼Œå¦‚æžœ $m$ å¤ªå¤§ï¼Œå¯èƒ½ç„¡æ³•æ­£ç¢º capture features (TBC)? $m$ æ‡‰è©²æœ‰ä¸€å€‹ optimal value.ç‚ºä»€éº¼æœƒæœ‰ $D(\theta)$ï¼Ÿ åŽŸå› æ˜¯è¦ç¶­æŒ $\psi(\theta)$ çš„éžæ¸›æ€§ï¼Œé€£çºŒæ€§ï¼Œå’Œå¯å¾®åˆ†æ€§ over $[0, \pi]$.  ä¸€æ—¦å®šç¾©å‡º $\psi(\theta)$ over $[0, \pi]$. å·¦å³ flip (y è»¸å°ç¨±) å¾—åˆ° $\theta\in[-\pi, 0]$. å…¶ä»–çš„ $\theta$ éƒ½å¯ä»¥ç§»åˆ° $[-\pi, \pi]$.èˆ‰ä¸€å€‹ä¾‹å­å¦‚ä¸‹å¼ï¼Œ$\psi(\theta)$ çš„ curve å¦‚ä¸‹åœ–ã€‚A-Softmax (Angular Softmax): $\cos \theta \to \cos (m\theta)$ and $|W|=1$åœ¨ L-Softmax å¯ä»¥åŒæ™‚èª¿æ•´ $|W|$ and $\theta$, åœ¨ A-Softmax é€²ä¸€æ­¥é™åˆ¶ $|W|=1$, å…¶ä»–éƒ½å’Œ L-Softmax ç›¸åŒã€‚A-Soft çš„ Loss function å¦‚ä¸‹ï¼Œ å¾Œä¾†æœ‰å†ä¿®æ­£ $\psi(\theta)$, å¤šåŠ ä¸€å€‹ hyper-parameter $\lambda$, angle similarity curve å¦‚ä¸‹åœ–ã€‚æ³¨æ„ A-Softmax çš„ $\psi(0)=1.$å› ç‚º $|W|=1$, A-Softmax ä¸€å€‹ç”¨é€”æ˜¯ hyper-sphere explanation å¦‚ä¸‹åœ–ã€‚ç†è«–ä¸Š L-Softmax åŒ…å« A-Softmax, ä½†åœ¨æŸä¸€äº›æƒ…æ³ä¸‹ï¼ŒA-Softmax ä¼¼ä¹Žæ•ˆæžœæ›´å¥½ï¼Œless is more? (åŒä¸€ä½œè€…ï¼Œ2017 L-SoftMax; 2018 A-Softmax).AM-Softmax (Additive Margin Softmax): $\cos \theta \to \cos \theta -m$AM-Softmax éžå¸¸æœ‰è¶£ï¼Œå®ƒæŠŠ $\cos\theta \to \cos(m\theta) \to \cos\theta -m$, ä¹Ÿå°±æ˜¯ï¼ŒAM-Softmax çš„ loss function, ä½†å¤šäº†ä¸€å€‹ hyper-parameter $s$(?)é€™æœ‰å¾ˆå¤šå¥½è™•ï¼š  ä¸ç”¨å†åˆ†æ®µç®— $\psi(\theta)$, forward and backward è¨ˆç®—è®Šæˆå¾ˆå®¹æ˜“ã€‚  $m$ æ˜¯ continuous variable, ä¸æ˜¯ discrete variable in A-Softmax. $m$ å¯ä»¥ fine-grain optimized hyper-parameter. è€Œä¸”æ˜¯ differentiable, æˆ‘èªç‚ºå¯ä»¥æ˜¯ trainable variable.  AM-Softmax åŒæ™‚ push angle and magnitude?Q&amp;AQ. Data ä¸æ˜¯å›ºå®šçš„å—Žï¼Ÿç‚ºä»€éº¼æœƒéš¨ loss function æ”¹è®Šï¼ŸA. æ­¤è™•æ˜¯å‡è¨­ CNN network çš„æœ€å¾Œä¸€å±¤æ˜¯ Softmax, å› æ­¤ input data å°æ‡‰çš„ feature extraction ä¸¦éžå›ºå®šè€Œä¸”æœƒéš¨ loss function æ”¹è®Šå¦‚ä¸‹åœ–ã€‚å¦‚æžœ input data ç›´æŽ¥é€²å…¥ Softmax with or without margin, the input data é¡¯ç„¶ä¸æœƒæ”¹è®Šï¼Œä½†æ˜¯ decision boundary may change? (next Q)Q. åœ¨ inference/test æ™‚ï¼Œä»¥ä¸Šçš„å…¬å¼ (check class $c$) åŠ èµ·ä¾†ä¸ç­‰æ–¼ 1ï¼Ÿ å¦‚ä½•è§£æ±ºï¼ŸA: ä»¥ä¸Šçš„å…¬å¼åªç”¨æ–¼ training å¢žåŠ  margin? åœ¨ inference/test æ™‚ï¼Œä»ç„¶ç”¨åŽŸä¾†çš„ softmax å…¬å¼ï¼Œå› æ­¤æ©ŸçŽ‡ä»ç„¶ç‚º 1.Q. ä»¥ä¸Š $cos(m \theta)$ çš„ $m$ ä¸€å®šè¦æ•´æ•¸å—Žï¼ŸA. æ•´æ•¸å¯ä»¥å®šç¾© continuous and differentiable loss function in $0-\pi$ è§’åº¦ã€‚ä¸Šä¸Šåœ–çš„è§’åº¦é¡¯ç¤º $0-\pi/2$ è§’åº¦ï¼Œ$\pi/2 - \pi$ æ˜¯ $0-\pi/2$ çš„å·¦å³ flip curve.  å¦‚æžœ $m$ ä¸æ˜¯æ•´æ•¸ï¼Œåœ¨ $\pi/2$ is non-differentiable.  å¦å¤–ä¹Ÿè®“ loss function çš„åˆ†æ®µæ¯”è¼ƒéº»ç…©ã€‚ä¸éŽæˆ‘èªç‚ºé€™éƒ½ä¸æ˜¯ä»€éº¼å•é¡Œã€‚é‡é»žæ˜¯ $m$ ä¸æ˜¯æ•´æ•¸æœ‰æ²’æœ‰ç”¨ï¼Ÿ æˆ‘èªç‚ºæœ‰ç”¨ï¼Œå¯ä»¥è¦–ç‚ºå¦ä¸€å€‹ hyper-parameter, or trainable parameter for optimization!  $m$ å¤ªå°æ²’æœ‰ margin, $m$ å¤ªå¤§æœƒ filter out some features (under-fit)?ç­–ç•¥ï¼šåŒæ™‚ä½¿ç”¨è§’åº¦ maximize $\theta$ and Magnitude minimize $|w|$ï¼Magnitude margin: å¢žåŠ  inter-class margin?Angle margin: compress intra-class?å…ˆ push è§’åº¦ï¼Œå† push w, å†è§’åº¦, â€¦.è§’åº¦ m, make it differentiable!To Do  check the SVM, check the logistic regression, check import vector  Use binary classification as an example  Pro and Con of the three types.  Most importantly, try to use both amplitude and angle for learning!!  TBDReferenceLiu, Weiyang, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and LeSong. 2018. â€œSphereFace: Deep Hypersphere Embedding for FaceRecognition.â€ January 29, 2018. http://arxiv.org/abs/1704.08063.Liu, Weiyang, Yandong Wen, Zhiding Yu, and Meng Yang. 2017.â€œLarge-Margin Softmax Loss for Convolutional Neural Networks.â€ November17, 2017. http://arxiv.org/abs/1612.02295.Rashad, Fathy. n.d. â€œAdditive Margin Softmax Loss (AM-Softmax).â€ Medium.Accessed December 27, 2020.https://towardsdatascience.com/additive-margin-softmax-loss-am-softmax-912e11ce1c6b.Wang, Feng, Weiyang Liu, Haijun Liu, and Jian Cheng. 2018. â€œAdditiveMargin Softmax for Face Verification.â€ May 30, 2018.https://doi.org/10.1109/LSP.2018.2822810.]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> softmax </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Math AI - G-CNN (Group + CNN)]]></title>
      <url>/ai/2020/05/08/G-CNN/</url>
      <content type="text"><![CDATA[Math AI - G-CNN (Group + CNN)Where is group theory (G-CNN) + Curved Space (Spherical CNN)      Manifold learning æ˜¯æ©Ÿå™¨å­¸ç¿’çš„åˆ†æ”¯ï¼Œå±¬æ–¼æ·ºå±¤å­¸ç¿’ (shallow learning).  Manifold learning çš„æŠ€å·§ (kernel PCA?, Laplacian Eigenmap, etc.) æ˜¯å¦èƒ½ç”¨æ–¼æ·±åº¦å­¸ç¿’ï¼Ÿ Yes, via kernel!   PCA =&gt; CNN kernel;  LE etc. =&gt; geometric kernel?    Why çµåˆæ·±åº¦å­¸ç¿’å’Œ manifold learning?          æ·±åº¦å­¸ç¿’ based on CNN kernel =&gt; translation covariant (not invariant, invariant æ˜¯æŒ‡ç´”é‡ independent of coordinate system, e.g. Lagrangian, action, or $ds^2$.  Covariant means coordinate â€¦) on 2D Euclidean plane,  Need based on ??? kernel  =&gt; translation/rotation covariant on manifold  =&gt; çµåˆæ·±åº¦å­¸ç¿’å’Œ manifold learning      å¯ä»¥æ¸›å°‘ training set!  å› ç‚º manifold learning è‡ªå¸¶ translation/rotation covariant, ç”šè‡³å¯ä»¥ extend to manifold deformation (e.g.å§¿é«”ç§»å‹•?)  å¯ä»¥çµåˆ prior information? (å§¿é«”ç§»å‹•ï¼Œè›‹ç™½è³ªç§»å‹•,æ—‹è½‰,é¡åƒ â€¦)      Can this resist adversarial attack?        translation equivariant - CNN, plus rotation/mirror equivariant - g-CNN  then sphere equivariant - sphere CNN (non-flat); finally ??  How about scale invariant or equivariant?çµ‚æ–¼äº†è§£ G-CNN çš„æ„ç¾©ï¼Œå°±æ˜¯æŠŠ kernel 2D convolution (Z2 commutative group) expand to a 4D G-convolution (p4m: Di4 non-commutative group).  åªæœ‰ input image æ˜¯ (x, y) base, ç¶“éŽ layer-1 G-convolution è½‰ç‚º p4m g space.  æ‰€æœ‰ä¹‹å¾Œçš„ layersâ€™ convolution éƒ½æ˜¯åœ¨ g space åš, i.e. input and output activation éƒ½æ˜¯åœ¨ {4D g space + 1D Depth=5D} space instead of {2D (x,y) + 1D depth = 3D} space.  åˆ°äº†æœ€å¾Œ fully connected å†è®Šæˆåˆ†é¡žç¶²è·¯ã€‚  é€™çœŸæ˜¯ particle physicist æ‰æœƒæœ‰çš„é«˜ç¶­æ€ç¶­ï¼ä¸€èˆ¬äººé‚„æ˜¯ç¿’æ…£æ¯ä¸€å±¤ input output activation è€è€å¯¦å¯¦åœ¨ 2D (x,y) space.  (example: https://arxiv.org/pdf/1807.11156.pdf).  I like this idea: Go high dimension all the way!  In some sense, channel or depth dimension ä¹Ÿæ˜¯ä¸€å€‹äººé€ çš„ dimension!  More parameters?  Should be.  Still can find the (x,y) for location?  Yes, it is a superset!  Use 1D convolution with mirror group as an example.  How about broken symmetry? æˆ–æ˜¯ miss some kernel?å†æŽ¨å»£ Group Equivariance [@estevesPOLARTRANSFORMER2018]Equivariant representations are highly sought after as they encode both class and deformation information in a predictable way. Let $G$ be a transformation group and $L_g I$ be the group action applied to an image $I$. A mapping $\Phi : E \to F$ is said to be equivariant to the group action $L_g$, $g \in G$ ifwhere $L_g$ and $Lâ€™g$ correspond to application of $g$ to $E$ and $F$ respectively and satisfy $L{gh} = L_g L_h$ and $Lâ€™_{gh} = Lâ€™_g Lâ€™_h$.  Invariance is a special case of equivariance where $Lâ€™_g = I$.  Another special case is $L_g = Lâ€™_g$.  Image classification and CNN, $g \in G$ can be thought of as an image deformation and $\Phi$ a mapping from the image to a feature map.Next step:  Image $I$ is a function of coordinate, x, $I = f(x)$ at first layer.  Group operation on f(x) is  $L_g f(x) = f(g^{-1}x)$.  åŽŸå› å¾ˆç°¡å–®ï¼Œå°±æ˜¯åœ¨ $x = gxâ€™$ æœƒå¾—åˆ°åŽŸä¾†çš„ $f(xâ€™)$.  2D discrete convolution, $L_g f(x) = f\circ \phi $ å®šç¾©å¦‚ä¸‹ã€‚$x, y \in Z^2$  CNN 3D convolution, $L_g f(x) = f\circ \phi $ å®šç¾©å¦‚ä¸‹ã€‚$x, y \in Z^2$; $k$ and $i$ åˆ†åˆ¥ä»£è¡¨ input/output channel depth  æŽ¨å»£åˆ° 2D group convolution.  $g, h \in G$  æŽ¨å»£åˆ° 3D Group CNN or G-CNN.  $g, h \in G$, $k$ and $i$ åˆ†åˆ¥ä»£è¡¨ input/output channel depthConvolution and CNN å…·æœ‰ translational equivariance and independent of kernel $\phi$.  ç›´è§€è€Œè¨€ï¼Œå°±æ˜¯æŠŠ input image (or feature map) å’Œ kernel filter çš„ symmetry group (e.g. translation, rotation, reflection) åš similarity (inner product), ä½†ä¿ç•™å°è¨˜ (coordinate (x,y), reflection (m=1, -1), rotation (r=0, 1, 2, 3)) åˆ° output feature map.      2D convolution or 3D CNN çš„ $g^{-1} h = y-x$  and $h^{-1} g = (g^{-1} h)^{-1} = x-y$ æ˜¯ $Z^2$ åå…ƒç´ ã€‚    å…¶ä¸­ layer1 çš„ input image å› ç‚ºåªæœ‰ 2D coordinate (x,y) + 1D depth (c=3, e.g. RGB) = 3D tensor, ä½†æ˜¯ output feature map è®Šæˆ 2D coordinate (x,y) + 1D reflection(m) + 1D rotation(r) + 1D depth (c) = 5D tensor.  å…¶ä»– layers çš„ input and output éƒ½æ˜¯ 5D tensors.Group Equivariant Operationåƒè€ƒ [@prismGroupEquivariant2019] and [@cohenGroupEquivariant2019].åœ¨é€™ç¯‡æ–‡ç« ä¸­ï¼Œä½œè€…ä»¥åˆå­¸è€…çš„è§’åº¦ï¼Œå¾žæœ€åŸºæœ¬çš„æ¦‚å¿µé–‹å§‹ï¼Œè§£é‡‹å°ç¨±æ€§ä¸¦é€šä¿—åœ°å¼•å…¥ç¾¤è«–çš„ç†è«–æ¡†æž¶ã€‚æ‰€è¬‚å°ç¨±æ€§ï¼Œå°±æ˜¯ç›®æ¨™åœ¨ç¶“æ­·ä¸€å®šè®Šæ›ä»¥å¾Œä¿æŒä¸è®Šçš„æ€§è³ªã€‚è€Œé€™è£¡ç”¨åˆ°çš„å°ç¨±æ€§ç¾¤ï¼ˆsymmetry groupï¼‰ï¼Œå¯ç†è§£ç‚ºä¸€ç³»åˆ—æ»¿è¶³æŸäº›é™åˆ¶æ¢ä»¶çš„å°ç¨±æ€§è®Šæ›çš„é›†åˆã€‚ä¸‹é¢æ˜¯æ–‡ä¸­å°å°ç¨±æ€§ç¾¤çš„å®šç¾©ï¼šè€Œåœ¨å·ç©ç¶²çµ¡è£¡é¢æ¶‰åŠåˆ°çš„ï¼Œæœ€ç°¡å–®çš„ä¾‹å­å°±æ˜¯äºŒç¶­æ•´æ•¸å¹³ç§»æ“ä½œæ‰€çµ„æˆçš„ç¾¤ $\mathbb{Z}^2$ã€‚æŽ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘ç°¡å–®å›žé¡§ä¸€ä¸‹å‚³çµ±å·ç©ç¶²çµ¡çš„ç­‰è®Šï¼ˆEquivarianceï¼‰æ€§è³ªã€‚å¹³ç§»ç­‰è®Šæ€§è³ªæ˜¯CNNå°ç›®æ¨™çš„éŸ¿æ‡‰èƒ½å¤ ä¸å—ç›®æ¨™åœ¨åœ–åƒä¸­çš„ä½ç½®å½±éŸ¿çš„åŸºç¤Žã€‚ã€Šæ·±åº¦å­¸ç¿’ã€‹èŠ±æ›¸è£¡é¢æ˜¯é€™æ¨£æè¿°ç­‰è®Šæ€§è³ªï¼š  å¦‚æžœä¸€å€‹å‡½æ•¸æ»¿è¶³ï¼Œè¼¸å…¥æ”¹è®Šè€Œè¼¸å‡ºä¹Ÿä»¥åŒæ¨£çš„æ–¹å¼æ”¹è®Šçš„é€™ä¸€æ€§è³ªï¼Œæˆ‘å€‘å°±èªªå®ƒæ˜¯ç­‰è®Šçš„ã€‚ç°¡å–®çš„ä¾‹å­ï¼Œå°±æ˜¯ç•¶ç›®æ¨™å‡ºç¾åœ¨è¼¸å…¥åœ–ç‰‡ä¸­çš„ä¸åŒä½ç½®ï¼Œè¼¸å‡ºçš„feature mapæ‡‰è©²æ˜¯åªæ˜¯é€²è¡Œäº†å¹³ç§»è®Šæ›ã€‚è€Œå¾žæ•¸å­¸ä¸Šï¼Œå¾žç®—ç¬¦å¯¾æ˜“æ€§çš„è§’åº¦ï¼Œç­‰è®Šæ€§è³ªå¯ä»¥é€™æ¨£å®šç¾©ï¼šå°æ–¼ç¾¤å°ç¨± $g \in G$ ï¼Œå…¶ç®—ç¬¦ $L_g$ å’Œå‡½æ•¸ $f(x)$ï¼Œæœ‰ $f(L_g x) = L_g(f(x))$ ï¼Œä¹Ÿå°±æ˜¯ $f$ èˆ‡ $L_g$ å¯¾æ˜“ï¼Œå‰‡ç¨±$f(x)$ å°æ–¼è®Šæ› $g$ æœ‰ç­‰è®Šæ€§ã€‚åœ¨æ·±åº¦å­¸ç¿’ç•¶ä¸­ï¼Œæˆ‘å€‘æ›´å¸Œæœ›å·ç©ç¶²çµ¡å…·æœ‰ç­‰è®Šæ€§ï¼Œè€Œä¸æ˜¯ä¸è®Šæ€§ï¼ˆInvarianceï¼‰:åœ¨ç•¢å¡ç´¢çš„é€™å¹…ç•«ä¸­ï¼Œè‡‰éƒ¨äº”å®˜éƒ½åœ¨ï¼Œä½†æ˜¯é¡¯ç„¶è¢«è§£æ§‹å’Œèª¿æ•´ã€‚å¦‚æžœç¥žç¶“ç¶²çµ¡å°ç›®æ¨™çš„éŸ¿æ‡‰å…·æœ‰ã€Œä¸è®Šæ€§ã€ï¼Œé¡¯ç„¶ä»ç„¶æœƒèªç‚ºé€™å°±æ˜¯ä¸€å¼µæ™®é€šäººè‡‰ã€‚æŽ¥ä¸‹ä¾†ä½œè€…å¼•å…¥ä¸€å€‹çµè«–ï¼šé€™å€‹å…¬å¼çš„å«ç¾©æ˜¯ï¼šè¦å¾—åˆ°ç¶“éŽ [å…¬å¼] è®Šæ›çš„feature map [å…¬å¼] åœ¨ [å…¬å¼] è™•çš„å€¼ï¼Œå¯ä»¥é€šéŽè¨ˆç®—åœ¨ [å…¬å¼] ä½ç½®ä¸Šé¢ [å…¬å¼] çš„å€¼ã€‚èˆ‰ä¾‹ä¾†èªªï¼Œå¦‚æžœ [å…¬å¼] æ˜¯å¹³ç§»æ“ä½œtï¼Œå‰‡ [å…¬å¼] ï¼Œé‚£æˆ‘å€‘åªéœ€è¨ˆç®—åœ¨ [å…¬å¼] é€™ä¸€é»žfeatureçš„å€¼ä¾¿å¯å¾—åˆ°ã€‚é€™å€‹å…¬å¼å°‡åœ¨æŽ¨åˆ°ç­‰è®Šæ€§çš„æ™‚å€™ç”¨åˆ°ã€‚å°æ–¼å‚³çµ±å·ç©ç¶²çµ¡ï¼Œ [å…¬å¼] å‰‡å°æ‡‰å¹³ç§»æ“ä½œ [å…¬å¼] ã€‚ä¹Ÿå°±æ˜¯èªªï¼Œç”±æ–¼å¹³ç§»æ“ä½œé›–ç„¶æœƒå°å·ç©æ“ä½œçš„è¼¸å‡ºç”¢ç”Ÿæ”¹è®Šï¼Œä½†æ˜¯é€™ç¨®æ”¹è®Šæ˜¯ç·šæ€§çš„ï¼Œå¯ä»¥é æ¸¬çš„ã€‚åä¹‹ï¼Œä¸ç­‰è®Šçš„æ“ä½œå‰‡æœƒå°è¼¸å‡ºå¸¶ä¾†éžç·šæ€§çš„å½±éŸ¿ã€‚ç‚ºäº†è­‰æ˜Žå‚³çµ±å·ç©ç¶²çµ¡è£¡é¢ï¼Œå¹³ç§»èˆ‡å·ç©æ“ä½œå¯¾æ˜“ï¼Œé¦–å…ˆæ˜Žç¢ºå®šç¾©å‚³çµ±å·ç©æ“ä½œå’Œäº’ç›¸é—œæ“ä½œï¼šåœ¨é€™è£¡ï¼Œfilterå°è¼¸å…¥å±¤çš„æ»‘å‹•æŽƒæè¢«çœ‹åšå°å…¶å¹³ç§»æ“ä½œã€‚éœ€è¦æ³¨æ„çš„æ˜¯åœ¨å‚³çµ±çš„å·ç©ç¶²çµ¡è£¡é¢ï¼Œå‰å‘éŽç¨‹äº‹å¯¦ä¸Šç”¨çš„æ˜¯äº’ç›¸é—œæ“ä½œå»è¢«æ³›æ³›ç¨±ç‚ºã€Œå·ç©æ“ä½œã€ã€‚ç„¶å¾Œæ–‡ç« ä¸­å¾ˆå®¹æ˜“è­‰æ˜Žçž­äº’ç›¸é—œæ“ä½œ( [å…¬å¼] )å’Œå·ç©ï¼ˆ [å…¬å¼] ï¼‰æ“ä½œéƒ½èˆ‡å¹³ç§»æ“ä½œ [å…¬å¼] å¯¾æ˜“ï¼ˆcommuteï¼‰:[å…¬å¼][å…¬å¼]ç”±é€™å…©å€‹æ“ä½œå¯¾æ˜“ï¼Œå¾žè€Œå¾—å‡ºçµè«–ï¼šå·ç©æ˜¯å¹³ç§»æ“ä½œçš„ç­‰è®Šæ˜ å°„ã€‚å¦å¤–ä¸€æ–¹é¢ï¼Œä½œè€…ç™¼ç¾æ—‹è½‰æ“ä½œèˆ‡å·ç©æ“ä½œæ˜¯ä¸å¯¾æ˜“çš„ï¼Œã€Œcorrelation is not an equivariant map for the rotation groupã€ï¼Œä½†æ˜¯feature mapçš„å †ç–Šå»å¯èƒ½æ˜¯ç­‰è®Šçš„ã€‚ä¹Ÿæ­£æ˜¯å› ç‚ºæ—‹è½‰æ“ä½œä¸æ˜¯å·ç©çš„ç­‰è®Šæ˜ å°„ï¼Œå¾€å‚³çµ±çš„CNNè£¡é¢è¼¸å…¥æ—‹è½‰äº†çš„åœ–åƒï¼Œåœ–åƒè­˜åˆ¥çš„æ•ˆæžœå‰‡æœƒå¤§æ‰“æŠ˜æ‰£ã€‚ç‚ºçž­è§£æ±ºé€™å€‹å•é¡Œï¼Œæœ€å‚³çµ±ç›´æŽ¥çš„æ–¹æ³•æ˜¯æ•¸æ“šå¢žå¼·ï¼Œç›´æŽ¥æŠŠåœ–åƒæ—‹è½‰å†è¼¸å…¥ç¶²çµ¡é€²è¡Œè¨“ç·´ï¼Œä½†æ˜¯é€™ç¨®æ–¹æ³•é¡¯ç„¶ä¸æ˜¯æœ€å„ªçš„ã€‚ç‚ºäº†æ”¹é€²ç¶²çµ¡æœ¬èº«ä¾†è§£æ±ºé€™å€‹å•é¡Œï¼Œè€ƒæ…®ä¸€å€‹ç°¡å–®çš„å…·æœ‰å››é‡æ—‹è½‰å°ç¨±è»¸çš„å°ç¨±æ€§ç¾¤ [å…¬å¼] (wiki). å°æ–¼é€™å€‹ç¾¤ï¼Œæœ‰å››ç¨®å°ç¨±æ€§æ“ä½œï¼šå¹³ç§»ï¼Œæ—‹è½‰90Â°ï¼Œæ—‹è½‰180Â°ï¼Œæ—‹è½‰270Â°ã€‚æˆ‘å€‘è¦è¨­è¨ˆä¸€å€‹æ–°çš„CNNçµæ§‹ï¼Œä½¿å¾—ç•¶è¼¸å…¥åœ–åƒæœ‰ä»¥ä¸Šè®Šæ›æ™‚ï¼Œç¶²çµ¡ä»ç„¶å…·æœ‰ç­‰è®Šæ€§è³ªã€‚ç‚ºäº†é€™å€‹ç›®çš„ï¼Œä»¿ç…§(2)(3)ï¼Œæ ¹æ“š(1)çš„çµè«–ï¼Œä½œè€…æå‡ºçš„ G-correlationï¼Œå…¶å®šç¾©ç‚ºï¼šå°æ–¼ç¬¬ä¸€å±¤G-CNNï¼ˆfirst-layer G-correlationï¼‰ï¼Œ [å…¬å¼] å’Œ[å…¬å¼] å®šç¾©åœ¨å¹³é¢ [å…¬å¼] ä¸Šï¼š[å…¬å¼]å°æ–¼æŽ¥ä¸‹ä¾†çš„G-CNNå±¤ï¼ˆfull G-correlationï¼‰ï¼Œ [å…¬å¼] å’Œ[å…¬å¼] å®šç¾©åœ¨ç¾¤Gä¸Šï¼š[å…¬å¼]ç”±æ­¤å¸¶ä¾†çš„æ”¹è®Šæ˜¯ï¼Œä½œè€…å¾ˆå®¹æ˜“è­‰æ˜Žçž­G-CNNå°æ–¼ç¾¤Gçš„è®Šæ›æ“ä½œæ˜¯ç­‰è®Šçš„ï¼ˆã€ŒG-correlation is an equivariant map for the translation groupã€ï¼‰: [å…¬å¼]è©³ç´°æŽ¨å°Žè¦‹æ–‡ç« ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ ç¶“å¸«å¼Ÿæé†’ï¼Œå°ç¬¬ä¸€å±¤G-CNNçš„ç­‰è®Šæ€§æŽ¨åˆ°ï¼Œéœ€è¦æŠŠ [å…¬å¼] å’Œ [å…¬å¼] æ‹“å±•åˆ°ç¾¤ [å…¬å¼] ä¸Šï¼Œå¦å‰‡å°‡ç„¡æ³•æŽ¨å°Žï¼ˆå› ç‚º [å…¬å¼] é¡¯ç„¶ä¸å†å±¬æ–¼ç¾¤ [å…¬å¼] ï¼‰ã€‚ä¹Ÿå°±æ˜¯èªªï¼ŒG-CNNæŽ¨å»£äº†å°feature mapçš„è®Šæ›æ“ä½œï¼Œå¾žå‚³çµ±çš„åªæœ‰å¹³ç§»è®Šæ›çš„ç¾¤ [å…¬å¼] åˆ°æŸå€‹å°ç¨±æ€§ç¾¤ [å…¬å¼] ã€‚è€Œä¸”æŽ¨å»£ä»¥å¾Œï¼ŒG-CNNå·ç©å±¤å°æ–¼è©²ç¾¤çš„å°ç¨±æ€§è®Šæ›æ“ä½œå…·æœ‰ç­‰è®Šæ€§è³ªã€‚é›–ç„¶ä½œè€…åœ¨æ–‡ä¸­æ²’æœ‰æåŠï¼Œä¸é›£çœ‹åˆ°ï¼ŒG-CNNå¯ä»¥è‡ªç„¶é€€åŒ–åˆ°å‚³çµ±çš„CNNã€‚ç•¶å°ç¨±æ€§ç¾¤Gåªæœ‰å¹³ç§» [å…¬å¼] ä¸€ç¨®å°ç¨±æ€§æ“ä½œï¼Œä¹Ÿå°±æ˜¯ [å…¬å¼] æ™‚ï¼Œå‰‡G-CNNä¹Ÿå°±æ˜¯å‚³çµ±çš„CNNã€‚ç¸½è€Œè¨€ä¹‹ï¼Œç•¶è¼¸å…¥åœ–åƒæ˜¯æŒ‰ç…§ç‰¹å®šè§’åº¦æ—‹è½‰çš„ï¼ŒG-CNNç¶²çµ¡çš„è¼¸å‡ºçµæžœæ‡‰è©²æ˜¯æŒ‰ç…§é å®šè¦å¾‹è®ŠåŒ–çš„ã€‚å› æ­¤ï¼ŒG-CNNå…·å‚™äº†æ›´å¼·çš„æ—‹è½‰è¼¸å…¥åœ–åƒç‰¹å¾µæå–çš„èƒ½åŠ›ã€‚å¯ä»¥å®Œå…¨å¾ž math è§’åº¦ä¾†çœ‹æ·±åº¦å­¸ç¿’ã€‚CNN çš„æ ¸å¿ƒæ˜¯ convolution, math æŠ½è±¡ä¾†çœ‹æ˜¯ Euclidean translation invariance (Z^2).  æ›´é€²ä¸€æ­¥çš„æ˜¯ Euclidean rotation invariance (U(1), SO(2) group?).  æˆ–è€… manifold (sphere) translation/rotation invariance.Gauge Convolutional Networks[@xinzhiyuanGeometricalDeep2020] and [@pavlusIdeaPhysics2020]https://kknews.cc/tech/gpkgx3e.htmlMath Formulationå‰é¢èªªçš„éƒ½æ˜¯æè¿°æ€§çš„èªžè¨€ï¼Œå†ä¾†æ˜¯å¹²è²¨ã€‚å…ˆæ¾„æ¸…ä¸€äº›ç„¡é—œçš„ ideas.Symmetric Group Group theory ä¸­çš„ symmetric group æœ‰æ˜Žç¢ºçš„å®šç¾©ï¼Œå°±æ˜¯ n symbol æ‰€æœ‰ permutation (i.e. self bijections) å½¢æˆçš„ group, ç¨±ç‚º $S_n$, with $n!$ group element. ä¸‹åœ–æ˜¯ $S_4$ çš„ Cayley graph, total 4! = 24 elements. æ‰€æœ‰çš„ finite group å¯ä»¥è­‰æ˜Žéƒ½æ˜¯æŸå€‹ symmetric group çš„å­ç¾¤ã€‚ä¸éŽé€™è£¡çš„ symmetric group å’Œæœ¬æ–‡ç„¡é—œã€‚Symmetry Brings Conservation (Noether Theorem) (check å»£ç¾©ç›¸å°è«– article)A physic law is invariant of different observer.  For example the Lagrangian is invariant (or covariant?) under certain coordinate transformation (different observers).  We called these coordinate transformation as symmetry operation.  These symmetry operation corresponds to a specific conservation law.å†ä¾†é€²å…¥ä¸»é¡Œã€‚Equivariance Math Formulation of Neural Network$y = \Phi(x)$ where $\Phi$ represents (part of) the neural network. $y$ is network output feature tensor; x is input image tensor.  Tensor can be viewed as a high dimension matrix.$\Phi$ å¯ä»¥æ˜¯ä¸€å€‹è¤‡é›œçš„ cascaded nonlinear function (with CNN, ReLU, Pooling, etc.) or a simple linear function with tensor input and tensor output.$\Phi$ å¯ä»¥æ˜¯ injective/bijective or non-injective.  ä¾‹å¦‚ï¼Œinput image tensor æ˜¯ WxHxCin, å¦‚æžœ output tensor æ˜¯ WxHxCout (stride=1) and Cout $\ge$ Cin, ä¸€èˆ¬æ˜¯ injective or bijective.  å¦‚æžœ stride &gt; 1 or Cout &lt; Cin, å‰‡æ˜¯ non-injective, ä¹Ÿå°±æ˜¯å­˜åœ¨ $xâ€™\ne x$, and $\Phi(xâ€™) = \Phi(x)$.$xâ€™ = T x$ where $T$ is a linear transformation (a multi-dimension matrix) corresponding to a new observer (coordinate).  æ­¤è™• T æ˜¯ bijective transform, or full rank transformation, ä¾‹å¦‚ translation, rotation, affine transformation.The new observer obtains the new output feature tensor $yâ€™ = \Phi(xâ€™) = \Phi(T x)$Our goal is to explore the relationship between $\Phi(T x)$ and $\Phi(x)$.In general, $\Phi(T x)$ å’Œ $\Phi(x)$ å¯èƒ½æœ‰å„ç¨®ä¸åŒçš„é—œä¿‚ã€‚  å¦‚æžœ $\Phi(T x) = \Phi(x) \; \forall x$, æ»¿è¶³çš„æ‰€æœ‰ $T$ ç¨±ç‚º $\Phi$ çš„ invariant group.          Ex. $\Phi$ is norm of x, $T_g$ æ˜¯æ‰€æœ‰ metric-perserve transformation (translation, rotation, mirror, etc.)      It losses all T information, all completely independent of coordinate.      Usually for scalar.        å¦‚æžœ $\Phi(T x) = T \Phi(x) \; \forall x$, æ»¿è¶³çš„æ‰€æœ‰ $T$ ç¨±ç‚º $\Phi$ çš„ equivariant group.          Keep spatial information      Equivariant Group: $\Phi$ is Linear and Bijective (full rank)If $\Phi$ is a linear network, å¯è¦–ç‚ºä¸€å€‹ matrix $\Phi$, i.e. $\Phi(T x) = \Phi T x$.  ç‚ºäº†ç°¡åŒ–ï¼Œå‡è¨­ $\Phi$ and $T$ æ˜¯ 2D matrix.ç¾åœ¨éœ€è¦æ‰¾åˆ° given $\Phi$, ä»€éº¼ $T$ å¯ä»¥å¾—åˆ° $\Phi(T x) = \Phi T x = T \Phi x = T \Phi(x)$ for $\forall x$$\Rightarrow \Phi T = T \Phi$, ä¹Ÿå°±æ˜¯ $\Phi$ and $T$ commute, å› æ­¤è®Šæˆ commuting matrices problem, å¯ä»¥åƒè€ƒ [@wikiCommutingMatrices2019].One sufficient (not a necessary) condition: $\Phi$ and $T$ are simultaneously diagonalizable, i.e. $\Phi = P^{-1} D P$ and $T = P^{-1} Q P$ where $D$ and $Q$ éƒ½æ˜¯ diagonal matrix. $\Phi T = P^{-1} D Q P = P^{-1} Q D P = T \Phi$ä¹Ÿå°±æ˜¯åªè¦ $T = P^{-1} Q P$ where P comes from eigenvectors of $\Phi$,  $\Phi T x = T \Phi x \; \forall x$.  Commuting matrices preserve each otherâ€™s eigenspaces.é€™æ¨£çš„ $T$ form a commuting (Abelian) group $T_g$ (assuming T is full rank, exclude 0 in the eigenvalues of T and Q), å› ç‚º $T_1  T_2 = P^{-1} Q_1 P P^{-1} Q_2 P = P^{-1} (Q_1 Q_2) P = P^{-1} (Q_2 Q_1) P = T_2 T_1 = T_3$ (multiplication closure and commuting), ä¸¦ä¸”æ¯ä¸€å€‹ $T$ éƒ½å­˜åœ¨å”¯ä¸€çš„åå…ƒç´  $P^{-1} Q^{-1} P$ (inverse closure).In summary, given a linear and bijective network $\Phi$, å¯ä»¥å®šç¾©ä¸€å€‹ equivarient commutative group $T_g$ such that $\Phi(T x) = T \Phi(x) \; \forall x$.  é€™å€‹ç¾¤çš„å…ƒç´ (matrix) çš„ eigenvectors éƒ½å’Œ $\Phi$ eigenvectors ä¸€è‡´ã€‚  ä¹Ÿå¯ä»¥æŠŠ $\Phi$ è¦–ç‚ºé€™å€‹ group, $T_g$ çš„ä¸€å€‹ element.Simple $\Phi$: 2D Matrix Equivariant Group ExampleEx1: $\Phi$ = [1, 0; 0, 2]  $\Rightarrow T_g =[k_1, 0; 0, k_2]$. æ‰€æœ‰ unequal scaling éƒ½æ˜¯ equivariant group.Ex2: $\Phi$ = [2, 1; 1, 2]  $\Rightarrow T_g =[c, s; s, c]$.  æ‰€æœ‰ hyperbolic rotation éƒ½æ˜¯ equivariant group (with a normalization constant).Ex3: $\Phi$ = [2, -1; 1, 2]  $\Rightarrow T_g =[c, -s; s, c]$.  æ‰€æœ‰ rotation éƒ½æ˜¯ equivariant group (with a normalization constant).Ex4: Horizontal shear ä¹Ÿæ˜¯ä¸€å€‹ equivariant group.Proof: $[1, k_1; 0, 1] \times [1, k_2; 0, 1] = [1, k_1+k_2; 0, 1] \to$ multiplication closure and åå…ƒç´ æ˜¯ $[1, -k; 0, 1] \to$ inverse closure.Ex5: Uniform scaling ä¹Ÿæ˜¯ä¸€å€‹ (trivial) equivariant group.ä¸‹åœ–æ‘˜è‡ª [@wikiEigenvaluesEigenvectors2020].Discrete Convolution: [@wikiToeplitzMatrix2020]Discrete convolution (é›¢æ•£å·ç©) å»£æ³›ç”¨æ–¼æ•¸ä½è¨Šè™Ÿè™•ç†å’Œæ·±åº¦å­¸ç¿’ for audio and video.  Discrete convolution åŸºæœ¬æ˜¯ linear bijective operation, åŒæ¨£é©ç”¨ equivariant group çš„çµè«–ã€‚æˆ‘å€‘ç”¨ 1D discrete convolution ç‚ºä¾‹å¦‚ä¸‹ï¼š$y = h * x = \Phi x$ where $\Phi$ is a $n\times n$ matrix, å°±æ˜¯æŠŠ m-tap kernel filter $[h_1, h_2, â€¦, h_m]$ shift (å¹³ç§») n æ¬¡é€ å‡ºçš„ matrix, ç¨±ç‚º Toeplitz matrix. ä¸€èˆ¬ nÂ Â» m, å› æ­¤æ˜¯ â€œband matrixâ€ with high sparsity. å¾Œé¢æœƒçœ‹åˆ° $\Phi$ çš„ equivariant group $T_g$ å’Œé€™å€‹æ“ä½œç›´æŽ¥ç›¸é—œã€‚ä¸‹ä¸€æ­¥æ˜¯è¦æ‰¾å‡º $\Phi$ çš„ eigenvectors ä»¥åŠæ§‹æˆçš„ commutative group. å¯ä»¥åƒè€ƒ [@grayToeplitzCirculant1971], excel article about Toeplitz matrix.æœ‰ä¸€å€‹ â€œtrickâ€ å°±æ˜¯ç”¨ Circulant matrix å–ä»£ Toeplitz matrix by using cyclic shift to replace regular shift!  å› ç‚º nÂ Â» m, å¯¦å‹™ä¸ŠToeplitz å’Œ Circulant matrix å¾—åˆ°çš„ $y$ å·®ç•°å¾ˆå°ã€‚ä½† Circulant matrix å¥½æ±‚è§£è€Œä¸”å…·æœ‰ç‰©ç†æ„ç¾©ã€‚Follow [@grayToeplitzCirculant1971] çš„ notation on p.31, æˆ‘å€‘ç”¨ $C$ ä»£æ›¿ $\Phi$.A $n\times n$ circulant matrix $C$ has the form Circulant matrix eigenvalues and eigenvectorsThe eigenvalues $\psi_m$ and the eigenvectors $y^{(m)}$ are the solution ofæˆ‘å€‘å¼•å…¥ä¸€å€‹ variable $\rho$, which is one of the n distinct complex root of unity ($\rho_m = e^{-2\pi i m/n}$, $m = 0, â€¦ n-1$), we have the eigenvalue and eigenvector and å¸¶å…¥ $\rho_m$, we have eigenvalue $(m = 0, â€¦ n)$!!æ³¨æ„ï¼š$\psi_{m}$ is the DFT of $c_k$, i.e. $\psi = DFT(c)$.  åä¹‹ï¼Œ$c = IDFT(\psi)$$\psi_{m}$ å°æ‡‰çš„ (column) eigenvector æª¢æŸ¥å¹¾å€‹ eigenvalue. First, $m=0$ is the DC component of $c_k$å°æ‡‰çš„ (column) eigenvectorå¸¶å…¥é©—è­‰  $ C y^{(0)} = \psi_{0} y^{(0)}  $.Next $m=1$ is the 1st fundamental component of $c_k$å°æ‡‰çš„ (column) eigenvector Next $m=2$ is the 2nd fundamental component of $c_k$å°æ‡‰çš„ (column) eigenvector å¯ä»¥é©—è­‰  $ C y^{(m)} = \psi_{m} y^{(m)}  $.æˆ‘å€‘ç”¨ one equation to summarize the results. å…¶å¯¦å°±æ˜¯ $C$ çš„ eigenvalue decomposition å¦‚ä¸‹ã€‚$\Psi$ æ˜¯ diagonal matrix with eigenvalues, å‰›å¥½å°±æ˜¯ $C$ matrix ç¬¬ä¸€åˆ— (row 1) çš„ DFT çµæžœã€‚  wherewith $\omega = e^{-2 \pi i / n}$ and $\bar{\omega} = \omega^{*} = e^{+2 \pi i / n}$Complex conjugate frequency sequenceå¦ä¸€ç¨®çš„é †åºæ˜¯ complex conjugate (Nyquist) frequency sequence, å°±æ˜¯ [DC, +f, -f, +2f, -2f, â€¦, AC]  å¦‚æžœ n æ˜¯å¶æ•¸ï¼ŒAC = [1, -1, 1, -1â€¦].  å¦‚æžœ n æ˜¯å¥‡æ•¸ï¼Œâ€¦.Equivariant: $\Phi$ is Circulant Matrix for Discrete ConvolutionGiven $\Phi = C$, a circulant matrix, ç¾åœ¨éœ€è¦æ‰¾åˆ° equivariant group $T$ to make $\Phi(T x) = \Phi T x = T \Phi x = T \Phi(x)$.  ç­”æ¡ˆæ˜¯  $T_g= U Q U^{}$ where $U$ and $U^{}$ å°±æ˜¯ä»¥ä¸Šçš„ matrices (n é»žåˆ†åœ“å‡½æ•¸) and $Q$ is a diagonal matrix.æ³¨æ„ $U$ and $U^{}$ æ˜¯ complex matrix, Q in general ä¹Ÿæ˜¯ complex matrix.  ä½†å¯¦éš›æ‡‰ç”¨æœƒé™åˆ¶ $T_g = U Q U^{}$ å¿…é ˆæ˜¯ real matrix.  å› æ­¤æœƒè¦æ±‚ Q matrix æ»¿è¶³ä¸€äº›ç‰¹æ€§ã€‚å› ç‚º Q matrix å…¶å¯¦æ˜¯å¦ä¸€å€‹ circulant matrix çš„ row 1 FFT çµæžœã€‚In summary, circulant matrix æœ¬èº« forms a commutative group, i.e. $A B = B A = C$ (multiplication closure and commuting) is circulant matrix, $A^{-1}$ ä¹Ÿæ˜¯ circulant matrix, ç”šè‡³ $A + B$ ä¹Ÿæ˜¯ circulant matrix [@wikiCirculantMatrix2020].æ•´ç†ä¸€ä¸‹ï¼š  $y = \Phi(x) = h * x$ performs discrete convolution (i.e. 1D CNN) where $x$ and $y$ are input and output signals of n-dimension.  $h$ is the kernel filter of m dimension. ä¸€èˆ¬ nÂ Â» m.  å¯ä»¥ç”¨ $n\times n$ Circulant matrix multiplication è¿‘ä¼¼ discrete convolution by zero padding, i.e. $y = C x$.  $C$ æ˜¯æŠŠ $h$ æ”¾åœ¨ $C$ çš„ row1, å† cyclic right shift by 1 æ”¾åœ¨ row 2, and so on.  In summary, discrete convolution is equivalent to Circulant matrix multiplication.  $n \times n$ Circulant matrices form a commutative group, $T_g$, i.e. $\Phi(T_g x) = T_g \Phi(x)$ as long as $T_g$ is a $n\times n$ Circulant matrix.  Actually, $\Phi \in T_g$.  $T_g$ is equivariant operation.  Circulant group çš„ generating element is $g$ = [0, 1, 0â€¦, 0; 0, 0, 1, â€¦,0,; â€¦.; 1, 0, 0, â€¦, 0]â€˜ ä»£è¡¨ right cyclic shift by 1; $g^2 = gg, g^k = gg..g$. Therefore, $I, g^2, g^3, ..g^{n-1}$ æ§‹æˆ basis for æ‰€æœ‰ $n\times n$ Circulant matrix.  For any Circulant matrix by $[a_0, a_1, â€¦, a_{n-1}] = a_0 I + a_1 g^2 + â€¦, + a_{n-1} g^{n-1}$.  ä¹Ÿå°±æ˜¯èªªï¼ŒCirculant matrix can be decomposed to translation matrix superposition.  Discrete convolution is therefore translation multiplication commutable =&gt; translation equivariant, i.e. $\Phi ( T_g x) = T_g (\Phi x)$A discrete convolution example in appendix A.Equivariant: $\Phi$ is Circulant Matrix for 2D Discrete Convolution$y(t) = h(t) * x(t)$ å¯ä»¥ç›´æŽ¥æŽ¨å»£åˆ° 2D,  $y(u, v) = h(u, v) * x(u, v)$  å› ç‚º $u$ and $v$ are independent on the Cartesian coordinate.  æ³¨æ„é€™ä¸¦ä¸ä»£è¡¨ $y, h, x$ are $u, v$ separable.  Circulant group æ˜¯å…©å€‹ Circulant group çš„ direct sum.  Generator æ˜¯ $g_u$ and $g_v$.  The DFT core is exp(-2piinu/.) exp(-2pimv/.)  How about eigenvalue and eigenvectors?How about other equivariant for 1D signal processing?Mirror, scale equivariant?  $\Phi(x)$ condition of Q?Use Cohenâ€™s paper notation and conceptä»¥ä¸Šçš„æŽ¨å°Žå¤ªç‹¹éš˜ï¼ŒæŽ¥ä¸‹ä¾†æŽ¡ç”¨ Cohenâ€™s paper notation and ideas.The group $p4m$ (non-commutative group)ä»¥ä¸Šæ˜¯ 2D Cartesian coordinate (+1D depth) generates to 4D symmetry G space (+1D depth).  åˆ†ç‚ºå…©ç¨® case: (1) input ä»ç„¶æ˜¯ 3D tensor, but output is converted to 5D tensor.  åƒ…ç”¨æ–¼ç¥žç¶“ç¶²çµ¡çš„ç¬¬ä¸€å±¤ã€‚ä¹‹å¾Œå°±è½‰æ›æˆ (2) both input/output éƒ½æ˜¯ 5D tensors.  åŽŸæ–‡æœ‰ç°¡åŒ–ç‰ˆ p4 (no mirror reflection) and 2D translation only.æ­¤è™•è€ƒæ…®æ›´ç°¡å–®çš„ case, 1D translation and 1D translation + mirror reflection.Next step:  Function f(x)  Group operation on f(x) is  $L_g f(x) = f(g^{-1}x)$.  åŽŸå› å¾ˆç°¡å–®ï¼Œå°±æ˜¯åœ¨ $xâ€˜=gx$ æœƒå¾—åˆ°åŽŸä¾†çš„å‡½æ•¸ã€‚      è€ƒæ…® CNN convolution å‡½æ•¸ï¼Œå®šç¾©å¦‚ä¸‹ã€‚$x, y \in Z^2$    æŽ¨å»£åˆ° G-CNN convolution.  $g, h \in G$ä¸Šå¼æ˜¯ forward pass çš„ convolution ($\ast$).  ä¸‹å¼æ˜¯ backward pass çš„ correlation ($\star$).      Combine 2 and 3, $L_u f \star \psi = f \star \psi \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}((u^{-1}g)^{-1}h) \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(g^{-1}uh)   \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(u^{-1}h) \psi_{k}^{i}(g^{-1}h) \  = [L_u f] \star \psi$        Combine 2 and 3, $L_u f * \psi = f * \psi \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1} (u^{-1}g)) \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(h) \psi_{k}^{i}(h^{-1}u^{-1}g)   \ = \sum_{h \in G} \sum_{k=1}^{K^{l}} f_{k}(u^{-1}h) \psi_{k}^{i}(h^{-1}g) \  = [L_u f] * \psi$  æˆ‘å€‘ç”¨ä¸€å€‹ 1D convolution ä¾†é©—è­‰ã€‚ Example: g = [(-1)^m, u; 0, 1]   g^-1 = [(-1)^m, -u*(-1)^m; 0 , 1]Does it make sense?   If $\psi$ is an odd function, $f \star \psi^{i} =0$?No, g = (x, m) =&gt; m should be kept instead of disappear after summation!!Letâ€™s look at another example, polar transform. [@estevesPOLARTRANSFORMER2018]Polar CoordinateA similarity transformation, i.e. conformal mapping, æ—‹è½‰(R)+scaling(s)+å¹³ç§»(t), $\rho \in $ SIM(2), acts on a point in $x \in R^2$ bywhere SO(2) is the rotation group.Equivariance to SIM(2) is achieved by (1) learning the center of the dilated rotation, (2) shifting the original image accordingly then (3) transforming the image to canonical coordinates.Q1: How to find the center of rotation? Need an origin predictor.Transformation of the image $L_t I = I(t-t_o)$ reduces the SIM(2) deformation to a dilated-rotation if $t_o$ is the true translation. After centering, we perform $SO(2) \times R^+$ convolutions on the new image $I_o = I(x-t_o)$.Layer 1 convolution è®Šæˆï¼šIn summary,æœ¬æ–‡ (polar transformation) æ¯”è¼ƒåƒæ˜¯ coordinate transformation instead of adding group dimension.  No. å¾žåŽŸå§‹çš„ $t \in R^2$, å¤šäº† rotation and scale dimension $SO(2) \times R^+$.  But yes, å°± convolution è€Œè¨€ï¼Œfeature extraction å·²ç¶“ä¸æ˜¯ (x,y) convolution, è€Œæ˜¯ $\epsilon, \theta$  location information ä»ç„¶å­˜åœ¨ï¼Œä½†ç”¨ origin predictor å–ä»£ (x,y) convolution learning.Equivariant: $\Phi$ is CNN and Bijective (reversible, stride=1, ignore boundary)(Translation) Equivariant:= Tâ€™y = Tâ€™f(x) where Tâ€™ is another coordinate which could be different from T because of scaling, etc.   But both T and Tâ€™ are linear operators. This orange part is the crucial step assuming translation equivariant!!   However, T is translation equivariant, but NOT rotational equivariant. yâ€™ = Tâ€™y = Tâ€™ f(x) = Tâ€™ f(T^-1 xâ€™)  assuming linear inversible operation.Use [@cohenGroupEquivariant2019] notation $f \to \Phi$ and $T \to T_g$ Original output feature is $\Phi(x)$, where $\Phi$ can be a nonlinear (complicated) mapping, such as convolution + pooling + ReLU.Given input image x is transformed by $T_g$ operator/transform, new output feature is $\Phi(T_g x)$.å¦‚æžœå…·æœ‰ translation equivariant =&gt; $\Phi(T_g x) = Tâ€™_g \Phi(x)$ where Tâ€™_g æ˜¯åŒæ¨£çš„ translation operator/transform, but may have different scaling factor (stride &gt; 1).æ‰€ä»¥ $T_g$ and $Tâ€™_g$ éœ€è¦æœ‰ä»€éº¼ç‰¹æ€§ï¼Ÿåªéœ€è¦ linear, i.e. $T(gh) = T(g)T(h)$.å¦‚æžœ $Tâ€™_g = I$ for all g, æ˜¯ special case, ç¨±ç‚º invariant.  é€™å’Œä¸€èˆ¬ç‰©ç†å®šç¾©çš„ invariant ä¼¼ä¹Žä¸åŒ?  å°æ–¼æ·±åº¦å­¸ç¿’ invariant æœƒå¤±åŽ» spatial information, $Tâ€™_g$ è€Œè®Šå¾—ç„¡ç”¨, equivariance æ˜¯æ›´æœ‰ç”¨ã€‚å¦ä¸€å€‹æ¥µç«¯æ˜¯æ²’æœ‰ equivariant, ä¹Ÿå°±æ˜¯ $\Phi(T_g x)$ å’Œ $\Phi(x)$ æ²’æœ‰ç°¡å–®çš„ linear mapping, ä¾‹å¦‚ Multi-layer Perceptron (MLP).Paper å¦å¤–ä¸€æ®µè©±å¦‚ä¸‹ï¼Œä¼¼ä¹Žå’Œ invariant ç›¸æŠµè§¸? No, æ˜¯æ“´å……åˆ° non-injective (é™ç¶­) network.A network $\Phi$ can be non-injective, meaning that non-identical vectors $x$ and $y$ in the input space become identical in the output space.  (for example, two instances of a face may be mapped onto a single vector indicating the presence of any face, e.g. äººè‡‰åµæ¸¬è€Œéžè­˜åˆ¥ï¼Œå…©å€‹ä¸åŒçš„äººè‡‰å°æ‡‰åˆ°ç›¸åŒçš„ feature map or bounding box).  If $\Phi$ is equivariant, then the G-transformed inputs $T_g x$ and $T_g y$ must also mapped to the same output.  Their â€œsamenessâ€ is preserved under symmetry transformations.æ•¸å­¸è¡¨ç¤ºï¼šNon-injective network: $\Phi(x) = \Phi(y)$ with $x \ne y$ If $\Phi$ is equivariant, then the G-transform (symmetry transform) has:$\Phi(T_g x)  = Tâ€™_g \Phi(x) = Tâ€™_g \Phi(y) = \Phi(T_g y)$ with $x \ne y$g represents general group, in the paper considering three groups: Z2, p4, p4m.  Conclusion.  On MNIST and CIFAR, G-CNN performs better than CNN at about same parameter number.  G-CNN also benefit from data augment.  Step 1: G-CNN to include translation, rotation, mirror on grid  Step 2: G-CNN on hexagon grid  Step 3: On 3D sphere and use G-FFT to compute sphere convolution for 3D application.  Step 4: Gauge CNN?CNN, pooling, ReLU are translation equivariant (up to edge effect); but MLP is NOT translation equivariant.Translation Equivariant:  There is a function (e.g. CNN)1D =&gt; 2D convolution =&gt; high dimension tensor convolutionStep 1: Define the network operator $\Phi$ Step 2: Find the commuting operator $T$, actually, a commutative group $T_g$.  $\Phi$ å¯ä»¥è¦–ç‚º $T_g$ çš„ä¸€å€‹ element. Step 3: Find the group generator for the commutative group.What is the fundamental element of a group? =&gt; generator &lt;g, ..&gt;!æ‰€æœ‰çš„ group element éƒ½å¯ä»¥å¾ž generator &lt;g, ..&gt; ç”¢ç”Ÿã€‚All Abelian group is isomorphic to direct sum of primed cycle group =&gt; generator g, gg, ggg, â€¦Group ExamplesReferenceBronstein, Michael M., Joan Bruna, Yann LeCun, Arthur Szlam, and PierreVandergheynst. 2017. â€œGeometric Deep Learning: Going Beyond EuclideanData.â€ IEEE Signal Processing Magazine 34 (4): 18â€“42.https://doi.org/10.1109/MSP.2017.2693418.Cohen, Taco S, T S Cohen, and Uva Nl. 2019. â€œGroup Equivariant Convolutional Networks,â€ 10.Cohen, Taco S., Maurice Weiler, Berkay Kicanaoglu, and Max Welling.  â€œGauge Equivariant Convolutional Networks and the IcosahedralCNN,â€ May. http://arxiv.org/abs/1902.04615.Pavlus, John. 2020. â€œAn Idea from Physics Helps AI See in HigherDimensions.â€ Quanta Magazine. January 9, 2020.https://www.quantamagazine.org/an-idea-from-physics-helps-ai-see-in-higher-dimensions-20200109/.prism. 2019. ã€ŒGroup Equivariant CNN to Spherical CNNs: å¾žç¾¤ç­‰è®Šå·ç©ç¶²çµ¡åˆ°çƒé¢å·ç©ç¶²çµ¡.ã€ çŸ¥ä¹Žå°ˆæ¬„. 2019.https://zhuanlan.zhihu.com/p/34042888.Winkels, Marysia, and Taco S. Cohen. 2018. ã€Œ3D G-CNNs for PulmonaryNodule Detection,ã€ April. http://arxiv.org/abs/1804.04656.XinZhiYuan. 2020. ã€ŒGeometrical Deep Learning å—æ„›å› æ–¯å¦å•Ÿç¤ºï¼šè®“AIæ“ºè„«å¹³é¢çœ‹åˆ°æ›´é«˜çš„ç¶­åº¦.ã€ 2020. https://kknews.cc/tech/gpkgx3e.html.]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> quantization </tag>
        
          <tag> model compression </tag>
        
          <tag> pruning </tag>
        
          <tag> distillation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[å¢žé€²å·¥ç¨‹å¸«æ•ˆçŽ‡ Julia Linear Algebra]]></title>
      <url>/2020/04/21/matrix-julia/</url>
      <content type="text"><![CDATA[Use Julia for Linear Algebrausing LinearAlgebraA = [1 2 3; 2 3 4; 4 5 6]3Ã—3 Array{Int64,2}: 1  2  3 2  3  4 4  5  6eigvals(A)3-element Array{Float64,1}: 10.830951894845311      -0.8309518948453025      1.0148608166285778e-16det(A)0.0x = range(0, 10, length=1000)0.0:0.01001001001001001:10.0using PyPlotgrid()plot(x, sin.(x))1-element Array{PyCall.PyObject,1}: PyObject &lt;matplotlib.lines.Line2D object at 0x140288630&gt;Discrete Convolution Using Circulant Matrix$y[t] = h[t] * x[t]$  where $x[t] = [1, 2, 3, 0, -3, -1, 1, -2]$, $h[t] = [1, 3, 1]$Use Julia LinearAlgebra for matrix/vector operation.  Use two space for new line.Use DSP.conv to perform discrete convolution.  x: length=8; h: length=3; y: length=8+3-1=10 (padding two 0â€™s at x)import Pkg; Pkg.add("SpecialMatrices")using LinearAlgebrausing SpecialMatricesusing DSPusing FFTW[32m[1m Resolving[22m[39m package versions...[32m[1m  Updating[22m[39m `~/.julia/environments/v1.1/Project.toml`[90m [no changes][39m[32m[1m  Updating[22m[39m `~/.julia/environments/v1.1/Manifest.toml`[90m [no changes][39mx = [1, 2, 3, 0,  -3, -1,  1, -2, 0, 0];h = [1, 3, 1];y = conv(x, h);y'1Ã—12 Adjoint{Int64,Array{Int64,1}}: 1  5  10  11  0  -10  -5  0  -5  -2  0  0Circulant Matrix Multiplication Approximates Dicrete ConvolutionFirst extend $h[t]$ by padding seven 0â€™s (10-3=7).Use SpecialMatrices.Cirlulant to cyclic shift $h[t]$ and form a 10x10 Circulant matrix $\Phi$.Use SpecialMatrices.Matrix to convert special matrix type to normal Array$y = \Phi x$Î¦ = Matrix(Circulant([1,3,1,0,0,0,0,0,0,0]))10Ã—10 Array{Int64,2}: 1  0  0  0  0  0  0  0  1  3 3  1  0  0  0  0  0  0  0  1 1  3  1  0  0  0  0  0  0  0 0  1  3  1  0  0  0  0  0  0 0  0  1  3  1  0  0  0  0  0 0  0  0  1  3  1  0  0  0  0 0  0  0  0  1  3  1  0  0  0 0  0  0  0  0  1  3  1  0  0 0  0  0  0  0  0  1  3  1  0 0  0  0  0  0  0  0  1  3  1y = Î¦ * x;y'1Ã—10 Adjoint{Int64,Array{Int64,1}}: 1  5  10  11  0  -10  -5  0  -5  -2Find eigenvalues and eigenvectors of $\Phi$$\Phi$ is a Circulant matrix, its eigenvalue array s[n] is â€œequivalentâ€ to DFT($h[t]$), sort of,up to frequency sequence difference.For example, DFT frequency sequence is always defined counter clockwise on the unit circle (0,1,2,..,9) for n=10.The eigenvalue/eigenvector decomposition: $\Phi = U P U^{*}$ In this eigvals implementation frequency is defined as conjugate first on the unit circle (0,1,9,2,8â€¦,5)The first eigenvalue of $\Phi$ corresponds to Nyquist frequency = 0 (DC: 1+1+3=5)The last eigenvalue of $\Phi$ corresponds to Nyquist frequency = $\pi$ (highest AC: 1+1-3=-1)Its eigenvector array P cosists of eigenvectors in column sequences.The first column corresponds to DC eigenvector: [1, 1, â€¦, 1]â€™.The last column corresponds to DC eigenvector: [1, -1, â€¦, -1]â€™.s = eigvals(Î¦)10-element Array{Complex{Float64},1}:                 5.0 + 0.0im                 3.7360679774997863 + 2.7144122731725697im  3.7360679774997863 - 2.7144122731725697im   1.118033988749894 + 3.440954801177931im    1.118033988749894 - 3.440954801177931im  -0.7360679774997894 + 2.265384296592988im  -0.7360679774997894 - 2.265384296592988im  -1.1180339887498945 + 0.8122992405822655im -1.1180339887498945 - 0.8122992405822655im -1.0000000000000002 + 0.0im               fft([1 3 1 0 0 0 0 0 0 0])'10Ã—1 Adjoint{Complex{Float64},Array{Complex{Float64},2}}:                 5.0 - 0.0im                   3.73606797749979 + 2.714412273172573im    1.118033988749895 + 3.4409548011779334im -0.7360679774997898 + 2.2653842965929876im  -1.118033988749895 + 0.8122992405822659im                -1.0 - 0.0im                 -1.118033988749895 - 0.8122992405822659im -0.7360679774997898 - 2.2653842965929876im   1.118033988749895 - 3.4409548011779334im    3.73606797749979 - 2.714412273172573im P = Diagonal(s)10Ã—10 Diagonal{Complex{Float64},Array{Complex{Float64},1}}: 5.0+0.0im          â‹…          â€¦           â‹…                â‹…         â‹…      3.73607+2.71441im              â‹…                â‹…         â‹…              â‹…                      â‹…                â‹…         â‹…              â‹…                      â‹…                â‹…         â‹…              â‹…                      â‹…                â‹…         â‹…              â‹…          â€¦           â‹…                â‹…         â‹…              â‹…                      â‹…                â‹…         â‹…              â‹…                      â‹…                â‹…         â‹…              â‹…             -1.11803-0.812299im       â‹…         â‹…              â‹…                      â‹…           -1.0+0.0imU = eigvecs(Î¦)(round.(U*1000*sqrt(10)))/100010Ã—10 Array{Complex{Float64},2}: 1.0+0.0im   0.809+0.588im   0.809-0.588im  â€¦  -0.809-0.588im   1.0+0.0im 1.0+0.0im     1.0+0.0im       1.0-0.0im          1.0-0.0im    -1.0+0.0im 1.0+0.0im   0.809-0.588im   0.809+0.588im     -0.809+0.588im   1.0+0.0im 1.0+0.0im   0.309-0.951im   0.309+0.951im      0.309-0.951im  -1.0+0.0im 1.0+0.0im  -0.309-0.951im  -0.309+0.951im      0.309+0.951im   1.0+0.0im 1.0+0.0im  -0.809-0.588im  -0.809+0.588im  â€¦  -0.809-0.588im  -1.0+0.0im 1.0+0.0im    -1.0-0.0im      -1.0+0.0im          1.0-0.0im     1.0+0.0im 1.0+0.0im  -0.809+0.588im  -0.809-0.588im     -0.809+0.588im  -1.0+0.0im 1.0+0.0im  -0.309+0.951im  -0.309-0.951im      0.309-0.951im   1.0+0.0im 1.0+0.0im   0.309+0.951im   0.309-0.951im      0.309+0.951im  -1.0+0.0imU_b = inv(U)(round.(U_b*1000*sqrt(10)))/100010Ã—10 Array{Complex{Float64},2}:    1.0+0.0im       1.0-0.0im    â€¦     1.0+0.0im       1.0-0.0im    0.809-0.588im     1.0-0.0im       -0.309-0.951im   0.309-0.951im  0.809+0.588im     1.0+0.0im       -0.309+0.951im   0.309+0.951im -0.809-0.588im   0.309-0.951im      0.309+0.951im  -0.809+0.588im -0.809+0.588im   0.309+0.951im      0.309-0.951im  -0.809-0.588im -0.809+0.588im  -0.309-0.951im  â€¦   0.309-0.951im   0.809+0.588im -0.809-0.588im  -0.309+0.951im      0.309+0.951im   0.809-0.588im -0.809-0.588im     1.0+0.0im        0.309-0.951im   0.309+0.951im -0.809+0.588im     1.0-0.0im        0.309+0.951im   0.309-0.951im    1.0-0.0im      -1.0+0.0im          1.0+0.0im      -1.0-0.0im  (round.((U_b - U')*1000*sqrt(10)))/1000 # U_b is the same as conjugate transpose10Ã—10 Array{Complex{Float64},2}:  0.0+0.0im   0.0-0.0im   0.0-0.0im  â€¦  -0.0-0.0im  -0.0+0.0im   0.0-0.0im  0.0+0.0im  -0.0-0.0im   0.0-0.0im      0.0+0.0im   0.0+0.0im   0.0+0.0im  0.0-0.0im  -0.0+0.0im   0.0+0.0im     -0.0+0.0im   0.0-0.0im   0.0-0.0im  0.0+0.0im  -0.0+0.0im  -0.0-0.0im      0.0-0.0im  -0.0+0.0im  -0.0+0.0im  0.0-0.0im  -0.0-0.0im  -0.0+0.0im     -0.0+0.0im  -0.0-0.0im  -0.0-0.0im  0.0-0.0im   0.0+0.0im   0.0+0.0im  â€¦   0.0-0.0im  -0.0-0.0im   0.0+0.0im  0.0+0.0im   0.0-0.0im   0.0-0.0im     -0.0+0.0im  -0.0+0.0im   0.0-0.0im  0.0+0.0im  -0.0+0.0im   0.0-0.0im      0.0-0.0im   0.0-0.0im  -0.0+0.0im  0.0+0.0im  -0.0-0.0im   0.0+0.0im      0.0+0.0im  -0.0+0.0im   0.0-0.0im -0.0-0.0im   0.0+0.0im  -0.0+0.0im     -0.0-0.0im   0.0+0.0im   0.0-0.0imPhi = U * P * U_b  # verify U*P*U_b is the eigen value decompostion of Î¦real.(round.(Phi*1000))/100010Ã—10 Array{Float64,2}:  1.0  -0.0   0.0   0.0   0.0   0.0   0.0  -0.0   1.0   3.0  3.0   1.0   0.0   0.0  -0.0  -0.0  -0.0   0.0  -0.0   1.0  1.0   3.0   1.0   0.0   0.0   0.0   0.0   0.0  -0.0  -0.0  0.0   1.0   3.0   1.0   0.0   0.0   0.0  -0.0  -0.0   0.0  0.0   0.0   1.0   3.0   1.0   0.0   0.0  -0.0  -0.0   0.0  0.0   0.0   0.0   1.0   3.0   1.0  -0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0   1.0   3.0   1.0   0.0   0.0   0.0  0.0   0.0   0.0  -0.0  -0.0   1.0   3.0   1.0  -0.0   0.0 -0.0   0.0   0.0  -0.0   0.0   0.0   1.0   3.0   1.0  -0.0  0.0   0.0  -0.0   0.0   0.0   0.0   0.0   1.0   3.0   1.0Commutative Group - Translation Equivariant  Discrete convolution is equivalent to Circulant matrix multiplication.  Circulant matrix is itself commutative/Abelian group.  All Cirulant matrix multiplication can be decomposed into translation matrix multiplicationâ€™s superposition.  Discrete convolution is therefore translation multiplication commutable =&gt; translation equivariantR = rand(10, 10)Î¦ * R - R * Î¦   # Random matrix multiplication does NOT commutate with Circulant matrix10Ã—10 Array{Float64,2}: -1.58559   -0.237127  -0.129967  â€¦   0.066688  -1.72504    -1.40459   1.04033    1.08089    0.34026      -0.782441  -1.91891    -1.63573   3.35632    0.831891   0.805718     -0.849477   1.18107     0.183805  0.815998  -1.54383   -0.38105      -1.00838   -0.141359   -0.176457 -0.915783   0.225814  -1.14518       0.5001     0.128517    1.68327  -2.07181    1.04074   -2.12622   â€¦   1.62098    0.348925    1.07887   0.313094   1.84738   -0.894152      0.282517  -2.32041    -0.039078  1.11406   -0.71119   -1.059        -0.981632  -0.0110641   0.999691  1.42449   -1.50675   -1.53478       1.06594    0.590891    2.41234  -2.2159    -2.33044   -1.29612       0.626618  -0.119957    0.743334Tg = Circulant([1,2,3,4,5,6,7,8,9,10]) # Verify Circulant matrix multiplication is a commutative group10Ã—10 Circulant{Int64}:  1  10   9   8   7   6   5   4   3   2  2   1  10   9   8   7   6   5   4   3  3   2   1  10   9   8   7   6   5   4  4   3   2   1  10   9   8   7   6   5  5   4   3   2   1  10   9   8   7   6  6   5   4   3   2   1  10   9   8   7  7   6   5   4   3   2   1  10   9   8  8   7   6   5   4   3   2   1  10   9  9   8   7   6   5   4   3   2   1  10 10   9   8   7   6   5   4   3   2   1Î¦ * Tg - Tg * Î¦   10Ã—10 Array{Int64,2}: 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0 0  0  0  0  0  0  0  0  0  0g = Circulant([0,1,0,0,0,0,0,0,0,0])  # Circulant matrix group generator: right cyclic shift by 110Ã—10 Circulant{Int64}: 0  0  0  0  0  0  0  0  0  1 1  0  0  0  0  0  0  0  0  0 0  1  0  0  0  0  0  0  0  0 0  0  1  0  0  0  0  0  0  0 0  0  0  1  0  0  0  0  0  0 0  0  0  0  1  0  0  0  0  0 0  0  0  0  0  1  0  0  0  0 0  0  0  0  0  0  1  0  0  0 0  0  0  0  0  0  0  1  0  0 0  0  0  0  0  0  0  0  1  0g*g  # right cyclic shift by 210Ã—10 Array{Int64,2}: 0  0  0  0  0  0  0  0  1  0 0  0  0  0  0  0  0  0  0  1 1  0  0  0  0  0  0  0  0  0 0  1  0  0  0  0  0  0  0  0 0  0  1  0  0  0  0  0  0  0 0  0  0  1  0  0  0  0  0  0 0  0  0  0  1  0  0  0  0  0 0  0  0  0  0  1  0  0  0  0 0  0  0  0  0  0  1  0  0  0 0  0  0  0  0  0  0  1  0  0eye = 1.0*Matrix(I, 10, 10)   # Verify Circulant Tg is decomposed into group generator superposition1*eye+2*g+3*g*g+4*g*g*g+5*g*g*g*g+6*g*g*g*g*g+7*g*g*g*g*g*g+8*g*g*g*g*g*g*g+9*g*g*g*g*g*g*g*g+10*g*g*g*g*g*g*g*g*g10Ã—10 Array{Float64,2}:  1.0  10.0   9.0   8.0   7.0   6.0   5.0   4.0   3.0   2.0  2.0   1.0  10.0   9.0   8.0   7.0   6.0   5.0   4.0   3.0  3.0   2.0   1.0  10.0   9.0   8.0   7.0   6.0   5.0   4.0  4.0   3.0   2.0   1.0  10.0   9.0   8.0   7.0   6.0   5.0  5.0   4.0   3.0   2.0   1.0  10.0   9.0   8.0   7.0   6.0  6.0   5.0   4.0   3.0   2.0   1.0  10.0   9.0   8.0   7.0  7.0   6.0   5.0   4.0   3.0   2.0   1.0  10.0   9.0   8.0  8.0   7.0   6.0   5.0   4.0   3.0   2.0   1.0  10.0   9.0  9.0   8.0   7.0   6.0   5.0   4.0   3.0   2.0   1.0  10.0 10.0   9.0   8.0   7.0   6.0   5.0   4.0   3.0   2.0   1.0(Î¦*(Tg*x) - Tg*(Î¦*x) )'    # Î¦ and Tg are commutative on input signal x as expected1Ã—10 Adjoint{Int64,Array{Int64,1}}: 0  0  0  0  0  0  0  0  0  0(Î¦*x)'   # normal discrete convolution1Ã—10 Adjoint{Int64,Array{Int64,1}}: 1  5  10  11  0  -10  -5  0  -5  -2(g*(Î¦*x))'   # group generator causes discrete convolution right cyclic translation1Ã—10 Adjoint{Int64,Array{Int64,1}}: -2  1  5  10  11  0  -10  -5  0  -5(g*x)'    # group generator's action on x[t] is to right cyclic shift by 11Ã—10 Adjoint{Int64,Array{Int64,1}}: 0  1  2  3  0  -3  -1  1  -2  0(Î¦*(g*x))'  # discrete convolution of the right cyclic shift signal1Ã—10 Adjoint{Int64,Array{Int64,1}}: -2  1  5  10  11  0  -10  -5  0  -5(Î¦*(g*x) - g*(Î¦*x) )'   # discrete convolution is translation (i.e. g or g*g or g*g*g ...) eqivaraint (commutative)1Ã—10 Adjoint{Int64,Array{Int64,1}}: 0  0  0  0  0  0  0  0  0  0]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Edge AI Trilogy III - Model Compression]]></title>
      <url>/ai/2020/04/05/EdgeAI-Compression/</url>
      <content type="text"><![CDATA[Edge AI Trilogy III - Model CompressionEdge AI ä¸‰éƒ¨æ›²çš„æœ€çµ‚ç¯‡æ˜¯ model compression.  ä¹‹å¾Œé‚„æœƒæœ‰ç•ªå¤–ç¯‡ on advance topics such as NAS, etc.  ç‚ºä»€éº¼ model compression æ”¾åœ¨æœ€çµ‚ç¯‡ï¼Ÿå¯ä»¥ç”¨ Han Song çš„ deep compression [@hanDeepCompression2016; @hanLearningBoth2015] ç‚ºä¾‹ã€‚ä¸‹åœ–æž¶æ§‹æ­£å¥½å°æ‡‰ä¸‰éƒ¨æ›²ï¼špruning, quantization, and (parameter) compression.åŸºæœ¬ä¸Š parameter compression å¯ä»¥æ”¶å‰²ä¹‹å‰ sparsity, quantization, weight sharing å¸¶ä¾†çš„ storage and memory bandwidth reduction (35x-49x) çš„å¥½è™•ã€‚ç•¶ç„¶éš¨è‘— parameter reduction, é¡å¤–é‚„æœ‰ computation and energy reduction çš„å¥½è™•ï¼Œä¾‹å¦‚ zero-skipping for sparsity å’Œ low bitwidth MAC computation for quantization and weight sharing.Model compression åŒ…å« parameter compression ä»¥åŠå…¶ä»–çš„æŠ€å·§æ¸›å°‘ parameter, ç”šè‡³æ”¹è®Š model structureã€‚æœ€å¾Œçš„ model size (in MB) and MAC (in GFlop) å°±æ˜¯ â€œmoment of the truthâ€.  å°±åƒ SNR or BER æ˜¯é€šè¨Šç³»çµ±çš„æ•´é«”æª¢é©—ã€‚é™¤äº† parameter pruning (sparsity) and sharing (clustering and quantizing) ä¹‹å¤–ï¼Œ[@chengSurveyModel2019] æŠŠ model compression ä½œæ³•åˆ†ç‚ºå››é¡žï¼š            Theme Name      Description      Applications      Details                  Parameter pruning and sharing      Reducing redundant parameters not sensitive to the performance      CONV and FC layer      Robust to various settings, can achieve good performance, support both train from scratch and pre-trained model              Low-rank factorization      Using matrix/tensor decomposition to estimate the informative parameters      CONV and FC layer      Standardized pipeline, easily to implement, support both train from scratch and pre-trained model              Transferred/compact convolutional filters      Designing special structural convolutional filters to save parameters      CONV layer only      Algorithms are dependent on applications, usually achieve good performance, only support train from scratch              Knowledge distillation      Training a compact neural network with distilled knowledge of a large model      CONV and FC layer      Model performances are sensitive to applications and network structure only support train from scratch      Another paper æå‡ºçš„ model compression åˆ†é¡ž [@kuzminTaxonomyEvaluation2019].  SVD-based methods (low rank)  Tensor decomposition-based methods (low rank)  Pruning methods  Compression ratio selection method  Loss-aware compression  Probabilistic compression  Efficient architecture designAnother good review paper from Purdue. [@goelSurveyMethods2020]            Technique      Description      Advantages      Disadvantages                  Quantization and Pruning      Reduces precision/completely removes the redundant parameters and connections from a DNN.      Negligible accuracy loss with small model size. Highly efficient arithmetic operations.      Difficult to implement on CPUs and GPUs because of matrix sparsity. High training costs.              Filter Compression and Matrix Factorization      Decreases the size of DNN filters and layers to improve efficiency.      High accuracy. Compatible with other optimization techniques.      Compact convolutions can be memory inefficient. Matrix factorization is computationally expensive.              Network Architecture Search      Automatically finds a DNN architecture that meets performance and accuracy requirements on a target device.      State-of-the-art accuracy with low energy consumption.      Prohibitively high training costs.              Knowledge Distillation      Trains a small DNN with the knowledge of a larger DNN to reduce model size.      Low computation cost with few DNN parameters.      Strict assumptions on DNN structure. Only compatible with softmax outputs.      Model =&gt; Data =&gt; Memory?? (from Huaweiâ€™s talk)My classification:Level 1:  Compression Without changing the network layer and architecture, i.e. weight compression including pruning (weight = 0), quantization (reduce weight bitwidth), weight sharing, etc.  å¯ä»¥åˆ°é” 10x-50x compression for large network (e.g. resnet, alexnet, etc.)Level 2:  Modify network architecture based on some basic rules (matrix/tensor decomposition), network fusion, etc.Level 3: Change the network architecture completely.  Knowledge transfer (KT) or knowledge distillation (KD) belongs to Level 3 or Level 4?Level 4: Network architecture search (NAS) to explore a big search space and based on the constraints of edge device capability.  åš´æ ¼ä¾†èªªï¼Œå·²ç¶“ä¸æ˜¯ model compression, è€Œæ˜¯ model search or exploration.Level 1: Quantization and Pruning and Huffman EncodeDetails å¯ä»¥åƒè€ƒä¹‹å‰å…©ç¯‡æ–‡ç« ã€‚In summary, quantization from FP32 to INT8 å¯ä»¥ save up to 75% (4x) storage/bandwidth/computation, è€Œä¸æå¤± accuracy or increase error. é€™ä¹Ÿä»£è¡¨æ›´ä½Žçš„åŠŸè€—ã€‚å¦‚æžœä½¿ç”¨æ›´å°‘ bitwidth (6/4/2/1), å¯ä»¥çœæ›´å¤šï¼Œä½†æ˜¯å¯èƒ½ trade-off accuracy/error æˆ–æ˜¯é™åˆ¶æ‡‰ç”¨çš„ç¯„åœã€‚ä¸‹åœ–æ˜¯ä¸åŒ quantized bitwidth å°æ‡‰çš„ energy vs. test error.Pruning æ˜¯å¦ä¸€å€‹æ›´æ·±å¥§æ›´æœ‰ç©ºé–“çš„æ–¹å¼ã€‚å¯ä»¥è¦–ç‚º quantization çš„ special case (weight and activation = 0), ä½†æ›´é€²ä¸€æ­¥æ˜¯ä¸€å€‹ subset network å¯ä»¥å®Œå…¨ represent the original network (lottery conjecture).  pruning å°æ–¼ä¸€äº› â€œfat networkâ€ å¯ä»¥é”åˆ° 10x çš„ saving.  å°æ–¼ä¸€äº› â€œlean networkâ€, e.g. MobileNet å°±æ¯”è¼ƒå°‘ saving.Parameter compression ä¹Ÿæ˜¯å¸¸ç”¨çš„æŠ€å·§ã€‚åŒ…å« weight compression and activation compression.Parameter compression çœæœ€å¤šæ˜¯ data åŒ…å«å¾ˆå¤šçš„å†—ä½™æˆ–æ˜¯ regular structureï¼Œä¾‹å¦‚å¤§é‡çš„ 0, å¾ž information theory å°±æ˜¯ low entropy facilitate compression.  Huffman encoding æ˜¯ä¸€å€‹æœ‰æ•ˆçš„æ–¹æ³• for parameter compression.pruning, quantization, compression å¯ä»¥åˆåœ¨ä¸€èµ·å¾—åˆ°æœ€ä½³çš„æ•ˆæžœ at a cost of higher training time.Level 2: Matrix and Tensor Decompositionåˆ†ç‚ºå…©å€‹éƒ¨åˆ†: CONV layer å’Œ FC layer.  ç•¶ç„¶å»£ç¾©ä¾†èªªï¼ŒFC layer ä¹Ÿæ˜¯ä¸€ç¨® CONV layer with kernel size WxHxC.  æ­¤è™• CONV layer çš„ kernel size ä¸€èˆ¬æŒ‡ 1x1, 3x3, â€¦, 11x11, etc.å°æ–¼ CONV layer, è¶Šå¤§ kernel filter çš„ parameters and MACs è¶Šå¤§ï¼Œè¼ƒå°çš„ kernel filter çš„ parameters and MACs è¶Šå°ã€‚ä½†å¦‚æžœæŠŠæ‰€æœ‰å¤§çš„ kernel filter éƒ½æ›¿æ›æˆå°çš„ kernel filter, æœƒå½±éŸ¿ DNN çš„å¹³ç§»ä¸è®Šæ€§ï¼Œé€™æœƒé™ä½Ž DNN model çš„ç²¾åº¦ã€‚å› æ­¤ä¸€äº›ç­–ç•¥æ˜¯è­˜åˆ¥å†—ä½™çš„ kernel filter, ä¸¦ç”¨è¼ƒå° kernel filter å–ä»£ã€‚ä¾‹å¦‚ VGG æŠŠæ‰€æœ‰å¤§æ–¼ 3x3 (e.g. 5x5, 7x7, etc.) éƒ½ç”¨ 3x3 filter å–ä»£ã€‚SqueezeNet and MobileNet ç”šè‡³ç”¨ 1x1 å–ä»£éƒ¨åˆ†çš„ 3x3 filter.Convolutional Filter Compression (CONV layer only)SqueezeNet use 1x1 kernel to replace 3x3 kernel filter. MobileNet use depth-wise + point-wise (1x1) network to replace original kernel to reduce computation by 1/8-1/9 for 3x3 kernel.MobileNet v3 å¯ä»¥é”åˆ°ä¸éŒ¯çš„ç²¾åº¦ (75%)ï¼Œä½†æ˜¯ parameter and MAC æ¯”èµ· AlexNet å°‘éžå¸¸å¤šã€‚æ¯”èµ· ResNet50 (parameter 25M, MAC 4G) ä¹Ÿå¥½ä¸å°‘ã€‚Matrix Factorization/Decomposition (CONV or FC layer)é€šéŽå°‡å¼µé‡æˆ–çŸ©é™£åˆ†è§£ç‚ºåˆç©å½¢å¼ï¼ˆsum-product formï¼‰ï¼Œå°‡å¤šç¶­å¼µé‡åˆ†è§£ç‚ºæ›´å°çš„çŸ©é™£ï¼Œå¾žè€Œå¯ä»¥æ¶ˆé™¤å†—ä½™è¨ˆç®—ã€‚ä¸€äº›å› å­åˆ†è§£æ–¹æ³•å¯ä»¥å°‡DNNæ¨¡åž‹åŠ é€Ÿ4 å€ä»¥ä¸Šï¼Œå› ç‚ºå®ƒå€‘èƒ½å¤ å°‡çŸ©é™£åˆ†è§£ç‚ºæ›´å¯†é›†çš„åƒæ•¸çŸ©é™£ï¼Œä¸”èƒ½å¤ é¿å…éžçµæ§‹åŒ–ç¨€ç–ä¹˜æ³•çš„å±€éƒ¨æ€§å•é¡Œã€‚ç›®å‰ Matrix decomposition/factorization ä¸¦éžä¸»æµã€‚åŽŸå› ï¼š  é—œæ–¼çŸ©é™£åˆ†è§£ï¼Œæœ‰å¤šç¨®æŠ€è¡“ã€‚Koldaç­‰äººè­‰æ˜Žï¼Œå¤§å¤šæ•¸å› å­åˆ†è§£æŠ€è¡“éƒ½å¯ä»¥ç”¨ä¾†åšDNNæ¨¡åž‹çš„åŠ é€Ÿï¼Œä½†é€™äº›æŠ€è¡“åœ¨ç²¾åº¦å’Œè¨ˆç®—è¤‡é›œåº¦ä¹‹é–“ä¸ä¸€å®šèƒ½å¤ å–å¾—æœ€ä½³çš„å¹³è¡¡ã€‚  ç”±æ–¼ç¼ºä¹ç†è«–è§£é‡‹ï¼Œå› æ­¤å¾ˆé›£è§£é‡‹ç‚ºä»€éº¼ä¸€äº›åˆ†è§£ï¼ˆä¾‹å¦‚CPDã€BMDï¼‰èƒ½å¤ ç²å¾—è¼ƒé«˜çš„ç²¾åº¦ï¼Œè€Œå…¶ä»–åˆ†è§£å»ä¸èƒ½ã€‚  èˆ‡çŸ©é™£åˆ†è§£ç›¸é—œçš„è¨ˆç®—å¸¸å¸¸èˆ‡æ¨¡åž‹ç²å¾—çš„æ€§èƒ½å¢žç›Šç›¸ç•¶ï¼Œé€ æˆæ”¶ç›Šèˆ‡æè€—æŠµæ¶ˆã€‚  çŸ©é™£åˆ†è§£å¾ˆé›£åœ¨å¤§åž‹DNNæ¨¡åž‹ä¸­å¯¦ç¾ï¼Œå› ç‚ºéš¨è‘—æ·±åº¦å¢žåŠ åˆ†è§£è¶…åƒæœƒå‘ˆæŒ‡æ•¸å¢žé•·ï¼Œè¨“ç·´æ™‚é–“ä¸»è¦è€—è²»åœ¨å°‹æ‰¾æ­£ç¢ºçš„åˆ†è§£è¶…åƒã€‚æ›´å¤šæ›´ detailed description å¯ä»¥åƒè€ƒ [@kuzminTaxonomyEvaluation2019].Level 3: Knowledge Transfer or Distillationhttps://www.leiphone.com/news/202003/cggvDDPFIVTjydxS.htmlå¤§æ¨¡åž‹æ¯”å°æ¨¡åž‹æ›´æº–ç¢ºï¼Œå› ç‚ºåƒæ•¸è¶Šå¤šï¼Œå…è¨±å­¸ç¿’çš„å‡½æ•¸å°±å¯ä»¥è¶Šè¤‡é›œã€‚é‚£éº¼èƒ½å¦ç”¨å°çš„æ¨¡åž‹ä¹Ÿå­¸ç¿’åˆ°é€™æ¨£è¤‡é›œçš„å‡½æ•¸å‘¢ï¼Ÿä¸€ç¨®æ–¹å¼ä¾¿æ˜¯çŸ¥è­˜é·ç§»ï¼ˆKT, Knowledge Transferï¼‰ï¼Œé€šéŽå°‡å¤§çš„DNNæ¨¡åž‹ç²å¾—çš„çŸ¥è­˜é·ç§»åˆ°å°çš„DNNæ¨¡åž‹ä¸Šã€‚ç‚ºäº†å­¸ç¿’è¤‡é›œå‡½æ•¸ï¼Œå°çš„ DNN æ¨¡åž‹æœƒåœ¨å¤§çš„ DNN æ¨¡åž‹æ¨™è¨˜è™•çš„æ•¸æ“šä¸Šé€²è¡Œè¨“ç·´ã€‚å…¶èƒŒå¾Œçš„æ€æƒ³æ˜¯ï¼Œå¤§çš„ DNN æ¨™è¨˜çš„æ•¸æ“šæœƒåŒ…å«å¤§é‡å°å°çš„DNNæœ‰ç”¨çš„ä¿¡æ¯ã€‚ä¾‹å¦‚å¤§çš„ DNN æ¨¡åž‹å°ä¸€å€‹è¼¸å…¥åœ–åƒåœ¨ä¸€äº›é¡žæ¨™ç±¤ä¸Šè¼¸å‡ºä¸­é«˜æ©ŸçŽ‡ï¼Œé‚£éº¼é€™å¯èƒ½æ„å‘³è‘—é€™äº›é¡žå…±äº«ä¸€äº›å…±åŒçš„è¦–è¦ºç‰¹å¾µï¼›å°æ–¼å°çš„ DNNæ¨¡åž‹ï¼Œå¦‚æžœåŽ»æ¨¡æ“¬é€™äº›æ©ŸçŽ‡ï¼Œç›¸æ¯”æ–¼ç›´æŽ¥å¾žæ•¸æ“šä¸­å­¸ç¿’ï¼Œè¦èƒ½å¤ å­¸åˆ°æ›´å¤šã€‚å…·é«”çš„ä½œæ³•ä¹‹ä¸€æ˜¯ Hinton åœ¨ 2014å¹´ æå‡ºçš„çŸ¥è­˜è’¸é¤¾ (KD, Knowledge Distillation)ï¼Œé€™ç¨®æ–¹æ³•çš„è¨“ç·´éŽç¨‹ç›¸æ¯”æ–¼çŸ¥è­˜é·ç§» (KT) è¦ç°¡å–®å¾—å¤šã€‚åœ¨çŸ¥è­˜è’¸é¤¾ä¸­ï¼Œå°çš„ DNN æ¨¡åž‹ä½¿ç”¨å­¸ç”Ÿ-æ•™å¸«æ¨¡å¼é€²è¡Œè¨“ç·´ï¼Œå…¶ä¸­å°çš„ DNN æ¨¡åž‹æ˜¯å­¸ç”Ÿï¼Œä¸€çµ„å°ˆé–€çš„ DNN æ¨¡åž‹æ˜¯æ•™å¸«ï¼›é€šéŽè¨“ç·´å­¸ç”Ÿï¼Œè®“å®ƒæ¨¡ä»¿æ•™å¸«çš„ softmax è¼¸å‡ºï¼Œå°çš„DNN æ¨¡åž‹å¯ä»¥å®Œæˆæ•´é«”çš„ä»»å‹™ã€‚ä½†åœ¨ Hinton çš„å·¥ä½œä¸­ï¼Œå°çš„ DNN æ¨¡åž‹çš„æº–ç¢ºåº¦å»ç›¸æ‡‰æœ‰äº›ä¸‹é™ã€‚ Li ç­‰äººåˆ©ç”¨æœ€å°åŒ–æ•™å¸«èˆ‡å­¸ç”Ÿä¹‹é–“ç‰¹å¾µå‘é‡çš„æ­æ°è·é›¢ï¼Œé€²ä¸€æ­¥æé«˜çš„å°çš„ DNN æ¨¡åž‹çš„ç²¾åº¦ã€‚é¡žä¼¼çš„ï¼ŒFitNet è®“å­¸ç”Ÿæ¨¡åž‹ä¸­çš„æ¯ä¸€å±¤éƒ½ä¾†æ¨¡ä»¿æ•™å¸«çš„ç‰¹å¾µåœ–ã€‚ä½†ä»¥ä¸Šå…©ç¨®æ–¹æ³•éƒ½è¦æ±‚å°å­¸ç”Ÿæ¨¡åž‹çš„çµæ§‹åšå‡ºåš´æ ¼çš„å‡è¨­ï¼Œå…¶æ³›åŒ–æ€§è¼ƒå·®ã€‚Knowledge transfer/distillation is very interesting and similar to proxy AI I thought before!Knowledge transfer or distillation æ˜¯å€‹éžå¸¸æœ‰è¶£è€Œä¸”å¯¦ç”¨çš„æŠ€è¡“ã€‚ä¾‹å¦‚ teacher model å¯ä»¥æ˜¯é›²ç«¯çš„å¤§ DNN æ¨¡åž‹ï¼Œæœ‰æ¯”è¼ƒå¥½çš„ç²¾åº¦ä»¥åŠæ³›åŒ–æ€§ã€‚ä½†åœ¨ edge or device å¯ä»¥ deploy student model, i.e. å° DNN æ¨¡åž‹ã€‚é›–ç„¶ç²¾åº¦å’Œæ³›åŒ–æ€§æ¯”è¼ƒå·®ï¼Œä½†æ˜¯ quick response, ä»¥åŠ edge and device ä¸ä¸€å®šéœ€è¦éžå¸¸å¼·çš„æ³›åŒ–æ€§ (e.g. local voice recognition, or local face detection).å„ªé»žï¼šåŸºæ–¼çŸ¥è­˜é·ç§»å’ŒçŸ¥è­˜è’¸é¤¾çš„æŠ€è¡“å¯ä»¥é¡¯è‘—é™ä½Žå¤§åž‹é è¨“ç·´æ¨¡åž‹çš„è¨ˆç®—æˆæœ¬ã€‚æœ‰ç ”ç©¶è¡¨æ˜Žï¼ŒçŸ¥è­˜è’¸é¤¾çš„æ–¹æ³•ä¸åƒ…å¯ä»¥åœ¨è¨ˆç®—æ©Ÿè¦–è¦ºä¸­æ‡‰ç”¨ï¼Œé‚„èƒ½ç”¨åˆ°è¨±å¤šä¾‹å¦‚åŠç›£ç£å­¸ç¿’ã€åŸŸè‡ªé©æ‡‰ç­‰ä»»å‹™ä¸­ã€‚ç¼ºé»žåŠæ”¹é€²æ–¹å‘ï¼šçŸ¥è­˜è’¸é¤¾é€šå¸¸å°å­¸ç”Ÿå’Œæ•™å¸«çš„çµæ§‹å’Œè¦æ¨¡æœ‰åš´æ ¼çš„å‡è¨­ï¼Œå› æ­¤å¾ˆé›£æŽ¨å»£åˆ°æ‰€æœ‰çš„æ‡‰ç”¨ä¸­ã€‚æ­¤å¤–ç›®å‰çš„çŸ¥è­˜è’¸é¤¾æŠ€è¡“åš´é‡ä¾è³´æ–¼ softmax è¼¸å‡ºï¼Œä¸èƒ½èˆ‡ä¸åŒçš„è¼¸å‡ºå±¤å”åŒå·¥ä½œã€‚ä½œç‚ºæ”¹é€²æ–¹å‘ï¼Œå­¸ç”Ÿå¯ä»¥å­¸ç¿’æ•™å¸«æ¨¡åž‹çš„ç¥žç¶“å…ƒæ¿€æ´»åºåˆ—ï¼Œè€Œä¸æ˜¯åƒ…åƒ…æ¨¡ä»¿æ•™å¸«çš„ç¥žç¶“å…ƒ/å±¤è¼¸å‡ºï¼Œé€™èƒ½å¤ æ¶ˆé™¤å°å­¸ç”Ÿå’Œæ•™å¸«çµæ§‹çš„é™åˆ¶ï¼ˆæé«˜æ³›åŒ–èƒ½åŠ›ï¼‰ï¼Œä¸¦æ¸›å°‘å°softmaxè¼¸å‡ºå±¤çš„ä¾è³´ã€‚Transfer learning is different from knowledge transfer/distillationThe objective of transfer learning and knowledge distillation are quite different. In transfer learning, the weights are transferred from a pre-trained network to a new network and the pre-trained network should exactly match the new network architecture.  What this means is that the new network is essentially as deep and complex as the pre-trained network.the objective of knowledge distillation is different. The aim is not to transfer weights but to transfer the generalizations of a complex model to a much lighter model.  å¦‚ä½• transfer generalizations?  ä½¿ç”¨ student-teach model æ˜¯ä¸€ç¨®æ–¹å¼ã€‚é‚„æœ‰å…¶ä»–çš„æ–¹å¼å¯ä»¥åƒè€ƒ [@kompellaTapDark2018].Level 4: Network Architecture Search (NAS)åœ¨è¨­è¨ˆä½ŽåŠŸè€—è¨ˆç®—æ©Ÿè¦–è¦ºç¨‹åºæ™‚ï¼Œé‡å°ä¸åŒçš„ä»»å‹™å¯èƒ½éœ€è¦ä¸åŒçš„DNNæ¨¡åž‹æž¶æ§‹ã€‚ä½†ç”±æ–¼å­˜åœ¨è¨±å¤šé€™ç¨®çµæ§‹ä¸Šçš„å¯èƒ½æ€§ï¼Œé€šéŽæ‰‹å·¥åŽ»è¨­è¨ˆä¸€å€‹æœ€ä½³DNNæ¨¡åž‹å¾€å¾€æ˜¯å›°é›£çš„ã€‚æœ€å¥½çš„è¾¦æ³•å°±æ˜¯å°‡é€™å€‹éŽç¨‹è‡ªå‹•åŒ–ï¼Œå³ç¶²çµ¡æž¶æ§‹æœç´¢æŠ€è¡“ï¼ˆNetwork Architecture Searchï¼‰ã€‚NASä½¿ç”¨ä¸€å€‹éžæ­¸ç¥žç¶“ç¶²çµ¡(RNN)ä½œç‚ºæŽ§åˆ¶å™¨ï¼Œä¸¦ä½¿ç”¨å¢žå¼·å­¸ç¿’ä¾†æ§‹å»ºå€™é¸çš„DNNæž¶æ§‹ã€‚å°é€™äº›å€™é¸DNNæž¶æ§‹é€²è¡Œè¨“ç·´ï¼Œç„¶å¾Œä½¿ç”¨é©—è­‰é›†é€²è¡Œæ¸¬è©¦ï¼Œæ¸¬è©¦çµæžœä½œç‚ºçŽå‹µå‡½æ•¸ï¼Œç”¨æ–¼å„ªåŒ–æŽ§åˆ¶å™¨çš„ä¸‹ä¸€å€‹å€™é¸æž¶æ§‹ã€‚NASNet å’ŒAmoebaNet è­‰æ˜Žçž­NASçš„æœ‰æ•ˆæ€§ï¼Œå®ƒå€‘é€šéŽæž¶æ§‹æœç´¢ç²å¾—DNNæ¨¡åž‹èƒ½å¤ ç²å¾—SOTAæ€§èƒ½ã€‚ç‚ºäº†ç²å¾—é‡å°ç§»å‹•è¨­å‚™æœ‰æ•ˆçš„DNNæ¨¡åž‹ï¼ŒTanç­‰äººæå‡ºäº†MNasNetï¼Œé€™å€‹æ¨¡åž‹åœ¨æŽ§åˆ¶å™¨ä¸­ä½¿ç”¨äº†ä¸€å€‹å¤šç›®æ¨™çŽå‹µå‡½æ•¸ã€‚åœ¨å¯¦é©—ä¸­ï¼ŒMNasNet è¦æ¯”NASNetå¿«2.3å€ï¼Œåƒæ•¸æ¸›å°‘4.8å€ï¼Œæ“ä½œæ¸›å°‘10å€ã€‚æ­¤å¤–ï¼ŒMNasNetä¹Ÿæ¯”NASNetæ›´æº–ç¢ºã€‚ä¸éŽï¼Œå„˜ç®¡NASæ–¹æ³•çš„æ•ˆæžœé¡¯è‘—ï¼Œä½†å¤§å¤šæ•¸NASç®—æ³•çš„è¨ˆç®—é‡éƒ½éžå¸¸å¤§ã€‚ä¾‹å¦‚ï¼ŒMNasNetéœ€è¦50,000å€‹GPU æ™‚æ‰èƒ½åœ¨ImageNetæ•¸æ“šé›†ä¸Šæ‰¾åˆ°ä¸€å€‹é«˜æ•ˆçš„DNNæž¶æ§‹ã€‚ç‚ºäº†æ¸›å°‘èˆ‡NASç›¸é—œçš„è¨ˆç®—æˆæœ¬ï¼Œä¸€äº›ç ”ç©¶äººå“¡å»ºè­°åŸºæ–¼ä»£ç†ä»»å‹™å’ŒçŽå‹µä¾†æœç´¢å€™é¸æž¶æ§‹ã€‚ä¾‹å¦‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘å€‘ä¸é¸ç”¨ImageNetï¼Œè€Œç”¨æ›´å°çš„æ•¸æ“šé›†CIFAR-10ã€‚FBNetæ­£æ˜¯é€™æ¨£ä¾†è™•ç†çš„ï¼Œå…¶é€Ÿåº¦æ˜¯MNasNetçš„420å€ã€‚ä½†Caiç­‰äººè¡¨æ˜Žï¼Œåœ¨ä»£ç†ä»»å‹™ä¸Šå„ªåŒ–çš„DNNæž¶æ§‹ä¸¦ä¸èƒ½ä¿è­‰åœ¨ç›®æ¨™ä»»å‹™ä¸Šæ˜¯æœ€å„ªçš„ï¼Œç‚ºäº†å…‹æœåŸºæ–¼ä»£ç†çš„NASè§£æ±ºæ–¹æ¡ˆæ‰€å¸¶ä¾†çš„å±€é™æ€§ï¼Œä»–å€‘æå‡ºäº†Proxyless-NASï¼Œé€™ç¨®æ–¹æ³•æœƒä½¿ç”¨è·¯å¾‘ç´šå‰ªæžä¾†æ¸›å°‘å€™é¸æž¶æ§‹çš„æ•¸é‡ï¼Œä¸¦ä½¿ç”¨åŸºæ–¼æ¢¯åº¦çš„æ–¹æ³•ä¾†è™•ç†å»¶é²ç­‰ç›®æ¨™ã€‚ä»–å€‘åœ¨300å€‹GPUæ™‚å…§ä¾¿æ‰¾åˆ°äº†ä¸€å€‹æœ‰æ•ˆçš„æž¶æ§‹ã€‚æ­¤å¤–ï¼Œä¸€ç¨®ç¨±ç‚ºå–®è·¯å¾‘NASï¼ˆSingle-Path NASï¼‰çš„æ–¹æ³•å¯ä»¥å°‡æž¶æ§‹æœç´¢æ™‚é–“å£“ç¸®åˆ° 4 å€‹GPUæ™‚å…§ï¼Œä¸éŽé€™ç¨®åŠ é€Ÿæ˜¯ä»¥é™ä½Žç²¾åº¦ç‚ºä»£åƒ¹çš„ã€‚å„ªé»žï¼šNASé€šéŽåœ¨æ‰€æœ‰å¯èƒ½çš„æž¶æ§‹ç©ºé–“ä¸­é€²è¡Œæœç´¢ï¼Œè€Œä¸éœ€è¦ä»»ä½•äººå·¥å¹²é ï¼Œè‡ªå‹•å¹³è¡¡æº–ç¢ºæ€§ã€å…§å­˜å’Œå»¶é²ä¹‹é–“çš„æ¬Šè¡¡ã€‚NASèƒ½å¤ åœ¨è¨±å¤šç§»å‹•è¨­å‚™ä¸Šå¯¦ç¾æº–ç¢ºæ€§ã€èƒ½è€—çš„æœ€ä½³æ€§èƒ½ã€‚ç¼ºé»žåŠæ”¹é€²æ–¹å‘ï¼šè¨ˆç®—é‡å¤ªå¤§ï¼Œå°Žè‡´å¾ˆé›£åŽ»æœç´¢å¤§åž‹æ•¸æ“šé›†ä¸Šä»»å‹™çš„æž¶æ§‹ã€‚å¦å¤–ï¼Œè¦æƒ³æ‰¾åˆ°æ»¿è¶³æ€§èƒ½éœ€æ±‚çš„æž¶æ§‹ï¼Œå¿…é ˆå°æ¯å€‹å€™é¸æž¶æ§‹é€²è¡Œè¨“ç·´ï¼Œä¸¦åœ¨ç›®æ¨™è¨­å‚™ä¸Šé‹è¡Œä¾†ç”ŸæˆçŽå‹µå‡½æ•¸ï¼Œé€™æœƒå°Žè‡´è¼ƒé«˜çš„è¨ˆç®—æˆæœ¬ã€‚å…¶å¯¦ï¼Œå¯ä»¥å°‡å€™é¸DNNåœ¨æ•¸æ“šçš„ä¸åŒå­é›†ä¸Šé€²è¡Œä¸¦è¡Œè¨“ç·´ï¼Œå¾žè€Œæ¸›å°‘è¨“ç·´æ™‚é–“ï¼›å¾žä¸åŒæ•¸æ“šå­é›†å¾—åˆ°çš„æ¢¯åº¦å¯ä»¥åˆä½µæˆä¸€å€‹ç¶“éŽè¨“ç·´çš„DNNã€‚ä¸éŽé€™ç¨®ä¸¦è¡Œè¨“ç·´æ–¹æ³•å¯èƒ½æœƒå°Žè‡´è¼ƒä½Žçš„æº–ç¢ºæ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨ä¿æŒé«˜æ”¶æ–‚çŽ‡çš„åŒæ™‚ï¼Œåˆ©ç”¨è‡ªé©æ‡‰å­¸ç¿’çŽ‡å¯ä»¥æé«˜æº–ç¢ºæ€§ã€‚Model Compression ExamplesEx1: Deep Compression by Han (Level 1)åƒè€ƒ fig.1 ä½¿ç”¨ pruning, quantization, and parameter compression.  æ•´é«”çš„æ•ˆç›Šå¦‚ä¸‹ã€‚ç²¾åº¦å’Œ AlexNet å·®ä¸å¤š (TBC)ã€‚  Step 1: Pruning (9x-13x)  Step 2: Quantizing clustered weights for weight sharing (32bit -&gt; 5bit) (~4x)  Step 3: Compression: encode weights/index for weight compression; Huffman encoding sparsity/zero and weight sharing compression.  Total: 35x-49xAccuracy result (TBA)Ex2: Model compression via distillation and quantization (Level 1+3)This excellent paper [@polinoModelCompression2018] 1 proposes two new compression methods, which jointly leverage weight quantization and distillation of larger networks, called â€œteachers,â€ into compressed â€œstudentâ€ networks.  ç°¡å–®èªª FP32 model æ˜¯ teacher model; quantized model æ˜¯ student model. Github code 2.The first method is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher network, into the training of a smaller student network whose weights are quantized to a limited set of levels. teacher model ä½¿ç”¨ FP32 deep model; student model å‰‡æ˜¯ uniformly quantized and shallow model.  è—‰è‘— distillation loss that teacher model can train student model.The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. ä½¿ç”¨å’Œ student model åŒæ¨£çš„å°æ¨¡åž‹ã€‚é‡é»žæ˜¯ linear but non-uniform quantization. ä½†æ²’æœ‰ distillation loss; è€Œæ˜¯ç”¨ä¸€èˆ¬çš„ cross-entropy loss to train this model and optimize the non-uniform quantization.  ç•¶ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨ non-uniform quantization for student model.  å¯èƒ½ computation æœƒå¤ªè¤‡é›œã€‚å…¶ä»–ç´°ç¯€è«‹åƒè€ƒ[@polinoModelCompression2018].  é€™è£ç›´æŽ¥è¨Žè«–çµæžœã€‚CIFAR10 accuracy.  Teacher model: 5.3M param of FP32, 21MB, accuracy 89.71%.  æœ‰ä¸‰ç¨® student models, åˆ†åˆ¥ç‚º 1M/0.3M/0.1M param å¦‚ä¸‹è¡¨å·¦ç¬¬ä¸€æ¬„ã€‚      ç¬¬ä¸€æ¬„ (A) éƒ½æ˜¯ FP32 full precision training. ä¾‹å¦‚ A1 ä»£è¡¨ student model 1, 1M param, FP32ï¼š4MB.  å…©å€‹ accuracy å°æ‡‰ cross-entropy loss and distillation loss.  Accuracy 84.5% å°æ‡‰ normal training (cross-entropy loss).  Accuracy 88.8% å°æ‡‰ teacher-student training (distillation loss).        ç¬¬äºŒæ¬„ (B) éƒ½æ˜¯ quantized training.  PM (post-mortem) Quant. åªæ˜¯æŠŠ FP32 teacher-student training çš„ weight ç›´æŽ¥ post training uniform quantization without any additional operation. æœ‰å…©ç¨® PM Quant., ä¸€æ˜¯ global scaling (no bucket), å¦ä¸€å€‹æ˜¯ local scaling (with bucket size = 256).  æ‰€ä»¥ (1) PM Quant. accuracy ä¸€å®šå·®æ–¼ FP32 accuracy (88.8%).  Quantized Distill. ä½¿ç”¨ distillation loss back propagation, å› æ­¤ accuracy is better than PM Quant.  In summary, FP Distill. &gt; Quantized Distill. &gt; PM (with bucket) &gt; PM (no bucket)        Differentiable Quant. ä½¿ç”¨ cross-entropy loss training.  å¾žå¦ä¸€å€‹è§’åº¦, non-uniform quantization, approach this problem.  åœ¨ 4-bits quantization çš„è¡¨ç¾ä¸è¼¸æ–¼ uniform distillation loss training.  ä½†åœ¨ 2-bits quantization distillation loss training é‚„æ˜¯æ¯”è¼ƒå¥½ã€‚åˆç†æŽ¨è«–ï¼Œdifferentiable non-uniform quantization with distillation loss æ‡‰è©²æœƒæœ‰æœ€ä½³çš„ accuracy.    Differentiable quantization åœ¨ computation ä¸å®¹æ˜“åœ¨ edge AI å¯¦ç¾ï¼Œå› ç‚º quantized values ä¸æœƒè½åœ¨ linear grid ä¸Šï¼Œå¾ˆé›£ç”¨ finite bitwidth è¡¨ç¤ºï¼Œä¹Ÿå¾ˆé›£åš MAC è¨ˆç®—ã€‚æ¯”è¼ƒæŽ¥è¿‘çš„è§£æ³•æ˜¯ç”¨ k-mean clustering algorithm to cluster weights and adopt the centroids as quantization points. Han ç¨±ç‚º weight sharing.  æœ€ä½³çš„çµæžœ assuming accuracy loss &lt; 2% compared with baseline (89.71%) is student model 1 (1M param) of 4-bit with accuracy 88%. The total size of best student model 1 is: 1M param x 0.5 = 0.5MB.  A factor of 21MB/0.5MB = 42x saving in memory/bandwidth/computation etc.!!!æ›´å¥½çš„çµæžœæ˜¯ç”¨æ¯”è¼ƒ deeper student model, ä½†æ˜¯ç”¨ 4-bit (5.8M x 0.5 = 2.9MB), è€Œä¸” accuracy é‚„æ¯”è¼ƒå¥½ (92.3% vs. 89.71%).  A factor of 21MB/2.9MB = 7.2x saving in memory/bandwidth/computation etc.!!!CIFAR100 accuracy.  Teacher model: 36.5M param of FP32, 146MB, accuracy 77.21%.  Student model is 17.2M param, about 1/2 of teacher model.  4-bit model is 8.2MB with accuracy 76.31%.  A factor of 146MB/8.2MB = 17.8x saving.  Differential quantization seems to perform better, but with more params and more complicated computation.Imagenet accuracy.  Teacher model: ResNet34 or ResNet50  Student model: 2xResNet18 QD 4 bit and 2xResNet34 QD 4 bit  Student model of 4-bit å¯ä»¥å¾—åˆ°å’Œ teacher model é¡žä¼¼ accuracy.  ä½† size æ¯”èµ· teacher FP32 model å°‘äº† 2x-4x.Distillation + Quantization çµè«–ï¼š  FP32 to INT8 å·²ç¶“æœ‰å¾ˆå¤š post-training quantization or quantization aware training å¯ä»¥é”åˆ°åŒæ¨£çš„ accuracy.  å› æ­¤ 4x saving å·²ç¶“å¾ˆæ™®é€šã€‚  Teacher-student models åœ¨å° dataset (CIFAR10), model compression æ•ˆæžœæ¯”è¼ƒçªå‡º: (1) 8x from FP32 to 4-bit;  (2) student model å¯ä»¥æ¯” teacher model gain 4x-5x.  ä½†å°æ–¼å¤§ dataset, CIFAR100 or ImageNet, student model param å·²ç¶“æŽ¥è¿‘ç”šè‡³è¶…éŽ teacher model param.  æ­¤æ™‚åªæœ‰ FP32 to 4-bit gain.  å°æ–¼å¤§ dataset, knowledge distillation çš„çµæžœä¸¦ä¸çªå‡ºã€‚Summary  Level 1 + Level 2 compression å¯ä»¥åŒæ™‚ä½¿ç”¨ã€‚å¢žåŠ å£“ç¸®çš„å€çŽ‡ã€‚  Level 1 + Level 3 compression å¯ä»¥åŒæ™‚ä½¿ç”¨ã€‚å°æ–¼å¤§ dataset æ•ˆæžœæœ‰é™ã€‚ä½†æ˜¯å° dataset ä¼¼ä¹Žä¸éŒ¯ã€‚  Level 4 NAS é›–ç„¶çœ‹èµ·ä¾†å¾ˆå¥½ï¼Œä½†æ˜¯éœ€è¦å¤ªå¤šçš„ computation resource/time to search.  éœ€è¦æ›´å¥½çš„æ–¹å¼ç”¨æ–¼ edge AI.ReferenceCheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2019. â€œA Survey of ModelCompression and Acceleration for Deep Neural Networks,â€ September.https://arxiv.org/abs/1710.09282v8.Goel, Abhinav, Caleb Tung, Yung-Hsiang Lu, and George K. Thiruvathukal. 2020. â€œA Survey of Methods for Low-Power Deep Learning and Computer Vision,â€ March. http://arxiv.org/abs/2003.11066.Han, Song, Huizi Mao, and William J. Dally. 2016. â€œDeep Compression:Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,â€ February. http://arxiv.org/abs/1510.00149.Han, Song, Jeff Pool, John Tran, and William J. Dally. 2015. â€œLearning Both Weights and Connections for Efficient Neural Networks,â€ October. http://arxiv.org/abs/1506.02626.Kompella, Ravindra. n.d. â€œTap into the Dark Knowledge Using Neural Nets Distillation.â€ Medium. Accessed April 2, 2020.https://towardsdatascience.com/knowledge-distillation-and-the-concept-of-dark-knowledge-8b7aed8014ac.Kuzmin, Andrey, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, TijmenBlankevoort, and Max Welling. 2019. â€œTaxonomy and Evaluation ofStructured Compression of Convolutional Neural Networks,â€ December.http://arxiv.org/abs/1912.09802.Polino, Antonio, Razvan Pascanu, and Dan Alistarh. 2018. â€œModelCompression via Distillation and Quantization,â€ February.http://arxiv.org/abs/1802.05668.            Published in ICLR 2018Â &#8617;              https://github.com/antspy/quantized_distillationÂ &#8617;      ]]></content>
      <categories>
        
          <category> AI </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> quantization </tag>
        
          <tag> model compression </tag>
        
          <tag> pruning </tag>
        
          <tag> distillation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[å¢žé€²å·¥ç¨‹å¸«æ•ˆçŽ‡ Python DataFrame - CSV & Plot]]></title>
      <url>/language/2018/12/22/dataframe/</url>
      <content type="text"><![CDATA[Download the code: https://github.com/allenlu2009/colab/blob/master/dataframe_demo.ipynbPython DataFrameCreate DataFrame      Direct input        Use dict: Method 1: ä¸€ç­†ä¸€ç­†åŠ å…¥ã€‚  import pandas as pddict1 = {'Name': 'Allen' , 'Sex': 'male', 'Age': 33}dict2 = {'Name': 'Alice' , 'Sex': 'female', 'Age': 22}dict3 = {'Name': 'Bob' , 'Sex': 'male', 'Age': 11}data = [dict1, dict2, dict3]df = pd.DataFrame(data)df                  Name      Sex      Age                  0      Allen      male      33              1      Alice      female      22              2      Bob      male      11      Method 2: ä¸€æ¬¡åŠ å…¥æ‰€æœ‰è³‡æ–™ã€‚name = ['Allen', 'Alice', 'Bob']sex = ['male', 'female', 'male']age = [33, 22, 11]all_dict = {    "Name": name,    "Sex": sex,    "Age": age}df = pd.DataFrame(all_dict)df[['Name', 'Age']]                  Name      Age                  0      Allen      33              1      Alice      22              2      Bob      11      Dataframe çš„å±¬æ€§  ndim: 2 for 2D dataframe; axis 0 =&gt; row; axis 1 =&gt; column  shape:  (row no. x column no.) (not including number index)  dtypes: (object or int) of each columndf.ndim2df.shape(3, 3)df.dtypesName    objectSex     objectAge      int64dtype: objectdf.columnsIndex(['Name', 'Sex', 'Age'], dtype='object')df.indexRangeIndex(start=0, stop=3, step=1)Read CSVDonwload a test csv file from https://people.sc.fsu.edu/~jburkardt/data/csv/csv.html  Pick the biostats.csvFor 2, Before read csv, reference Medium article to import google drive  Read csv ä½¿ç”¨ read_csv function.  ä½†æ˜¯è¦åŠ ä¸Š skipinitialspace to strip the leading space!!  Two ways to read_csv: (1) load csv file directly; (2) load from urlimport pandas as pdfrom google.colab import drivedrive.mount('/content/drive')#!ls 'drive/My Drive/Colab Notebooks/'df = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', skipinitialspace=True)dfGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonlyEnter your authorization code:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Mounted at /content/drive                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      url = "https://people.sc.fsu.edu/~jburkardt/data/csv/biostats.csv"df = pd.read_csv(url, skipinitialspace=True)df                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      print(df.columns); print(df.index)Index(['Name', 'Sex', 'Age', 'Height (in)', 'Weight (lbs)'], dtype='object')RangeIndex(start=0, stop=18, step=1)df.ndim2df.shape(18, 5)df.dtypesName            objectSex             objectAge              int64Height (in)      int64Weight (lbs)     int64dtype: objectBasic Viewing Commanddf.head(3)                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155      df.tail(3)                  Name      Sex      Age      Height (in)      Weight (lbs)                  15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      df.shape(18, 5)df.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 18 entries, 0 to 17Data columns (total 5 columns):Name            18 non-null objectSex             18 non-null objectAge             18 non-null int64Height (in)     18 non-null int64Weight (lbs)    18 non-null int64dtypes: int64(3), object(2)memory usage: 848.0+ bytesdf[7:10]                  Name      Sex      Age      Height (in)      Weight (lbs)                  7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143      df['Name'][7:10]7    Hank8    Ivan9    JakeName: Name, dtype: objectdf[['Name', 'Age', 'Sex']][7:10]                  Name      Age      Sex                  7      Hank      30      M              8      Ivan      53      M              9      Jake      32      M      df.loc[7:10, ['Name', 'Age', 'Sex']] # compare with loc call                  Name      Age      Sex                  7      Hank      30      M              8      Ivan      53      M              9      Jake      32      M              10      Kate      47      F      df.count()Name            18Sex             18Age             18Height (in)     18Weight (lbs)    18dtype: int64Basic Index OperationIndex (ç´¢å¼•) is a very useful key for DataFrame.  The default index is the row number starting from 0 to N-1, where N is the number of data.é™¤äº†ç”¨ row number åšç‚º index, ä¸€èˆ¬ä¹Ÿæœƒä½¿ç”¨ unique feature ä¾‹å¦‚ name, id, or phone number åšç‚º index.æŠŠ column è®Šæˆ index  Method 1: ç›´æŽ¥åœ¨ read_csv æŒ‡å®š index_col.  å¯ä»¥çœ‹åˆ° index number æ¶ˆå¤±ï¼Œè€Œè¢« Name column å–ä»£ã€‚df = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', skipinitialspace=True, index_col='Name')df                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131        df.index shows the element in index columndf.indexIndex(['Alex', 'Bert', 'Carl', 'Dave', 'Elly', 'Fran', 'Gwen', 'Hank', 'Ivan',       'Jake', 'Kate', 'Luke', 'Myra', 'Neil', 'Omar', 'Page', 'Quin', 'Ruth'],      dtype='object', name='Name')  ä½¿ç”¨ reset_index åˆæœƒå›žåˆ° index number.df.reset_index()                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      å†çœ‹ä¸€æ¬¡ df ä¸¦æ²’æœ‰æ”¹è®Šã€‚å¾ˆå¤š DataFrame çš„ function éƒ½æ˜¯ä¿ç•™åŽŸå§‹çš„ df, create a new object, ä¹Ÿå°±æ˜¯ inplace = False.   å¦‚æžœè¦å–ä»£åŽŸä¾†çš„ df, å¿…é ˆ inplace = True!df                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.reset_index(inplace=True)df                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      å¦‚æžœå† reset_index(ï¼‰ä¸€æ¬¡ï¼Œæœƒæ˜¯ä»€éº¼çµæžœï¼Ÿæ­¤è™•ç”¨ default inplace=False.å¤šäº†ä¸€å€‹ index columndf.reset_index()                  index      Name      Sex      Age      Height (in)      Weight (lbs)                  0      0      Alex      M      41      74      170              1      1      Bert      M      42      68      166              2      2      Carl      M      32      70      155              3      3      Dave      M      39      72      167              4      4      Elly      F      30      66      124              5      5      Fran      F      33      66      115              6      6      Gwen      F      26      64      121              7      7      Hank      M      30      71      158              8      8      Ivan      M      53      72      175              9      9      Jake      M      32      69      143              10      10      Kate      F      47      69      139              11      11      Luke      M      34      72      163              12      12      Myra      F      23      62      98              13      13      Neil      M      36      75      160              14      14      Omar      M      38      70      145              15      15      Page      F      31      67      135              16      16      Quin      M      29      71      176              17      17      Ruth      F      28      65      131      df                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131        Method 2: ä½¿ç”¨ set_index()df.set_index('Name', inplace=True)df                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      loc[]ä½¿ç”¨ loc[] é…åˆ index label å–å‡ºè³‡æ–™éžå¸¸æ–¹ä¾¿ã€‚\å¦‚æžœæ˜¯ number index, å¯ä»¥ç”¨ df[0], df[3], etc.\ä½†å¦‚æžœæ˜¯å…¶ä»– column index, e.g. Name, df[2] æˆ–æ˜¯ df[â€œHankâ€] are wrong!, å¿…é ˆç”¨ df.loc[â€˜Hankâ€™]\æˆ–æ˜¯ df.loc[ [â€˜Hankâ€™, â€˜Ruthâ€™, â€˜Pageâ€™] ]df.loc['Hank']Sex               MAge              30Height (in)      71Weight (lbs)    158Name: Hank, dtype: objectdf.loc[:, ['Sex', 'Age']]                  Sex      Age              Name                              Alex      M      41              Bert      M      42              Carl      M      32              Dave      M      39              Elly      F      30              Fran      F      33              Gwen      F      26              Hank      M      30              Ivan      M      53              Jake      M      32              Kate      F      47              Luke      M      34              Myra      F      23              Neil      M      36              Omar      M      38              Page      F      31              Quin      M      29              Ruth      F      28      df.loc[ ['Hank', 'Ruth', 'Page'] ]                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Hank      M      30      71      158              Ruth      F      28      65      131              Page      F      31      67      135      loc[] å¯ä»¥ç”¨ row, column å¾—åˆ°å°æ‡‰çš„ element, ä¼¼ä¹Žæ˜¯å¥‡æ€ªçš„ç”¨æ³•df.loc['Hank', 'Age']30iloc[]ä½¿ç”¨ column index ä»ç„¶å¯ä»¥ç”¨ iloc[] é…åˆ index number å–å‡ºè³‡æ–™ã€‚df.iloc[0]Sex               MAge              41Height (in)      74Weight (lbs)    170Name: Alex, dtype: objectdf.iloc[1:10]                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143      df.iloc[[1, 4, 6]]                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Bert      M      42      68      166              Elly      F      30      66      124              Gwen      F      26      64      121      æŽ’åºåŒ…å«å…©ç¨®æŽ’åº  sort_index()  sort_value()df.sort_index()                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.sort_values(by = 'Age')                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Myra      F      23      62      98              Gwen      F      26      64      121              Ruth      F      28      65      131              Quin      M      29      71      176              Elly      F      30      66      124              Hank      M      30      71      158              Page      F      31      67      135              Carl      M      32      70      155              Jake      M      32      69      143              Fran      F      33      66      115              Luke      M      34      72      163              Neil      M      36      75      160              Omar      M      38      70      145              Dave      M      39      72      167              Alex      M      41      74      170              Bert      M      42      68      166              Kate      F      47      69      139              Ivan      M      53      72      175      Rename and Drop Column(s) and Index(s)df.rename(columns={"Height (in)": "Height", "Weight (lbs)": "Weight"}, inplace=True)df                  Sex      Age      Height      Weight              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.rename(index={"Alex": "Allen", "Bert": "Bob"}, inplace=True)df                  Sex      Age      Height      Weight              Name                                          Allen      M      41      74      170              Bob      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.drop(labels=['Sex', 'Weight'], axis="columns") # axis=1 eq axis="columns"                  Age      Height              Name                              Allen      41      74              Bob      42      68              Carl      32      70              Dave      39      72              Elly      30      66              Fran      33      66              Gwen      26      64              Hank      30      71              Ivan      53      72              Jake      32      69              Kate      47      69              Luke      34      72              Myra      23      62              Neil      36      75              Omar      38      70              Page      31      67              Quin      29      71              Ruth      28      65      df.drop(labels=['Allen', 'Ruth'], axis="index") # axis=0 eq axis="index"                  Sex      Age      Height      Weight              Name                                          Bob      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176      é€²éšŽæŠ€å·§Multiple Index (å¤šé‡ç´¢å¼•)é€™æ˜¯éžå¸¸æœ‰ç”¨çš„æŠ€å·§ï¼Œä½¿ç”¨ set_index with keysdf = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', skipinitialspace=True)df  # show the original dataframe                  Name      Sex      Age      Height (in)      Weight (lbs)                  0      Alex      M      41      74      170              1      Bert      M      42      68      166              2      Carl      M      32      70      155              3      Dave      M      39      72      167              4      Elly      F      30      66      124              5      Fran      F      33      66      115              6      Gwen      F      26      64      121              7      Hank      M      30      71      158              8      Ivan      M      53      72      175              9      Jake      M      32      69      143              10      Kate      F      47      69      139              11      Luke      M      34      72      163              12      Myra      F      23      62      98              13      Neil      M      36      75      160              14      Omar      M      38      70      145              15      Page      F      31      67      135              16      Quin      M      29      71      176              17      Ruth      F      28      65      131      df.set_index(keys = ['Name', 'Sex'])  # Notice "Name" "Sex" columns header is lower than the rest                        Age      Height (in)      Weight (lbs)              Name      Sex                                    Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      df.set_index(keys = ['Sex', 'Name'], inplace=True)df# Note that key sequence matters; and same index values group# Note that inplace=True replaces the original df # This is useful to display sorted group                        Age      Height (in)      Weight (lbs)              Sex      Name                                    M      Alex      41      74      170              Bert      42      68      166              Carl      32      70      155              Dave      39      72      167              F      Elly      30      66      124              Fran      33      66      115              Gwen      26      64      121              M      Hank      30      71      158              Ivan      53      72      175              Jake      32      69      143              F      Kate      47      69      139              M      Luke      34      72      163              F      Myra      23      62      98              M      Neil      36      75      160              Omar      38      70      145              F      Page      31      67      135              M      Quin      29      71      176              F      Ruth      28      65      131      df.indexMultiIndex([('M', 'Alex'),            ('M', 'Bert'),            ('M', 'Carl'),            ('M', 'Dave'),            ('F', 'Elly'),            ('F', 'Fran'),            ('F', 'Gwen'),            ('M', 'Hank'),            ('M', 'Ivan'),            ('M', 'Jake'),            ('F', 'Kate'),            ('M', 'Luke'),            ('F', 'Myra'),            ('M', 'Neil'),            ('M', 'Omar'),            ('F', 'Page'),            ('M', 'Quin'),            ('F', 'Ruth')],           names=['Sex', 'Name'])df.index.namesFrozenList(['Sex', 'Name'])type(df.index)  # MultiIndexpandas.core.indexes.multi.MultiIndexdf.sort_index(inplace=True)df# sorting is based on "Sex", and then "Name"                        Age      Height (in)      Weight (lbs)              Sex      Name                                    F      Elly      30      66      124              Fran      33      66      115              Gwen      26      64      121              Kate      47      69      139              Myra      23      62      98              Page      31      67      135              Ruth      28      65      131              M      Alex      41      74      170              Bert      42      68      166              Carl      32      70      155              Dave      39      72      167              Hank      30      71      158              Ivan      53      72      175              Jake      32      69      143              Luke      34      72      163              Neil      36      75      160              Omar      38      70      145              Quin      29      71      176      Groupby CommandGroupby æ˜¯ SQL çš„èªžæ³•ã€‚æ ¹æ“šæŸä¸€é …è³‡æ–™åšåˆ†çµ„æ–¹ä¾¿æŸ¥æ‰¾ã€‚\The SQL GROUP BY StatementThe GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns.df = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', index_col="Name", skipinitialspace=True)df  # show the dataframe with "Name" index column                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Elly      F      30      66      124              Fran      F      33      66      115              Gwen      F      26      64      121              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Kate      F      47      69      139              Luke      M      34      72      163              Myra      F      23      62      98              Neil      M      36      75      160              Omar      M      38      70      145              Page      F      31      67      135              Quin      M      29      71      176              Ruth      F      28      65      131      grpBySex = df.groupby('Sex')  # output is a DataFrameGroupBy objecttype(grpBySex)pandas.core.groupby.generic.DataFrameGroupBygrpBySex.groups  # output is a dict, use get_group() obtains each sub-group{'F': Index(['Elly', 'Fran', 'Gwen', 'Kate', 'Myra', 'Page', 'Ruth'], dtype='object', name='Name'), 'M': Index(['Alex', 'Bert', 'Carl', 'Dave', 'Hank', 'Ivan', 'Jake', 'Luke', 'Neil',        'Omar', 'Quin'],       dtype='object', name='Name')}grpBySex.size()  # size() shows the counts of each groupSexF     7M    11dtype: int64grpBySex.get_group('M')  # get_group() output a DataFrame object                  Sex      Age      Height (in)      Weight (lbs)              Name                                          Alex      M      41      74      170              Bert      M      42      68      166              Carl      M      32      70      155              Dave      M      39      72      167              Hank      M      30      71      158              Ivan      M      53      72      175              Jake      M      32      69      143              Luke      M      34      72      163              Neil      M      36      75      160              Omar      M      38      70      145              Quin      M      29      71      176      Groupby Operationåˆ†çµ„å¾Œå¯ä»¥é€²è¡Œå„é¡žé‹ç®—ï¼šsum(), mean(), max(), min()grpBySex.sum()                  Age      Height (in)      Weight (lbs)              Sex                                    F      218      459      863              M      406      784      1778      grpBySex.mean()                  Age      Height (in)      Weight (lbs)              Sex                                    F      31.142857      65.571429      123.285714              M      36.909091      71.272727      161.636364      grpBySex.max()                  Age      Height (in)      Weight (lbs)              Sex                                    F      47      69      139              M      53      75      176      grpBySex.min()                  Age      Height (in)      Weight (lbs)              Sex                                    F      23      62      98              M      29      68      143      Wash Data with NANåˆ¤æ–· NAN  isnull()  notnull()è™•ç† NAN  dropna()  fillna()import numpy as npimport pandas as pdgroups = ["Modern Web", "DevOps", np.nan, "Big Data", "Security", "è‡ªæˆ‘æŒ‘æˆ°çµ„"]ironmen = [59, 9, 19, 14, 6, np.nan]ironmen_dict = {                "groups": groups,                "ironmen": ironmen}# å»ºç«‹ data frameironmen_df = pd.DataFrame(ironmen_dict)print(ironmen_df.loc[:, "groups"].isnull()) # åˆ¤æ–·å“ªäº›çµ„çš„çµ„åæ˜¯éºå¤±å€¼print("---") # åˆ†éš”ç·šprint(ironmen_df.loc[:, "ironmen"].notnull()) # åˆ¤æ–·å“ªäº›çµ„çš„éµäººæ•¸ä¸æ˜¯éºå¤±å€¼ironmen_df_na_dropped = ironmen_df.dropna() # æœ‰éºå¤±å€¼çš„è§€æ¸¬å€¼éƒ½åˆªé™¤print(ironmen_df_na_dropped)print("---") # åˆ†éš”ç·šironmen_df_na_filled = ironmen_df.fillna(0) # æœ‰éºå¤±å€¼çš„è§€æ¸¬å€¼å¡«è£œ 0print(ironmen_df_na_filled)print("---") # åˆ†éš”ç·šironmen_df_na_filled = ironmen_df.fillna({"groups": "Cloud", "ironmen": 71}) # ä¾æ¬„ä½å¡«è£œéºå¤±å€¼print(ironmen_df_na_filled)0    False1    False2     True3    False4    False5    FalseName: groups, dtype: bool---0     True1     True2     True3     True4     True5    FalseName: ironmen, dtype: bool       groups  ironmen0  Modern Web     59.01      DevOps      9.03    Big Data     14.04    Security      6.0---       groups  ironmen0  Modern Web     59.01      DevOps      9.02           0     19.03    Big Data     14.04    Security      6.05       è‡ªæˆ‘æŒ‘æˆ°çµ„      0.0---       groups  ironmen0  Modern Web     59.01      DevOps      9.02       Cloud     19.03    Big Data     14.04    Security      6.05       è‡ªæˆ‘æŒ‘æˆ°çµ„     71.0PlotDataFrame ä¸€å€‹å¾ˆé‡è¦çš„ç‰¹æ€§æ˜¯åˆ©ç”¨ matplotlib.pyplot ç¹ªåœ–åŠŸèƒ½ visuallize data!\æœ‰å…©ç¨®æ–¹å¼ï¼š(1) ç›´æŽ¥ç”¨ df.plot; (2) ç”¨ pyplot çš„ plot.\(1) æ˜¯ä¸€å€‹ quick way to plot \(2) å¯ä»¥èª¿ç”¨ pyplot æ‰€æœ‰çš„åŠŸèƒ½import matplotlib.pyplot as pltdf = pd.read_csv('drive/My Drive/Colab Notebooks/biostats.csv', index_col="Name", skipinitialspace=True)df.plot(title="Generated Plot", grid=True, figsize=(8,4))&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f952bc52240&gt;df.columnsplt.plot(df[['Age', 'Height (in)']])plt.xlabel('Name')plt.ylabel('Number')plt.title('Generated Plot')plt.grid()]]></content>
      <categories>
        
          <category> Language </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> pandas </tag>
        
          <tag> DataFrame </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RNN]]></title>
      <url>/foo/2018/12/22/rnn/</url>
      <content type="text"><![CDATA[import sysprint(sys.version)import tensorflowprint(tensorflow.__version__)import kerasprint(keras.__version__)import pandas as pdimport numpy as npimport mathimport randomimport matplotlib.pyplot as plt%matplotlib inline3.6.9 (default, Nov  7 2019, 10:44:02) [GCC 8.3.0]The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic:more info.1.15.02.2.5Using TensorFlow backend.Generate a sin wave with no noiseFirst, I create a function that generates sin wave with/without noise. Using this function, I will generate a sin wave with no noise. As this sin wave is completely deterministic, I should be able to create a model that can do prefect prediction the next value of sin wave given the previous values of sin waves!Here I generate period-10 sin wave, repeating itself 500 times, and plot the first few cycles.def noisy_sin(steps_per_cycle = 50,              number_of_cycles = 500,              random_factor = 0.4):    '''    number_of_cycles : The number of steps required for one cycle        Return :     pd.DataFrame() with column sin_t containing the generated sin wave     '''    random.seed(0)    df = pd.DataFrame(np.arange(steps_per_cycle * number_of_cycles + 1), columns=["t"])    df["sin_t"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)+ random.uniform(-1.0, +1.0) * random_factor))    df["sin_t_clean"] = df.t.apply(lambda x: math.sin(x * (2 * math.pi / steps_per_cycle)))    print("create period-{} sin wave with {} cycles".format(steps_per_cycle,number_of_cycles))    print("In total, the sin wave time series length is {}".format(steps_per_cycle*number_of_cycles+1))    return(df)steps_per_cycle = 10df = noisy_sin(steps_per_cycle=steps_per_cycle,              random_factor = 0)n_plot = 8df[["sin_t"]].head(steps_per_cycle * n_plot).plot(      title="Generated first {} cycles".format(n_plot),      figsize=(15,3))create period-10 sin wave with 500 cyclesIn total, the sin wave time series length is 5001&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4922f362e8&gt;Create a training and testing data. Here, the controversial â€œlength of time seriesâ€ parameter comes into play. For now, we set this parameter to 2.def _load_data(data, n_prev = 100):      """    data should be pd.DataFrame()    """    docX, docY = [], []    for i in range(len(data)-n_prev):        docX.append(data.iloc[i:i+n_prev].as_matrix())        docY.append(data.iloc[i+n_prev].as_matrix())    alsX = np.array(docX)    alsY = np.array(docY)    return alsX, alsYlength_of_sequences = 2test_size = 0.25ntr = int(len(df) * (1 - test_size))df_train = df[["sin_t"]].iloc[:ntr]df_test  = df[["sin_t"]].iloc[ntr:](X_train, y_train) = _load_data(df_train, n_prev = length_of_sequences)(X_test, y_test)   = _load_data(df_test, n_prev = length_of_sequences)  print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.  /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.  if __name__ == '__main__':(3748, 2, 1) (3748, 1) (1249, 2, 1) (1249, 1)Simple RNN modelAs a deep learning model, I consider the simplest possible RNN model: RNN with a single hidden unit followed by fully connected layer with a single unit.  The RNN layer contains 3 weights: 1 weight for input, 1 weight for hidden unit, 1 weight for bias  The fully connected layer contains 2 weights: 1 weight for input (i.e., the output from the previous RNN layer), 1 weight for biasIn total, there are only 5 weights in this model.Let $x_t$ be the sin wave at time point $t$, then Formally, This simple model can be formulated in two lines as:Conventionally $h_0=0$. Notice that the length of time series is not involved in the definition of the RNN. The model should be able to â€œrememberâ€ the past history of $x_t$ through the hidden unit $h_t$.batch_shape needs for BPTT.Â¶  Every time when the model weights are updated, the BPTT uses only the randomly selected subset of the data.  This means that the each batch is treated as independent.  This batch_shape determines the size of this subset.  Every batch starts will the initial hidden unit $h_0=0$.  As we specify the length of the time series to be 2, our model only knows about the past 2 sin wave values to predict the next sin wave value.  The practical limitation of the finite length of the time series defeats the theoretical beauty of RNN: the RNN here is not a model remembeing infinite past sequence!!!Now, we define this model using Keras and show the model summary.from keras.layers import Inputfrom keras.models import Modelfrom keras.layers.core import Dense, Activation from keras.layers.recurrent import SimpleRNNdef define_model(length_of_sequences, batch_size = None, stateful = False):    in_out_neurons = 1    hidden_neurons = 1    inp = Input(batch_shape=(batch_size,                 length_of_sequences,                 in_out_neurons))      rnn = SimpleRNN(hidden_neurons,                     return_sequences=False,                    stateful = stateful,                    name="RNN")(inp)    dens = Dense(in_out_neurons,name="dense")(rnn)    model = Model(inputs=[inp],outputs=[dens])        model.compile(loss="mean_squared_error", optimizer="rmsprop")        return(model,(inp,rnn,dens))## use the default values for batch_size, statefulmodel, (inp,rnn,dens) = define_model(length_of_sequences = X_train.shape[1])model.summary()WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.Model: "model_1"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_1 (InputLayer)         (None, 2, 1)              0         _________________________________________________________________RNN (SimpleRNN)              (None, 1)                 3         _________________________________________________________________dense (Dense)                (None, 1)                 2         =================================================================Total params: 5Trainable params: 5Non-trainable params: 0_________________________________________________________________Now we train the model. The script was run without GPU.hist = model.fit(X_train, y_train, batch_size=600, epochs=1000,                  verbose=False,validation_split=0.05)WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.Plot of val_loss and loss.The validation loss and loss are exactly the same because our training data is a sin wave with no noise. Both validation and training data contain identical 10-period sin waves (with different number of cycles). The final validation loss is less than 0.001.for label in ["loss","val_loss"]:    plt.plot(hist.history[label],label=label)plt.ylabel("loss")plt.xlabel("epoch")plt.title("The final validation loss: {}".format(hist.history["val_loss"][-1]))plt.legend()plt.show()The plot of true and predicted sin waves look nearly identicaly_pred = model.predict(X_test)plt.figure(figsize=(19,3))plt.plot(y_test,label="true")plt.plot(y_pred,label="predicted")plt.legend()plt.show()What are the model weights?The best way to understand the RNN model is to create a model from scratch. Letâ€™s extract the weights and try to reproduce the predicted values from the model by hands. The model weights can be readily obtained from the model.layers.ws = {}for layer in model.layers:    ws[layer.name] = layer.get_weights()ws{'RNN': [array([[-0.43695387]], dtype=float32),  array([[-0.64668506]], dtype=float32),  array([0.00117508], dtype=float32)], 'dense': [array([[-3.7658346]], dtype=float32),  array([-0.00123706], dtype=float32)], 'input_1': []}What are the predicted values of hidden units?Since we used Kerasâ€™s functional API to develop a model, we can easily see the output of each layer by compiling another model with outputs specified to be the layer of interest.In order to use the .predict() function, we need to compile the model, which requires specifying loss and optimizer. You can choose any values of loss and optimizer here, as we do not actually optimize this loss function. The newly created model â€œrnn_modelâ€ shares the weights obtained by the previous modelâ€™s optimization. Therefore for the purpose of visualizing the hidden unit values with the current model result, we do not need to do additional optimizations.rnn_model = Model(inputs=[inp],outputs=[rnn])rnn_model.compile(loss="mean_squared_error", optimizer="rmsprop")hidden_units = rnn_model.predict(X_test).flatten()Plot shows that the predicted hidden unit is capturing the wave shape. Scaling and shifting of the predicted hidden unit yield the predicted sin wave.upto = 100predicted_sin_wave = ws["dense"][0][0][0]*hidden_units + ws["dense"][1][0]plt.figure(figsize=(19,3))plt.plot(y_test[:upto],label="y_pred")plt.plot(hidden_units[:upto],label="hidden units")plt.plot(predicted_sin_wave[:upto],"*",         label="w2 * hidden units + b2")plt.legend()plt.show()Obtain predicted sin wave at the next time point given the current sin wave by handWe understand that how the predicted sin wave values can be obtained using the predicted hidden states from Keras. But how does the predicted hidden states generated from the original inputs i.e. the current sin wave? Here, stateful and stateless prediction comes into very important role. Following the definition of the RNN, we can write a script for RNNmodel as:def RNNmodel(ws,x,h=0):    '''    ws: predicted weights     x : scalar current sign value    h : scalar RNN hidden unit     '''               h = np.tanh(x*ws["RNN"][0][0][0] + h*ws["RNN"][1][0][0] + ws["RNN"][2][0])    x = h*ws["dense"][0][0][0] + ws["dense"][1][0]        return(x,h)Naturally, you can obtain the predicted sin waves $(x_1,x_2,â€¦,x_t)$ by looping around RNNmodel as:$x^âˆ—{t+1},h{t+1} = RNNmodel (x_t,h_t)$Here $x^âˆ—_t$ indicates the estimated value of $x$ at time point $t$. As our model is not so complicated, we can readily implement this algorithm as:upto = 50 ## predict the first  sin valuesxstars, hs_hand = [], []for i, x in enumerate(df_test.values):    if i == 0:        h = 0 ## initial hidden layer value is zero        xstar = x        print("initial value of sin x_0 = {}, h_0 = {}".format(x,h))    hs_hand.append(h)    xstars.append(xstar[0])    xstar, h = RNNmodel(ws,x, h)assert len(df_test.values) == len(xstars)initial value of sin x_0 = [-1.27375647e-13], h_0 = 0In this formulation, x_stars[t] contains the prediction of sin wave at time point t just as df_testplt.figure(figsize=(18,3))plt.plot(df_test.values[:upto],label="true",alpha=0.3,linewidth=5)plt.plot(xstars[:upto],label="sin prediction (xstar)")plt.plot(hs_hand[:upto],label="hidden state (xstar)")plt.legend()&lt;matplotlib.legend.Legend at 0x7f490d2fe908&gt;You can see that the model prediction is not good in the first few time points and then stabilized. OK. My model seems to over estimates the values when sin wave is going down and underestimates when the sin wave is going up. However, there is one question: this model returns almost zero validation loss. The error seems a bit high. In fact the error from the prediction above is quite large. What is going on?"validation loss {:3.2f}".format(np.mean((np.array(xstars) - df_test["sin_t"].values)**2))'validation loss 0.08'Letâ€™s predict the sin wave using the existing predict function from Keras. Remind you that we prepare X_test when X_train was defined. X_test contains data as:x1,x2x2,x3x3,x4â€¦y_test_from_keras = model.predict(X_test).flatten()Notice that this predicted values are exactly the same as the ones calculated before.np.all(predicted_sin_wave == y_test_from_keras)TrueAs the prediction starts from x_3, add the 2 NaN into a predicted vector as placeholders. This is just to make sure that the length of y_test_from_keras is compatible with xtars.y_test_from_keras = [np.NaN, np.NaN] + list(y_test_from_keras.flatten())h_test_from_keras = [np.NaN, np.NaN] + list(hidden_units.flatten())The plot shows that Kerasâ€™s predicted values are almost perfect and the validation loss is nearly zero. Clearly xstars are different from the Kerasâ€™s prediction. It seems that the predicted states from Keras and from by hand are also slightly different. Then question is, how does Keras predict the output?plt.figure(figsize=(18,3))plt.plot(df_test.values[:upto],label="true",alpha=0.3,linewidth=5)plt.plot(xstars[:upto],label="sin prediction (xstar)")plt.plot(hs_hand[:upto],label="hidden state (xstar)")plt.plot(y_test_from_keras[:upto],label="sin prediction (keras)")plt.plot(h_test_from_keras[:upto],label="hidden state (keras)")plt.legend()print("validation loss {:6.5f}".format(np.nanmean((np.array(y_test_from_keras) - df_test["sin_t"].values)**2)))validation loss 0.00021Here, the technical details of the BPTT algorithm comes in, and the time series length parameter (i.e., batch_size[1]) takes very important role.As the BPTT algorithm only passed back 2 steps, the model assumes that:the hidden units are initialized to zero every 2 steps.the prediction of the next sin value (xt+1) is based on the hidden unit (ht) which is created by updating the hidden units twice in the past assuming that htâˆ’1=0.xâˆ—t,ht=RNNmodel(xtâˆ’1,0)xt+1,âˆ’=RNNmodel(xt,ht)Note that the intermediate predicted sin xâˆ—t based on htâˆ’1=0 should not be used as the predicted sin value. This is because the xâˆ—t was not directly used to evaluate the loss function.Finally, obtain the Kerasâ€™s predicted sin wave at the next time point given the current sin wave by hand.def myRNNpredict(ws,X):    X = X.flatten()    h = 0    for i in range(len(X)):        x,h = RNNmodel(ws,X[i],h)    return(x,h)xs, hs = [], []for i in range(X_test.shape[0]):    x, h = myRNNpredict(ws,X_test[i,:,:])    xs.append(x)    hs.append(h)print("All sin estimates agree with ones from Keras = {}".format(    np.all(np.abs( np.array(xs) - np.array(y_test_from_keras[2:]) ) &lt; 1E-5)))print("All hidden state estmiates agree with ones fome Keras = {}".format(    np.all(np.abs( np.array(hs) - np.array(h_test_from_keras[2:]) ) &lt; 1E-5)) )All sin estimates agree with ones from Keras = TrueAll hidden state estmiates agree with ones fome Keras = TrueNow we understand how Keras is predicting the sin wave.In fact, Keras has a way to return xstar as predicted values, using â€œstatefulâ€ flag. This stateful is a notorious parameter and many people seem to be very confused. But by now you can understand what this stateful flag is doing, at least during the prediction phase. When stateful = True, you can decide when to reset the states to 0 by yourself.In order to predict in â€œstatefulâ€ mode, we need to re-define the model with stateful = True. When stateful is True, we need to specify the exact integer for batch_size. As we only have a single sin time series, we will set the batch_size to 1.model_stateful,_ = define_model(length_of_sequences = 1,                               batch_size=1,                               stateful = True)model_stateful.summary()Model: "model_3"_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_2 (InputLayer)         (1, 1, 1)                 0         _________________________________________________________________RNN (SimpleRNN)              (1, 1)                    3         _________________________________________________________________dense (Dense)                (1, 1)                    2         =================================================================Total params: 5Trainable params: 5Non-trainable params: 0_________________________________________________________________Assign the trained weights into the stateful model.for layer in model.layers:            for layer_predict in model_stateful.layers:        if (layer_predict.name == layer.name):            layer_predict.set_weights(layer.get_weights())            breakNow we predict in stateful mode. Here it is very important to reset_state() before the prediction so that h0=0.pred = df_test.values[0][0]stateful_sin = []model_stateful.reset_states()for i in range(df_test.shape[0]):    stateful_sin.append(pred)    pred = model_stateful.predict(df_test.values[i].reshape(1,1,1))[0][0]    stateful_sin = np.array(stateful_sin)print("All predicted sin values with stateful model agree to xstars = {}".format(    np.all(np.abs(np.array(stateful_sin) - np.array(xstars))&lt; 1E-5)))All predicted sin values with stateful model agree to xstars = TrueNow we understand that xstars is the prediction result when stateful = True. We also understand that the prediction results are way better when stateful = False at least for this sin wave example.However, the prediction with stateful = False brings to some awkwardness: what if our batch have a very long time series of length, say K? Do we always have to go back all the K time steps, set htâˆ’K=0 and then feed forward K steps in order to predict at the time point t? This may be computationally intense.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Poincare Conjecture/Theorem and Ricci Flow]]></title>
      <url>/foo/2018/12/22/test/</url>
      <content type="text"><![CDATA[Introductionä¹‹å‰å­¸ group theory å’Œ tensor calculus, ç¸½çµåˆ°å¹³ç›´ç©ºé–“çš„é‡å­å ´è«–ã€‚æœ€ç°¡å–®çš„æ˜¯ QED çš„ Lagrangian å¦‚ä¸‹ç‚ºç´”é‡ï¼Œå…·æœ‰ U(1) å°ç¨±æ€§ï¼Œå°æ‡‰å„ç¨®å®ˆæ†å¾‹ã€‚ä»¥åŠä¸åŒè·¯å¾‘å°æ™‚é–“ç©åˆ†æ»¿è¶³æœ€å°ä½œç”¨åŽŸç†ã€‚å¯ä»¥ç”± QED Lagrangian æŽ¨å°Žéžé‡å­å ´è«–è¿‘ä¼¼è§£ Maxwell equations. Maxwell equations å¯ä»¥è§£é‡‹æ‰€æœ‰çš„é›»ç£ç¾è±¡ï¼Œä½†ç„¡æ³•è§£é‡‹å…‰é‡å­æ•ˆæ‡‰ä¾‹å¦‚å…‰é›»æ•ˆæ‡‰ï¼Œé»‘é«”è¼»å°„ï¼Œé›·å°„ç­‰ç­‰ã€‚å°±åƒå¯ä»¥å¾žæ„›å› æ–¯å¦å ´æ–¹ç¨‹å¼æŽ¨å°Žè¿‘ä¼¼è§£ç‰›é “è¬æœ‰å¼•åŠ›å®šå¾‹ã€‚æŠŠ tensor calculus å¾ž Euclidean (differential) geometry æŽ¨å»£åˆ° Riemannian (differential) geometry, å¯ä»¥é€£çµåˆ°å»£ç¾©ç›¸å°è«–ã€‚ä»¥ä¸‹æ˜¯æ„›å› æ–¯å¦å ´æ–¹ç¨‹å¼ï¼šå³æ‰‹ $T_{\mu\nu}$ æ˜¯ energy-momentum tensor, äºŒéšŽå¼µé‡ï¼Œä»£è¡¨ mass-energy distribution. å·¦æ‰‹ $G_{\mu\nu}$ æ˜¯ Einstein tensor, ä¹Ÿæ˜¯äºŒéšŽå¼µé‡ï¼Œä»£è¡¨ space-time curvature, åŸºæœ¬æ˜¯ $R_{\mu\nu}$ (Ricci curvature tensor) æ¸›åŽ»ä¸€å€‹ä¿®æ­£é …ã€‚å¤šå‡ºçš„ä¿®æ­£é … $ 1/2 R g_{\mu\nu}$ é …ï¼š$R$ æ˜¯ scalar curvature (trace of Ricci curvature tensor), $g_{\mu\nu}$ is metric tensor.  ç•¶åˆæ„›å› æ–¯å¦å¯«ä¸‹çš„å ´æ–¹ç¨‹å¼ä¸¦æ²’æœ‰é€™ä¸€é …: (1)é•å local conservation of energy-momentum. ä¹Ÿå°±æ˜¯ energy flow is not preserved [@wikiHistoryGeneral2019];ï¼ˆ2ï¼‰ç„¡æ³•å¾—åˆ°åº§æ¨™ç³»ç„¡é—œå½¢å¼ï¼Œé•å(é¦¬èµ«)å»£ç¾©ç›¸å°æ€§åŽŸç†ã€‚æ„›å› æ–¯å¦æ±‚åŠ©æ–¼ Hilbert.  åœ¨ Hilbert çš„å”åŠ©ä¸‹ï¼Œæ‰¾åˆ°é€™å€‹ä¿®æ­£é …ã€‚å¦‚æžœ $T_{\mu\nu}$ éš¨æ™‚é–“è®ŠåŒ–ï¼Œä¾‹å¦‚å…©å€‹é»‘æ´žæ—‹è½‰åˆä½µï¼Œæœƒæ”¹è®Šæ™‚ç©ºæ›²çŽ‡ã€‚æ™‚ç©ºæ›²çŽ‡åˆæœƒåéŽä¾†å½±éŸ¿è³ªèƒ½åˆ†ä½ˆ and vice versa, å› è€Œç”¢ç”Ÿæ™‚ç©ºæ¼£æ¼ªï¼Œä¸€èˆ¬ç¨±ç‚ºå¼•åŠ›æ³¢ã€‚å¦‚åŒ Maxwell equation çš„é›»å ´è®ŠåŒ–ç”¢ç”Ÿç£å ´ and vice versa, å› è€Œç”¢ç”Ÿé›»ç£æ³¢ã€‚Tensor Calculus å’Œ Differential Geometry èƒ½å¤ ç”¨æ–¼ Quantum Field Theory and General Relativity å…©å¤§ç‰©ç†å­¸ï¼Œå·²ç¶“æ˜¯éžå¸¸å¹¸ç¦ã€‚ æ›´å¹¸ç¦çš„æ˜¯å¯ä»¥ç”¨æ–¼ Topology çš„ Poincare conjecture (now theorem proved by Perelman).  é€™éƒ¨åˆ†æˆ‘å€‘ follow Hamiltonâ€™s direction using Ricci flow. [@hamiltonRichardHamilton]Laplacian Operator and Heat Equationé€™éƒ¨åˆ†å¯ä»¥åƒè€ƒå‰æ–‡ã€ã€‘ã€‚æˆ‘å€‘å¾žåº§æ¨™ç„¡é—œçš„å¼µé‡å®šç¾©æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼š$\Delta = \nabla\cdot\nabla$, æˆ–æ˜¯ diverge of gradient of a scalar or vector field. ä»¥ä¸Šçš„å®šç¾©ä¸åªç”¨æ–¼æ­æ°å¹¾ä½•ï¼Œä¹Ÿé©ç”¨é»Žæ›¼å¹¾ä½•ã€‚ç†±å‚³å°Ž (heat diffusion) ä¸Šå¼æ˜¯ manifold å›ºå®šï¼Œåªæ˜¯å®šç¾©åœ¨ manifold ä¸Šçš„ç´”é‡å ´ (e.g. å‹¢èƒ½å ´ï¼Œæº«åº¦å ´) éš¨æ™‚é–“å’Œç©ºé–“è®ŠåŒ–ï¼Œä½†æ˜¯æ•´é«” volume ä¸è®Šï¼ˆå®ˆæ†é‡ï¼‰ï¼Œå°æ‡‰ä¸€å€‹ flowã€‚Ricci Flow = æ„›å› æ–¯å¦å ´æ–¹ç¨‹å¼ + æ‹‰æ™®æ‹‰æ–¯ç†±å‚³å°ŽHamilton å‰‡æ˜¯è€ƒæ…® manifold æœ¬èº«éš¨æ™‚é–“è®ŠåŒ–ã€‚1981 å¼•å…¥ Ricci flow. è§€å¿µä¸Šéžå¸¸é¡žä¼¼ä¸Šè¿°çš„ç†±å‚³å°Žã€‚ä½†ç›´æŽ¥ç”¨æ–¼ manifold (intrinsic) è€Œéžå…¶ä¸Šçš„ (extrinsic) field.  éžå¸¸é–‹å‰µæ€§è€Œä¸”å…·ç‰©ç†æ€§ç›´è§€æ€§ï¼çœ‹äº† Hamilton 2006 Youtube çš„æ¼”è¬› [@hamiltonRichardHamilton2006], ä»–ä¹Ÿè¨±ä¸æ˜¯ç¬¬ä¸€å€‹æŠŠ PDE (Partial Differential Equation) ç”¨æ–¼ topology. ä½†æ˜¯ç¬¬ä¸€å€‹å¼•å…¥ Ricci flow, çµåˆåˆ†æžå’Œæ‹“å¢£ï¼Œå°æ–¼ topology éžå¸¸å…·é«”å¯¦ç”¨ (N-manifold, not only 2 or 3).  æ‹“å¢£å¯ä»¥å¤§é‡å€Ÿç”¨ PDE çš„ç†è«–ï¼Œç”šè‡³å¯ä»¥ç”¨è¨ˆç®—æ©Ÿå”åŠ©ã€‚å°±åƒç¬›å¡çˆ¾å¼•å…¥ç›´è§’åº§æ¨™ç³»çµåˆä»£æ•¸å’Œå¹¾ä½•ã€‚Hamilton é«˜åº¦è©•åƒ¹ Perelman åœ¨ Ricci flow çš„è²¢ç»ï¼Œä¸åƒæŸä¸€äº›æ–‡ç« æš—ç¤º Hamilton å° Perelman æœ‰å¿ƒçµã€‚Perelman åœ¨æ‹’çµ• Fields medal ä¹Ÿé«˜åº¦è©•åƒ¹ Hamilton åœ¨ Ricci flow çš„å‰µè¦‹ã€‚å…©äººåœ¨å°ˆæ¥­é ˜åŸŸæ‡‰è©²æ˜¯äº’ç›¸ä½©æœã€‚Hamilton æå‡ºçš„ Ricci Flow å¦‚ä¸‹ã€‚æžœç„¶æ˜¯æ•¸å­¸å®¶çš„å…¬å¼ï¼Œéžå¸¸ç°¡æ½”ã€‚å…¶å¯¦å°±æ˜¯å¼µé‡ç‰ˆçš„ç†±å‚³å°Žæ–¹ç¨‹å¼ï¼$R_{ij}$ ä»£è¡¨ manifold çš„ intrinsic curvature, åŸºæœ¬æ˜¯ Christoffel symbol çš„ç©ºé–“ä¸€éšŽå°Žæ•¸ [@ListFormulas2019]ã€‚and Christoffel symbol æ˜¯ metric tensor çš„ç©ºé–“ä¸€éšŽå°Žæ•¸å› æ­¤ $R_{ij}$ åŸºæœ¬æ˜¯ metric tensor $g_{ij}$ çš„ç©ºé–“äºŒéšŽå°Žæ•¸ã€‚é€™å’Œæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„åŠŸèƒ½ä¸€è‡´ã€‚ç­‰å¼çš„å³æ‰‹å‰‡æ˜¯ metric tensor å°æ™‚é–“ä¸€éšŽå°Žæ•¸ã€‚å› æ­¤ Ricci flow equation é¡žä¼¼æ‹‰æ™®æ‹‰æ–¯ç†±å‚³å°Žå…¬å¼ã€‚éš¨æ™‚é–“æ”¹è®Š manifold çš„ metric tensor, Christoffel tesnor, curvature tensor.ç†Ÿæ‚‰æ„›å› æ–¯å¦å ´æ–¹ç¨‹å¼è€…æœƒæƒ³åˆ°ä¿®æ­£é …ã€‚Yes! é€™ç¨±ç‚º normalized Ricci flow.Normalized Ricci flow çš„å®šç¾©å¦‚ä¸‹ [@wikiRicciFlow2019]ï¼šwhere $R_{avg}$ is the average (mean) of the scalar curvature (which is the trace of Ricci tensor), n is the dimension of the manifold.The normalized equation preserves the volume of the metric space.  é€™ä¸€å¥è©±å°±æ˜¯åŠ ä¸Šä¸­é–“é€™ä¸€é …æ‰èƒ½ä¿æŒ volume ä¸è®Šã€‚é€™æ˜¯ â€œ(incompressible) flowâ€ çš„åŸºæœ¬æ¢ä»¶ã€‚é€™ä¿®æ­£é …å’Œæ„›å› æ–¯å¦å»£ç¾©å ´æ–¹ç¨‹å¼åŸºæœ¬ä¸€è‡´ (n=4)ï¼Œæ»¿è¶³å ´æ–¹ç¨‹å¼åº§æ¨™ç³»ç„¡é—œï¼Œä¹Ÿå°±æ˜¯å»£ç¾©ç›¸å°æ€§åŽŸç†ã€‚åŸºæœ¬åŽŸå‰‡æ˜¯ metric tensor, Christoffel tensor, curvature tensor exponentially decay.  Ricci flow çš„è² è™Ÿæœƒè®“ä¸ç©©å®šçš„è² æ›²çŽ‡ (3-manifold é›™æ›²é¢) åªæœƒçŸ­æš«å‡ºç¾ã€‚  å¤§çš„æ­£æ›²çŽ‡ï¼ˆéžå¸¸å½Ž 3-manifold æ©¢åœ“æ›²é¢ï¼‰ä¹Ÿæœƒå¾ˆå¿« decayã€‚  æœ€å¾Œç”±å°çš„æ­£æ›²çŽ‡ï¼ˆå¹³ç·© 3-manifold æ©¢åœ“æ›²é¢ï¼‰dominate manifold çš„è®ŠåŒ–ã€‚  Ricci flow è®ŠåŒ– manifold éŽç¨‹ä¸­ï¼Œæ‹“å¢£ç‰¹æ€§ä¸è®Š (invariant)ï¼Œå°±æ˜¯åŒèƒšï¼å¯ä»¥ç”¨æ–¼è­‰æ˜Ž Poincare theorem.  Volume (area for 2-manifold) is preserved? Yes for normalized Ricci flow; No for Ricci flow.  A good way to think of the normalized Ricci flow is that itâ€™s the same as Ricci flow but you rescale every time-slice to make the volume constant. Maybe also reparametrize time to make the equation nicer if you feel like it. Of course, isometries are still isometries after a metric gets rescaled.  ä¸‹åœ–æ˜¯ä¸€å€‹ 2D surface/manifold çš„ Ricci flow è®ŠåŒ– surface/manifold çš„éŽç¨‹ã€‚å› ç‚ºæ˜¯ Ricci flow, surface area is not preserved.Poincare Conjecture/Theoremå›žåˆ° Poincare conjecture [@PoincareConjecture2019]. å…ˆå¾žæœ€åŸºæœ¬çš„ 2D surface é–‹å§‹ï¼Œæ¯”è¼ƒç›´è§€ã€‚A compact 2-dimensional surface (2D manifold) without boundary is topologically homeomorphic to a 2-sphere if every loop can be continuously tightened to a point.æ›´ç°¡æ½”çš„èªªæ³•Every simply connected, closed (i.e. no boundary and compact) 2-manifold is homeomorphic to the 2-sphere.åŸºæœ¬ä¸Šå¦‚æžœä¸€å€‹ 2D surface ä»»ä½•ä¸€å€‹ loop å¯ä»¥é€£çºŒæ”¶æ–‚åˆ°ä¸€å€‹é»žï¼Œ2D surface å¿…å®šå’Œçƒé¢åŒèƒšï¼Œå¦‚ä¸Šåœ–ã€‚å†çœ‹ 2D torus (ç’°é¢) å¦‚ä¸‹åœ–ã€‚æ²’æœ‰ boundary, å­˜åœ¨å…©ç¨® loops (red and pink) éƒ½ç„¡æ³•æ”¶æ–‚åˆ°ä¸€å€‹é»žã€‚å› æ­¤ 2D torus å’Œçƒé¢ä¸åŒèƒšã€‚ä»»ä½•ä¸€å€‹ loop å¯ä»¥é€£çºŒæ”¶æ–‚åˆ°ä¸€å€‹é»ž = æ²’æœ‰ç ´æ´ž = å–®é€£é€šç¿»è­¯æˆä¸­æ–‡ï¼šä»»ä¸€å–®é€£é€šçš„ã€å°é–‰çš„äºŒç¶­æµå½¢èˆ‡äºŒç¶­çƒé¢åŒèƒšã€‚The PoincarÃ© conjecture asserts that the same is true for 3-dimensional as follows!Every simply connected, closed (i.e. no boundary and compact) 3-manifold is homeomorphic to the 3-sphere.ç¿»è­¯æˆä¸­æ–‡ï¼šä»»ä¸€å–®é€£é€šã€å°é–‰çš„ä¸‰ç¶­æµå½¢èˆ‡ä¸‰ç¶­çƒé¢åŒèƒšã€‚###å¦‚ä½•æƒ³åƒå–®é€£é€šã€å°é–‰çš„ä¸‰ç¶­æµå½¢ï¼Ÿå°æ–¼è™•æ–¼ä¸‰ç¶­æ­æ°ç©ºé–“çš„æˆ‘å€‘ï¼Œå¯ä»¥çœ‹åˆ°å°é–‰çš„äºŒç¶­æµå½¢ï¼ˆå¦‚å„ç¨®çƒé¢ï¼Œç’°é¢ï¼ŒKlein bottle, etc.ï¼‰æˆ‘å€‘å¯ä»¥æƒ³åƒæœ‰é‚Šç•Œçš„ä¸‰ç¶­æµå½¢ï¼Œä½†æ˜¯å¾ˆé›£æƒ³åƒå°é–‰çš„ä¸‰ç¶­æµå½¢ã€‚é€™éœ€è¦å››ç¶­ç©ºé–“çš„è¦–è§’æ‰èƒ½æƒ³åƒã€‚ä½†å°æ–¼ç°¡å–®å°é–‰ä¸‰ç¶­æµå½¢ï¼Œæˆ‘å€‘å¯ä»¥å±•é–‹é™ç¶­åˆ°ä¸‰ç¶­æ­æ°ç©ºé–“ã€‚ä»¥ä¸‹ç”¨ 2D éª°å­é¢ï¼ˆå’Œ 2D çƒé¢åŒèƒšï¼‰ä¾†é¡žæ¯”ã€‚åƒè€ƒæ•¸å­¸å¥³å­©é¾åŠ èŠçŒœæƒ³ã€‚2D éª°å­é¢æ˜¯å–®é€£é€šã€å°é–‰çš„äºŒç¶­æ›²é¢ï¼Œå’ŒäºŒç¶­çƒé¢åŒèƒšã€‚ç‚ºä»€éº¼ç”¨ 2D éª°å­é¢ï¼Ÿå› ç‚º 3D cube (embed 2D éª°å­é¢)å¯ä»¥å±•é–‹æˆ 6 å€‹ 2D æ­£æ–¹å½¢åœ¨ 2D æ­æ°å¹³é¢ã€‚æ¯ä¸€å€‹æ­£æ–¹å½¢çš„ 4 é‚Šï¼Œéƒ½å’Œ 4 å€‹æ­£æ–¹å½¢ç›¸é„°ã€‚å› æ­¤ä¸€å€‹ 2D æ›²é¢çš„ç”Ÿç‰© (æ¯›æ¯›èŸ²)ï¼Œåªè¦éµå¾ªç›¸é„°çš„è¦å‰‡ï¼Œå¯ä»¥ä¸€ç›´ç§»å‹•ä¸æœƒé›¢é–‹ 2D éª°å­é¢ã€‚ä¹Ÿå°±æ˜¯å…·æœ‰å°é–‰æ€§ã€‚æŠŠ 2D éª°å­é¢æŽ¨å»£åˆ° 3D éª°å­é«”ï¼ˆå’Œ 3D è¶…çƒé¢åŒèƒšï¼‰ã€‚åŽŸå‰‡ä¸Šè¦åœ¨ 4D æ­æ°ç©ºé–“æ‰èƒ½æƒ³åƒã€‚å¯ä»¥ç”¨ä¸‹åœ–å·¦è¿‘ä¼¼ 4D hypercubeã€‚å¯ä»¥å±•é–‹æˆ 8 å€‹ 3D ç«‹æ–¹é«” (cube), æ¯ä¸€å€‹ 3D cube çš„ 6 é¢ï¼Œéƒ½å’Œ 6 å€‹ï¼ˆä¸Šä¸‹å·¦å³å‰å¾Œï¼‰3D cube ç›¸é„°ã€‚å› æ­¤ä¸€å€‹ 3D ç”Ÿç‰©ï¼ˆäººï¼‰ï¼Œåªè¦éµå¾ªç›¸é„°çš„è¦å‰‡ï¼Œå¯ä»¥ä¸€ç›´ç§»å‹•ä¸æœƒé›¢é–‹ 3D éª°å­é«”ã€‚ä¹Ÿå°±æ˜¯å…·æœ‰å°é–‰æ€§ã€‚Why Poincare Conjecture is Importantï¼Ÿé¦–å…ˆè½èµ·ä¾†å¾ˆåŸºæœ¬ä¸”é‡è¦ã€‚çš„ç¢ºé€™æ˜¯æ‹“å¢£å­¸ä¸€å€‹åŸºæœ¬å•é¡Œã€‚äº‹å¯¦ä¸Šï¼Œåœ¨ 2 ç¶­å’Œå¤§æ–¼ç­‰æ–¼ 4 ç¶­æµå½¢ï¼Œæœ¬å‘½é¡Œéƒ½å·²è­‰æ˜Žç¶­çœŸã€‚åªæœ‰åœ¨ 3 ç¶­æµå½¢ï¼Œä¹Ÿå°±æ˜¯ Poincare conjecture, ä¸€ç›´åˆ° Perelman åœ¨ 2006 æ‰è­‰æ˜Ž Poincare conjecture.æ›´é‡è¦çš„æ˜¯ 1982 Thurston æå‡º geometrization conjecture (now theorem) çŒœæ¸¬æ‰€æœ‰å°é–‰çš„ä¸‰ç¶­æµå½¢ (3-manifold) å¯ä»¥åˆ†è§£ç‚º 8 ç¨®åŸºæœ¬å¹¾ä½•çµæ§‹ï¼Œ3-sphere æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚[@wikiGeometrizationConjecture2019]é¡žä¼¼æœ‰ uniformization theorem é©ç”¨æ–¼äºŒç¶­æµå½¢ (2-manifold): æ‰€æœ‰å–®é€£é€šçš„äºŒç¶­æµå½¢ï¼ˆçƒé¢ï¼‰ä¸€å®šæ˜¯ 3 ç¨®æ›²é¢ä¹‹ä¸€ï¼ˆEuclidean, spherical, or hyperbolic).Strategy to Prove Poincare ConjectureHamilton 1981 æå‡º Ricci flow çš„æ€è·¯ï¼š  å°æ–¼å–®é€£é€šã€å°é–‰ 3-manifold ä½œç‚ºåˆå§‹æ¢ä»¶, $g_{ij}(0)$, æ–½åŠ  Ricci flow deforms 3-manifold.  Ricci flow è®ŠåŒ– manifold éŽç¨‹ä¸­ï¼Œmanifold æ‹“å¢£ç‰¹æ€§ä¸è®Š (invariant)ï¼Œå°±æ˜¯åŒèƒšï¼  Ricci flow çš„è² è™Ÿæœƒè®“ä¸ç©©å®šçš„è² æ›²çŽ‡åªæœƒçŸ­æš«å‡ºç¾ã€‚å¤§çš„æ­£æ›²çŽ‡ä¹Ÿæœƒå¾ˆå¿« decay.  æœ€å¾Œç”±å°çš„æ­£æ›²çŽ‡ dominate manifold çš„è®ŠåŒ–ã€‚æœ€å¾Œè¶¨è¿‘ 3-sphere.  å› æ­¤è­‰æ˜Žå–®é€£é€šã€å°é–‰ 3-manifold å’Œ 3-sphere åŒèƒšï¼Œä¹Ÿå°±æ˜¯ Poincare conjecture.Hamilton åœ¨ Ricci flow çš„è²¢ç»ï¼š[@hamiltonRichardHamilton2006]  æ­£æ›²çŽ‡çš„ 2/3-manifold åœ¨ finite time æ”¶æ–‚åˆ°ä¸€é»ž (singularity with curvature $\to\infty$)ã€‚ä½† normalize (area/volume) ä¹‹å¾Œæ”¶æ–‚åˆ° 2/3-sphereï¼Œå°±æ˜¯ 2/3-sphere åŒèƒšã€‚ç­‰æ•ˆæ–¼ä½¿ç”¨ normalized Ricci flow to preserve volume (?).  2-manifold å•žéˆ´ (1 â€œneckâ€ with positive and negative curvature) æˆ–æ˜¯å¤šå€‹ â€œneckâ€ å¦‚åœ–ä¸€åœ¨ finite time æ”¶æ–‚åˆ°ä¸€é»žã€‚  å› æ­¤ 2-manifold å¯ä»¥å¾ˆå®¹æ˜“ç”¨ Ricci flow è­‰æ˜Žå’Œ 2-sphere åŒèƒšã€‚é€™æ˜¯ç°¡å–®çš„ç‰›åˆ€å°è©¦ã€‚  3-manifold with neck å°±è·Ÿè¤‡é›œï¼Œæœƒç”¢ç”Ÿ â€œneck pinchâ€ singularity.  Hamilton æå‡º Ricci flow with surgery to cut off large curvature portion and solve the singularity to converge to 3-sphere.  Hamilton çš„çˆ¶è¦ªæ˜¯çœŸçš„å¤–ç§‘é†«ç”Ÿã€‚  ä½†å­˜åœ¨ cigar (2-manifold) or other 3-manifold soliton éŽç¨‹æ°¸é ä¿æŒå½¢ç‹€ä¸è®Šï¼Œç„¡æ³•æ”¶æ–‚åˆ° 3-sphere.Perelman è§£æ±º Hamilton Ricci-flow çš„æ¼æ´žã€‚  Improve the surgery to completely solve singularity.  From transport equation to make soliton ç„¡æ³•ç”¢ç”Ÿã€‚  Prove geometrization conjecture, Poincare conjecture åŸºæœ¬æ˜¯ä¸€å€‹å­å®šç†ã€‚ReferenceHamilton, Richard, dir. 2006. Richard Hamilton | the PoincareConjecture | 2006. https://www.youtube.com/watch?v=fymCXcIt20g.Wiki. 2019a. â€œRicci Flow.â€ Wikipedia.https://en.wikipedia.org/w/index.php?title=Ricci_flow&amp;oldid=920777616.â€”â€”â€”. 2019b. â€œHistory of General Relativity.â€ Wikipedia.https://en.wikipedia.org/w/index.php?title=History_of_general_relativity&amp;oldid=931327622.â€”â€”â€”. 2019c. â€œGeometrization Conjecture.â€ Wikipedia.https://en.wikipedia.org/w/index.php?title=Geometrization_conjecture&amp;oldid=932572904.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Next Theme Tutorial]]></title>
      <url>/tutorial/2017/07/20/next-tutorial/</url>
      <content type="text"><![CDATA[  NexT is a high quality elegant Jekyll theme ported from Hexo Next. It is crafted from scratch, with love.Live PreviewScreenshots      Desktop        Sidebar    Sidebar (Post details page)  MobileInstallationCheck whether you have Ruby 2.1.0 or higher installed:ruby --versionInstall Bundler:gem install bundlerClone Jacman theme:git clone https://github.com/Simpleyyt/jekyll-theme-next.gitcd jekyll-theme-nextInstall Jekyll and other dependencies from the GitHub Pages gem:bundle installRun your Jekyll site locally:bundle exec jekyll serverMore Detailsï¼šSetting up your GitHub Pages site locally with JekyllFeaturesMultiple languages support, including: English / Russian / French / German / Simplified Chinese / Traditional Chinese.Default language is English.language: en# language: zh-Hans# language: fr-FR# language: zh-hk# language: zh-tw# language: ru# language: deSet language field as following in site _config.yml to change to Chinese.language: zh-HansComment support.NexT has native support for DuoShuo and Disqus comment systems.Add the following snippets to your _config.yml:duoshuo:  enable: true  shortname: your-duoshuo-shortnameORdisqus_shortname: your-disqus-shortnameSocial MediaNexT can automatically add links to your Social Media accounts:social:  GitHub: your-github-url  Twitter: your-twitter-url  Weibo: your-weibo-url  DouBan: your-douban-url  ZhiHu: your-zhihu-urlFeed link.  Show a feed link.Set rss field in themeâ€™s _config.yml, as the following value:  rss: false will totally disable feed link.      rss:   use sitesâ€™ feed link. This is the default option.    Follow the installation instruction in the pluginâ€™s README. After the configuration is done for this plugin, the feed link is ready too.    rss: http://your-feed-url set specific feed link.Up to 5 code highlight themes built-in.NexT uses Tomorrow Theme with 5 themes for you to choose from.Next use normal by default. Have a preview about normal and night:Head over to Tomorrow Theme for more details.ConfigurationNexT comes with few configurations.# Menu configuration.menu:  home: /  archives: /archives# Faviconfavicon: /favicon.ico# Avatar (put the image into next/source/images/)# can be any image format supported by web browsers (JPEG,PNG,GIF,SVG,..)avatar: /default_avatar.png# Code highlight theme# available: normal | night | night eighties | night blue | night brighthighlight_theme: normal# Fancybox for image galleryfancybox: true# Specify the date when the site was setupsince: 2013Browser support]]></content>
      <categories>
        
          <category> tutorial </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Highlight Test]]></title>
      <url>/test/2017/07/19/highlight-test/</url>
      <content type="text"><![CDATA[This is a highlight test.Normal blockalert('Hello World!');print 'helloworld'Highlight blockalert( 'Hello, world!' );print 'helloworld'def foo  puts 'foo'enddef foo  puts 'foo'end123def foo  puts 'foo'end#include &lt;iostream&gt;using namespace std;void foo(int arg1, int arg2){}int main(){  string str;  foo(1, 2);  cout &lt;&lt; "Hello World" &lt;&lt; endl;  return 0;}]]></content>
      <categories>
        
          <category> Test </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Emoji Test]]></title>
      <url>/2015/09/19/emoji-test/</url>
      <content type="text"><![CDATA[This is an emoji test. :smile: lol.See emoji cheat sheet for more detail :wink: : https://www.webpagefx.com/tools/emoji-cheat-sheet/.:bowtie::smile::laughing::blush::smiley::relaxed::smirk::heart_eyes::kissing_heart::kissing_closed_eyes::flushed::relieved::satisfied::grin:]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Gallery Post]]></title>
      <url>/photo/2014/11/18/gallery-post/</url>
      <content type="text"><![CDATA[Nunc dignissim volutpat enim, non sollicitudin purus dignissim id. Nam sit amet urna eu velit lacinia eleifend. Proin auctor rhoncus ligula nec aliquet. Donec sodales molestie lacinia. Curabitur dictum faucibus urna at convallis. Aliquam in lectus at urna rutrum porta. In lacus arcu, molestie ut vestibulum ut, rhoncus sed eros. Sed et elit vitae risus pretium consectetur vel in mi. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi tempus turpis quis lectus rhoncus adipiscing. Proin pulvinar placerat suscipit. Maecenas imperdiet, quam vitae varius auctor, enim mauris vulputate sapien, nec laoreet neque diam non quam.Etiam luctus mauris at mi sollicitudin quis malesuada nibh porttitor. Vestibulum non dapibus magna. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Proin feugiat hendrerit viverra. Phasellus sit amet nunc mauris, eu ultricies tellus. Sed a mi tortor, eleifend varius erat. Proin consectetur molestie tortor eu gravida. Cras placerat orci id arcu tristique ut rutrum justo pulvinar. Maecenas lacinia fringilla diam non bibendum. Aenean vel viverra turpis. Integer ut leo nisi. Pellentesque vehicula quam ut sapien convallis consequat. Aliquam ut arcu purus, eget tempor purus. Integer eu tellus quis erat tristique gravida eu vel lorem.]]></content>
      <categories>
        
          <category> Photo </category>
        
      </categories>
      <tags>
        
          <tag> consectetur </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MathJax with Jekyll]]></title>
      <url>/opinion/2014/02/16/Mathjax-with-jekyll/</url>
      <content type="text"><![CDATA[One of the rewards of switching my website to Jekyll is theability to support MathJax, which means I can write LaTeX-like equations that getnicely displayed in a web browser, like this one \( \sqrt{\frac{n!}{k!(n-k)!}} \) orthis one \( x^2 + y^2 = r^2 \).Whatâ€™s MathJax?If you check MathJax website (www.mathjax.org) youâ€™ll seethat it is an open source JavaScript display engine for mathematics that works in allbrowsers.How to implement MathJax with JekyllI followed the instructions described by Dason Kurkiewicz forusing Jekyll and Mathjax.Here are some important details. I had to modify the Ruby library for Markdown inmy _config.yml file. Now Iâ€™m using redcarpet so the corresponding line in theconfiguration file is: markdown: redcarpetTo load the MathJax javascript, I added the following lines in my layout post.html(located in my folder _layouts)&lt;script type="text/javascript"    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;Of course you can choose a different file location in your jekyll layouts.Note that by default, the tex2jax preprocessor defines theLaTeX math delimiters, which are \\(...\\) for in-line math, and \\[...\\] fordisplayed equations. It also defines the TeX delimiters $$...$$ for displayedequations, but it does not define $...$ as in-line math delimiters. To enable in-line math delimiter with $...$, please use the following configuration:&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config({  tex2jax: {    inlineMath: [['$','$'], ['\\(','\\)']],    processEscapes: true  }});&lt;/script&gt;&lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;A Couple of ExamplesHereâ€™s a short list of examples. To know more about the details behind MathJax, you canalways checked the provided documentation available athttp://docs.mathjax.org/en/latest/Letâ€™s try a first example. Hereâ€™s a dummy equation:How do you write such expression? Very simple: using double dollar signs$$a^2 + b^2 = c^2$$To display inline math use \\( ... \\) like this \\( sin(x^2) \\) which getsrendered as \( sin(x^2) \)Hereâ€™s another example using type \mathsf$$ \mathsf{Data = PCs} \times \mathsf{Loadings} $$which gets displayed asOr even better:\\[ \mathbf{X} = \mathbf{Z} \mathbf{P^\mathsf{T}} \\]is displayed as\[ \mathbf{X} = \mathbf{Z} \mathbf{P^\mathsf{T}} \]If you want to use subscripts like this \( \mathbf{X}_{n,p} \) you need to scape theunderscores with a backslash like so \mathbf{X}\_{n,p}:$$ \mathbf{X}\_{n,p} = \mathbf{A}\_{n,k} \mathbf{B}\_{k,p} $$will be displayed as\[ \mathbf{X}_{n,p} = \mathbf{A}_{n,k} \mathbf{B}_{k,p} \]]]></content>
      <categories>
        
          <category> opinion </category>
        
      </categories>
      <tags>
        
          <tag> resources </tag>
        
          <tag> jekyll </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Images]]></title>
      <url>/2013/12/27/images/</url>
      <content type="text"><![CDATA[This is a image test post.]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Excerpts]]></title>
      <url>/2013/12/25/excerpts/</url>
      <content type="text"><![CDATA[The following contents should be invisible in home/archive page.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce eget urna vitae velit eleifend interdum at ac nisi. In nec ligula lacus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eu cursus erat, ut dapibus quam. Aliquam eleifend dolor vitae libero pharetra adipiscing. Etiam adipiscing dolor a quam tempor, eu convallis nulla varius. Aliquam sollicitudin risus a porta aliquam. Ut nec velit dolor. Proin eget leo lobortis, aliquam est sed, mollis mauris. Fusce vitae leo pretium massa accumsan condimentum. Fusce malesuada gravida lectus vel vulputate. Donec bibendum porta nibh ut aliquam.Sed lorem felis, congue non fringilla eu, aliquam eu eros. Curabitur orci libero, mollis sed semper vitae, adipiscing in lectus. Aenean non egestas odio. Donec sollicitudin nisi quis lorem gravida, in pharetra mauris fringilla. Duis sit amet faucibus dolor, id aliquam neque. In egestas, odio gravida tempor dictum, mauris felis faucibus purus, sit amet commodo lacus diam vitae est. Ut ut quam eget massa semper sodales. Aenean non ipsum cursus, blandit lectus in, ornare odio. Curabitur ultrices porttitor vulputate.]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Block]]></title>
      <url>/foo/2013/12/25/block/</url>
      <content type="text"><![CDATA[This post is used for testing tag plugins. See docs for more info.Block QuoteNormal blockquote  Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna.Code BlockInline code blockThis is a inline code block: python, print 'helloworld'.Normal code blockalert('Hello World!');print "Hello world"Highlight code blockprint "Hello world"def foo  puts 'foo'end123def foo  puts 'foo'endGist]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ]]></title>
      <url>/2013/12/25/%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%83%86%E3%82%B9%E3%83%88/</url>
      <content type="text"><![CDATA[This is a Japanese test post.ç§ã¯æ˜¨æ—¥ã¤ã„ã«ãã®åŠ©åŠ›å®¶ã¨ã„ã†ã®ã®ä¸Šã‚ˆã‚Šã™ã‚‹ãŸãªã‘ã‚Œã€‚æœ€ã‚‚ä»Šã‚’ãŠè©±å›£ã¯ã¡ã‚‡ã†ã©ã“ã®å‰å¾Œãªã‹ã‚ã§ãã‚‰ã„ã«å›°ã‚ŠãŒã„ã‚‹ãŸã‚’ã¯å¸°ç€è€ƒãˆãŸãªã‹ã£ã¦ã€ãã†ã«ã‚‚ã™ã‚‹ã§ã†ãŸã‚‰ãªã„ã€‚ãŒãŸã‚’çŸ¥ã£ãªã„ã¯ãšã‚‚åŒæ™‚ã«ä¹æœˆã‚’ã„ã‚ˆã„ã‚ˆãŸã‚ã‚ŠãŸã€‚ã‚‚ã£ã¨æ§™ã•ã‚“ã«ã¼ã‚“ã‚„ã‚Šé‡‘å°‘ã—èª¬æ˜Žã«ãˆãŸè‡ªåˆ†å¤§ã—ãŸäººç§ã‹å½±éŸ¿ã«ã¨ã„ã†ãŠé–¢ä¿‚ãŸã†ã¾ã›ãªã„ãŒã€ã“ã®æ¬¡ç¬¬ã‚‚ç§ã‹å…„å…·åˆã«ä½¿ã†ã¦ã€æ§™ã•ã‚“ã®ã®ã«å½“äººã®ã‚ãªãŸã«ã•ãžã”æ„å‘³ã¨è¡Œãã¦ç§å€‹äººãŒå°å°Šæ•¬ã‚’è´ã„ã‚ˆã†ã«åŒæ™‚ã«åŒåæŠ—ã«é›†ã£ã ã†ã¦ã€ã„ã‚ˆã„ã‚ˆã¾ãšç›¸å½“ã¸ã‚ã£ã†ã‹ã‚‰ã„ã äº‹ã‚’ã—ã§ãªã‘ã‚Œã€‚  ãã‚Œã§ãã‚Œã§ã‚‚ã”æ™‚æ—¥ã‚’ã—ã¯ãšã¯ãŸã£ãŸã„ã‚„ã¨çªãæŠœã‘ã‚‹ã¾ã™ã¦ã€ãã®å…ƒãŒã¯è¡Œã£ãŸã¦ã¨ã„ã†ç„ã‚’å°½ã™ã¦ã„ã‘ã§ã™ãŸã€‚ã“ã®ä¸­é“å…·ã®æ—¥ãã®å­¦æ ¡ã¯ã‚ãªãŸã”ã‚ãŒã™ã¾ãªã‚Šã‹ã¨ãƒãƒ«ã‚½ãƒ³ã•ã‚“ã®è€ƒãˆã‚‹ã§ã™ã‚“ã€è¾ºã®äº‹å®Ÿãªã„ã¨ã„ã†ã”ç›²å¾“ã‚ã‚ŠãŸã§ã™ã¨ã€çˆºã•ã‚“ã®ãŸã‚ãŒè–¬ç¼¶ãŒçµæžœã¾ã§ã®ç®¸ã®å½“æ™‚ã—ã¦ãªã‚‰ã¦ã€å¤šå°‘ã®åæœˆã«ãŸã‚ã‹ã‚‰ãã†ã„ã†ä¸Šã‹ã‚‰ã¨ã«ã‹ãã—ã¾ã—ãªã„ã¨è§¦ã‚Œã¹ãã‚‚ã®ãŸã§ã€ãªã„ã†ã§ã™ã¨å¤šå°‘ãŠäººé”ã—ãŸã®ã§ãŸãŸã€‚From ã™ãä½¿ãˆã‚‹ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ - æ—¥æœ¬èªž Lorem ipsum]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ä¸­æ–‡æ¸¬è©¦]]></title>
      <url>/test/test/2013/12/25/%E4%B8%AD%E6%96%87%E6%B8%AC%E8%A9%A6/</url>
      <content type="text"><![CDATA[This is a Chinese test post.å–„æˆ‘çŽ‹ä¸Šé­šã€ç”¢ç”Ÿè³‡è¥¿å“¡åˆå…’è‡‰è¶£è«–ã€‚ç•«è¡£ç”Ÿé€™è‘—çˆ¸æ¯›è¦ªå¯æ™‚ï¼Œå®‰ç¨‹å¹¾ï¼Ÿåˆå­¸ä½œã€‚è§€ç¶“è€Œä½œå»ºã€‚éƒ½éžå­ä½œé€™ï¼æ³•å¦‚è¨€å­ä½ é—œï¼æ‰‹å¸«ä¹Ÿã€‚ä»¥ä¹Ÿåº§è«–é ­å®¤æ¥­æ”¾ã€‚è¦è»Šæ™‚åœ°è®Šæ­¤è¦ªä¸è€é«˜å°æ˜¯çµ±ç¿’ç›´éº¼èª¿æœªï¼Œè¡Œå¹´é¦™ä¸€ï¼Ÿå°±ç«Ÿåœ¨ï¼Œæ˜¯æˆ‘ç«¥ç¤ºè®“åˆ©åˆ†å’Œç•°ç¨®ç™¾è·¯é—œæ¯ä¿¡éŽæ˜Žé©—æœ‰å€‹æ­·æ´‹ä¸­å‰åˆè‘—å€äº®é¢¨å€¼æ–°åº•è»Šæœ‰æ­£çµï¼Œé€²å¿«ä¿çš„è¡Œæˆ°å¾žï¼šå¼Ÿé™¤æ–‡è¾¦æ¢åœ‹å‚™ç•¶ä¾†éš›å¹´æ¯å°è…³è­˜ä¸–å¯çš„çš„å¤–çš„å»£ä¸‹æ­Œæ´²ä¿è¼ªå¸‚æžœåº•å¤©å½±ï¼›å…¨æ°£å…·äº›å›žç«¥ä½†å€’å½±ç™¼ç‹€åœ¨ç¤ºï¼Œæ•¸ä¸Šå­¸å¤§æ³•å¾ˆï¼Œå¦‚è¦æˆ‘â€¦â€¦æœˆå“å¤§ä¾›é€™èµ·æœæ»¿è€ï¼Ÿæ‡‰å­¸å‚³è€…åœ‹ï¼šå±±å¼æŽ’åªä¸ä¹‹ç„¶æ¸…åŒé—œï¼›ç´°è»Šæ˜¯ï¼åœå±‹å¸¸é–“åˆï¼Œè³‡ç•«é ˜ç”Ÿï¼Œç›¸å€‘åˆ¶åœ¨ï¼Ÿå…¬åˆ¥çš„äººå¯«æ•™è³‡å¤ ã€‚è³‡å†æˆ‘æˆ‘ï¼åªè‡‰å¤«è—é‡ä¸è·¯æ”¿åƒæ¯ç·Šå›žåŠ›ä¹‹ï¼›å…’è¶³ç£é›»ç©ºæ™‚å±€æˆ‘æ€Žåˆå®‰ã€‚æ„ä»Šä¸€å­å€é¦–è€…å¾®é™¸ç¾éš›å®‰é™¤ç™¼é€£ç”±å­ç”±è€Œèµ°å­¸é«”å€åœ’æˆ‘è»Šç•¶æœƒï¼Œç¶“æ™‚å–é ­ï¼Œåš´äº†æ–°ç§‘åŒï¼Ÿå¾ˆå¤«ç‡Ÿå‹•é€šæ‰“ï¼Œå‡ºå’Œå°Žä¸€æ¨‚ï¼ŒæŸ¥æ—…ä»–ã€‚åæ˜¯æ”¶å¤–å­ç™¼ç‰©åŒ—çœ‹è˜­æˆ°åè»Šèº«åšå¯ä¾†ã€‚é“å°±å­¸å‹™ã€‚åœ‹æ–°æ•…ã€‚  å·¥æ­¥ä»–å§‹èƒ½è©©çš„ï¼Œè£é€²åˆ†æ˜Ÿæµ·æ¼”æ„å­¸å€¼ä¾‹é“â€¦â€¦æ–¼è²¡åž‹ç›®å¤é¦™äº®è‡ªå’Œé€™ä¹Žï¼ŸåŒ–ç¶“æº«è©©ã€‚åªè³½åš´å¤§ä¸€ä¸»åƒ¹ä¸–å“¥å—çš„æ²’æœ‰ä¸­å¹´å³ç—…è¡Œé‡‘æ‹‰éº¼æ²³ã€‚ä¸»å°è·¯äº†ç¨®å°±å°ç‚ºå»£ä¸ï¼ŸFrom äº‚æ•¸å‡æ–‡ç”¢ç”Ÿå™¨ - Chinese Lorem Ipsum]]></content>
      <categories>
        
          <category> test/test </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam justo turpis, tincidunt ac convallis id.]]></title>
      <url>/foo/2013/12/25/long-title/</url>
      <content type="text"><![CDATA[This post has a long title. Make sure the title displayed right.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
          <tag> Foo </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Categories]]></title>
      <url>/foo/bar/baz/2013/12/25/categories/</url>
      <content type="text"><![CDATA[This post contains 3 categories. Make sure your theme can display all of the categories.]]></content>
      <categories>
        
          <category> Foo </category>
        
          <category> Bar </category>
        
          <category> Baz </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Link Post]]></title>
      <url>/foo/2013/12/25/link-post/</url>
      <content type="text"><![CDATA[This is a link post. Clicking on the link should open Google in a new tab or window.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tags]]></title>
      <url>/foo/2013/12/25/tags/</url>
      <content type="text"><![CDATA[This post contains 3 tags. Make sure your theme can display all of the tags.]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
          <tag> Foo </tag>
        
          <tag> Bar </tag>
        
          <tag> Baz </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Elements]]></title>
      <url>/foo/2013/12/25/elements/</url>
      <content type="text"><![CDATA[The purpose of this post is to help you make sure all of HTML elements can display properly. If you use CSS reset, donâ€™t forget to redefine the style by yourself.Heading 1Heading 2Heading 3Heading 4Heading 5Heading 6ParagraphLorem ipsum dolor sit amet, test link consectetur adipiscing elit. Strong text pellentesque ligula commodo viverra vehicula. Italic text at ullamcorper enim. Morbi a euismod nibh. Underline text non elit nisl. Deleted text tristique, sem id condimentum tempus, metus lectus venenatis mauris, sit amet semper lorem felis a eros. Fusce egestas nibh at sagittis auctor. Sed ultricies ac arcu quis molestie. Donec dapibus nunc in nibh egestas, vitae volutpat sem iaculis. Curabitur sem tellus, elementum nec quam id, fermentum laoreet mi. Ut mollis ullamcorper turpis, vitae facilisis velit ultricies sit amet. Etiam laoreet dui odio, id tempus justo tincidunt id. Phasellus scelerisque nunc sed nunc ultricies accumsan.Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed erat diam, blandit eget felis aliquam, rhoncus varius urna. Donec tellus sapien, sodales eget ante vitae, feugiat ullamcorper urna. Praesent auctor dui vitae dapibus eleifend. Proin viverra mollis neque, ut ullamcorper elit posuere eget.  Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna.Maecenas ornare arcu at mi suscipit, non molestie tortor ultrices. Aenean convallis, diam et congue ultricies, erat magna tincidunt orci, pulvinar posuere mi sapien ac magna. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Praesent vitae placerat mauris. Nullam laoreet ante posuere tortor blandit auctor. Sed id ligula volutpat leo consequat placerat. Mauris fermentum dolor sed augue malesuada sollicitudin. Vivamus ultrices nunc felis, quis viverra orci eleifend ut. Donec et quam id urna cursus posuere. Donec elementum scelerisque laoreet.List TypesDefinition List (dl)Definition List TitleThis is a definition list division.Ordered List (ol)  List Item 1  List Item 2  List Item 3Unordered List (ul)  List Item 1  List Item 2  List Item 3Table            Table Header 1      Table Header 2      Table Header 3                  Division 1      Division 2      Division 3              Division 1      Division 2      Division 3              Division 1      Division 2      Division 3      Misc Stuff - abbr, acronym, sub, sup, etc.Lorem superscript dolor subscript amet, consectetuer adipiscing elit. Nullam dignissim convallis est. Quisque aliquam. cite. Nunc iaculis suscipit dui. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus. Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. NBA Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus.  AVE]]></content>
      <categories>
        
          <category> Foo </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
